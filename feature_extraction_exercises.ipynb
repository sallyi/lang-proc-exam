{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction\n",
    "\n",
    "In this notebook we will learn how to extract different features from a text and how to combine them. It's pretty simple, but if you have this part well organized, it will be really useful in the near future. So, let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import argparse\n",
    "import time\n",
    "import codecs\n",
    "from collections import defaultdict\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import re\n",
    "import random\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences_1 = [\n",
    "    'I liked this movie',\n",
    "    'The plot was intriguing',\n",
    "    'Oh, it was truly boring',\n",
    "]\n",
    "train_classes = [1,1,0]\n",
    "\n",
    "test_sentences = [\n",
    "    'I liked it',\n",
    "    'The plot was boring !',\n",
    "    'Oh, it was absolutely terrible !',\n",
    "]\n",
    "test_classes = [1,0,0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "g= open(\"known00002.txt\",'r+',encoding='utf-8')\n",
    "#texts.append((f.read(),label))\n",
    "\n",
    "\n",
    "lines =[line for line in g] \n",
    "\n",
    "text = ''.join([x for x in lines])\n",
    "#text\n",
    "\n",
    "#g.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "#bu = []\n",
    "#for ind,item in enumerate(train_sentences):\n",
    "pos_tags= [nltk.pos_tag(word_tokenize(train_sentences_1[ind])) for ind, item in enumerate(train_sentences_1)]\n",
    "\n",
    "pos_sents = []\n",
    "for sent in pos_tags:\n",
    "    pos = ' '.join([pos_tag[1] for pos_tag in sent])\n",
    "    pos_sents.append(pos)\n",
    "    \n",
    "vectorizer = CountVectorizer(ngram_range = (1,1),token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\", min_df=1)\n",
    "\n",
    "pos_ngram = vectorizer.fit_transform(pos_sents)\n",
    "\n",
    "pos_ngram.toarray(), pos_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = \"C:\\Users\\Patricija\\Desktop\\NLP2\\lang-proc-exam\\pan19-cross-domain-authorship-attribution-training-dataset-2019-01-23\\pan19-cross-domain-authorship-attribution-training-dataset-2019-01-23\\problem00001\\candidate00001/known00002.txt\"\n",
    "\n",
    "#def removePunctuation(word):\n",
    " #   return re.sub (\"[^a-zA-Z0-9\\s\\-\\']\", \"\", word)\n",
    "\n",
    "\n",
    "def open_file():\n",
    "    f= open(\"known00002.txt\",'r+',encoding='utf-8')\n",
    "#texts.append((f.read(),label))\n",
    "\n",
    "\n",
    "    lines =[line.lower() for line in f] \n",
    "\n",
    "    f.close()\n",
    "    \n",
    "    return lines\n",
    "def tokenizer(lines):\n",
    "\n",
    "    tok = CountVectorizer(ngram_range = (1,1), min_df = 1) #token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\"\n",
    "\n",
    "    tok_bow = tok.fit_transform(lines)\n",
    "\n",
    "    types_of_tokens = tok.get_feature_names()  #gives TYPES withou stuff like \"\" or \\, only differs by 11 types from the other method below\n",
    "\n",
    "    \n",
    "    #Tokenize and lemmatize text \n",
    "\n",
    "    lines_ = str(lines)\n",
    "\n",
    "    lemmatizer = nltk.WordNetLemmatizer() \n",
    "\n",
    "    tokens = nltk.word_tokenize(lines_) \n",
    "\n",
    "    lemmas = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "\n",
    "    types = set(tokens)\n",
    "    \n",
    "    lexical_diversity = len(types) / len(tokens)\n",
    "\n",
    "    return lexical_diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Get the bag of words representation of the training set\n",
    "\n",
    "You can make it simply by using the `CountVectorizer` class from `scikit-learn`. Once you instantiate the vectorizer and you `fit_transform` it, this should create a 3x11 sparse matrix.\n",
    "\n",
    "If the matrix is saved in a variable called `X`, you can see the values of the matrix by using the `X.toarray()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Iterable over raw text documents expected, string object received.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-78-4675d0a5bc65>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mvectorizer_words\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mngram_range\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtoken_pattern\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu\"(?u)\\\\b\\\\w+\\\\b\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1046\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1047\u001b[0m             raise ValueError(\n\u001b[1;32m-> 1048\u001b[1;33m                 \u001b[1;34m\"Iterable over raw text documents expected, \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1049\u001b[0m                 \"string object received.\")\n\u001b[0;32m   1050\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Iterable over raw text documents expected, string object received."
     ]
    }
   ],
   "source": [
    "#tokenizer(text)\n",
    "linez = str(text)\n",
    "\n",
    "vectorizer_words= CountVectorizer(ngram_range = (1,3),token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\", min_df=1)\n",
    "\n",
    "train_data = vectorizer_words.fit_transform(text)\n",
    "\n",
    "\n",
    "vectorizer_characters = CountVectorizer(analyzer = \"char\", ngram_range = (1,3),token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\", min_df=1)\n",
    "\n",
    "train_data_characters = vectorizer_characters.fit_transform(text)\n",
    "\n",
    "train_data_characters.toarray(), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "lower not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-1f3fceee8197>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;31m#NEED to combine features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;31m###\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mextract_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-67-1f3fceee8197>\u001b[0m in \u001b[0;36mextract_features\u001b[1;34m(train_data)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mvectorizer_characters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manalyzer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"char\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngram_range\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtoken_pattern\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu\"(?u)\\\\b\\\\w+\\\\b\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mtrain_data_characters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer_characters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1056\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m-> 1058\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m   1059\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1060\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    968\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    969\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 970\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    971\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalyzer\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'char'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 340\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_char_ngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalyzer\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'char_wb'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 256\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    257\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    687\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 689\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" not found\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: lower not found"
     ]
    }
   ],
   "source": [
    "def extract_features(train_data): \n",
    "\n",
    "    \n",
    "\n",
    "    vectorizer_words= CountVectorizer(ngram_range = (1,3),token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\", min_df=1)\n",
    "\n",
    "    train_data = vectorizer_words.fit_transform(train_data)\n",
    "\n",
    "\n",
    "    vectorizer_characters = CountVectorizer(analyzer = \"char\", ngram_range = (1,3),token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\", min_df=1)\n",
    "\n",
    "    train_data_characters = vectorizer_characters.fit_transform(train_data)\n",
    "\n",
    "    \n",
    "    \n",
    "    #feature_pipeline = Pipeline([(\"union\", FeatureUnion([(\"word\", CountVectorizer(ngram_range = (1,3), token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\",min_df = 1)),(\"characters\", CountVectorizer(analyzer = \"char\", ngram_range = (1,3),token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\", min_df=1))]))])\n",
    "    \n",
    "    #train_data_union = feature_pipeline.fit_transform(train_data)\n",
    "    #max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "\n",
    "    #scaled_train_data = max_abs_scaler.fit_transform(train_data_union)\n",
    "    \n",
    "        \n",
    "    return train_data_characters\n",
    "    #####\n",
    "    #NEED to combine features\n",
    "    ###\n",
    "extract_features(text)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #How to get the path of all of them? Maybe train_sentences \n",
    "    #One file is one training example. In manex's exercise, one sentence is one training example\n",
    "    \n",
    "    \n",
    "    \n",
    "    #lb = LabelEncoder().fit(train_data['author'])  Example of encoding labels\n",
    "    #lb.transform(train['author'])\n",
    "    \n",
    "    #diversity = {}\n",
    "    #for index, single_example in enumerate(train_data):   #Getting lexical diversity a single file at the time? How to make it at once?\n",
    "        \n",
    "     #   train_sentence = open_file(single_example)\n",
    "        \n",
    "      #  lexical_diversity = tokenizer(train_sentence)\n",
    "        \n",
    "       # diversity[index] = lexical_diversity\n",
    "        \n",
    "        #Add the lexical diversity to the overall matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe2 = Pipeline([(\"union\", FeatureUnion([(\"uni\", CountVectorizer(ngram_range = (1,3), min_df = 1)),(\"vectorizer_characters\", CountVectorizer(analyzer = \"char\", ngram_range = (1,3),token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\", min_df=1))]))])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "396"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pipe2 = Pipeline([\n",
    "    ('u1', FeatureUnion([\n",
    "        ('tfdif_features', Pipeline([\n",
    "            ('cv', CountVectorizer()),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "        ])),\n",
    "        ('pos_features', Pipeline([\n",
    "            ('pos', PosTagMatrix(tokenizer=nltk.word_tokenize) ),\n",
    "        ])),\n",
    "    ])),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,\n",
       "        0, 0, 0, 1, 1, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 1, 1, 0, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = extract_features(train_sentences_1)\n",
    "a.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Now get a bag of words of bigrams and trigrams from the training set\n",
    "\n",
    "Use the same class as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-3fc95f80bcbe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mbigr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mngram_range\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mbigrams_bow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbigr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_sentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_sentences' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "bigr = CountVectorizer(ngram_range = (1,1), min_df = 1) #token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\"\n",
    "\n",
    "bigrams_bow = bigr.fit_transform(train_sentences)\n",
    "\n",
    "\n",
    "trigr = CountVectorizer(ngram_range = (3,3))\n",
    "trigrams_bow = trigr.fit_transform(train_sentences)\n",
    "\n",
    "\n",
    "tokens_bi = bigr.get_feature_names()\n",
    "\n",
    "#tokens_bi, bigrams_bow.toa\n",
    "\n",
    "bigrams_bow.toarray()\n",
    "\n",
    "for i,v in enumerate(train_texts):\n",
    "    [i]=train_data[i]/len(train_texts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0, 'b': 1, 'c': 2}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "features = ['a', 'b', 'c']\n",
    "doc = ['a', 'c']\n",
    "\n",
    "vectoris = CountVectorizer(vocabulary=features, token_pattern=r\"\\b\\w+\\b\")\n",
    "\n",
    "vectoris.fit_transform(doc)\n",
    "\n",
    "vectoris.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hint:\n",
    "\n",
    "Oh, do you want to check the vocabulary of the training set? You can do it using the vectorizer. If the vectorizer is saved in a variable called `unigram_vectorizer`, you can check the attribute `unigram_vectorizer.vocabulary_` and you will see the vocabulary there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Test these vectorizers with the test set\n",
    "\n",
    "Try the vectorizers with the test set. What happens if a word doesn't appear in the training corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i liked': 1,\n",
       " 'liked it': 3,\n",
       " 'the plot': 6,\n",
       " 'plot was': 5,\n",
       " 'was boring': 8,\n",
       " 'oh it': 4,\n",
       " 'it was': 2,\n",
       " 'was absolutely': 7,\n",
       " 'absolutely terrible': 0}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_test = bigr.fit_transform(test_sentences)\n",
    "\n",
    "\n",
    "bigr.vocabulary_  #??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Calculate bag of postags for the training set and then apply the vectorizer on the test set.\n",
    "\n",
    "Calculate something similar to the bag of words, but instead of using the words, use the POS-tags of the sentences. The goal here is not to get perfect results, so then, use the `nltk.pos_tag()` function to get the part-of-speeches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function pos_tag_sents in module nltk.tag:\n",
      "\n",
      "pos_tag_sents(sentences, tagset=None, lang='eng')\n",
      "    Use NLTK's currently recommended part of speech tagger to tag the\n",
      "    given list of sentences, each consisting of a list of tokens.\n",
      "    \n",
      "    :param tokens: List of sentences to be tagged\n",
      "    :type tokens: list(list(str))\n",
      "    :param tagset: the tagset to be used, e.g. universal, wsj, brown\n",
      "    :type tagset: str\n",
      "    :param lang: the ISO 639 code of the language, e.g. 'eng' for English, 'rus' for Russian\n",
      "    :type lang: str\n",
      "    :return: The list of tagged sentences\n",
      "    :rtype: list(list(tuple(str, str)))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nltk.pos_tag_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1, 0, 1, 1, 0, 0, 1, 0],\n",
       "        [1, 0, 1, 0, 0, 0, 1, 1],\n",
       "        [0, 1, 0, 1, 1, 1, 1, 0]], dtype=int64),\n",
       " [[('I', 'PRP'), ('liked', 'VBD'), ('this', 'DT'), ('movie', 'NN')],\n",
       "  [('The', 'DT'), ('plot', 'NN'), ('was', 'VBD'), ('intriguing', 'VBG')],\n",
       "  [('Oh', 'UH'),\n",
       "   (',', ','),\n",
       "   ('it', 'PRP'),\n",
       "   ('was', 'VBD'),\n",
       "   ('truly', 'RB'),\n",
       "   ('boring', 'JJ')]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Combine all features for each sentence.\n",
    "\n",
    "Combine all the previous features, and generate a matrix encoding all previously mentioned features: unigrams, bigrams, trigrams and pos_tags. The resulting matrix should have the following dimensions: 3x31\n",
    "\n",
    "You could use the `sklearn.pipeline.FeatureUnion` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#toks = sentence_tokenize(train_sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra to play with: Check this website and think about it. Do you think you can use this for something? (in the exam)\n",
    "\n",
    "http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHARE YOUR KNOWLEDGE!\n",
    "\n",
    "### Do you know any other way of representing the features of the training/testing set?\n",
    "\n",
    "Please share your knowledge using the forum from Absalon!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
