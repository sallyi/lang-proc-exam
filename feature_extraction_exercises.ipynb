{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction\n",
    "\n",
    "In this notebook we will learn how to extract different features from a text and how to combine them. It's pretty simple, but if you have this part well organized, it will be really useful in the near future. So, let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = [\n",
    "    'I liked this movie',\n",
    "    'The plot was intriguing',\n",
    "    'Oh, it was truly boring',\n",
    "]\n",
    "train_classes = [1,1,0]\n",
    "\n",
    "test_sentences = [\n",
    "    'I liked it',\n",
    "    'The plot was boring !',\n",
    "    'Oh, it was absolutely terrible !',\n",
    "]\n",
    "test_classes = [1,0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Get the bag of words representation of the training set\n",
    "\n",
    "You can make it simply by using the `CountVectorizer` class from `scikit-learn`. Once you instantiate the vectorizer and you `fit_transform` it, this should create a 3x11 sparse matrix.\n",
    "\n",
    "If the matrix is saved in a variable called `X`, you can see the values of the matrix by using the `X.toarray()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\",min_df=1)\n",
    "unigrams = vectorizer.fit_transform(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['boring',\n",
       "  'i',\n",
       "  'intriguing',\n",
       "  'it',\n",
       "  'liked',\n",
       "  'movie',\n",
       "  'oh',\n",
       "  'plot',\n",
       "  'the',\n",
       "  'this',\n",
       "  'truly',\n",
       "  'was'],\n",
       " array([[0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1],\n",
       "        [1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1]], dtype=int64))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_uni = vectorizer.get_feature_names()\n",
    "tokens_uni, X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Now get a bag of words of bigrams and trigrams from the training set\n",
    "\n",
    "Use the same class as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "bigr = CountVectorizer(ngram_range = (2,2), token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\",min_df=1)\n",
    "bigrams_bow = bigr.fit_transform(train_sentences)\n",
    "\n",
    "\n",
    "trigr = CountVectorizer(ngram_range = (3,3))\n",
    "trigrams_bow = trigr.fit_transform(train_sentences)\n",
    "bigr.vocabulary_\n",
    "\n",
    "tokens_bi = bigr.get_feature_names()\n",
    "\n",
    "tokens_bi, bigrams_bow.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hint:\n",
    "\n",
    "Oh, do you want to check the vocabulary of the training set? You can do it using the vectorizer. If the vectorizer is saved in a variable called `unigram_vectorizer`, you can check the attribute `unigram_vectorizer.vocabulary_` and you will see the vocabulary there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Test these vectorizers with the test set\n",
    "\n",
    "Try the vectorizers with the test set. What happens if a word doesn't appear in the training corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i liked': 1,\n",
       " 'liked it': 3,\n",
       " 'the plot': 6,\n",
       " 'plot was': 5,\n",
       " 'was boring': 8,\n",
       " 'oh it': 4,\n",
       " 'it was': 2,\n",
       " 'was absolutely': 7,\n",
       " 'absolutely terrible': 0}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_test = bigr.fit_transform(test_sentences)\n",
    "\n",
    "\n",
    "bigr.vocabulary_  #??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Calculate bag of postags for the training set and then apply the vectorizer on the test set.\n",
    "\n",
    "Calculate something similar to the bag of words, but instead of using the words, use the POS-tags of the sentences. The goal here is not to get perfect results, so then, use the `nltk.pos_tag()` function to get the part-of-speeches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function pos_tag_sents in module nltk.tag:\n",
      "\n",
      "pos_tag_sents(sentences, tagset=None, lang='eng')\n",
      "    Use NLTK's currently recommended part of speech tagger to tag the\n",
      "    given list of sentences, each consisting of a list of tokens.\n",
      "    \n",
      "    :param tokens: List of sentences to be tagged\n",
      "    :type tokens: list(list(str))\n",
      "    :param tagset: the tagset to be used, e.g. universal, wsj, brown\n",
      "    :type tagset: str\n",
      "    :param lang: the ISO 639 code of the language, e.g. 'eng' for English, 'rus' for Russian\n",
      "    :type lang: str\n",
      "    :return: The list of tagged sentences\n",
      "    :rtype: list(list(tuple(str, str)))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nltk.pos_tag_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('I', 'PRP'), ('liked', 'VBD'), ('this', 'DT'), ('movie', 'NN')],\n",
       " [('The', 'DT'), ('plot', 'NN'), ('was', 'VBD'), ('intriguing', 'VBG')],\n",
       " [('Oh', 'UH'),\n",
       "  (',', ','),\n",
       "  ('it', 'PRP'),\n",
       "  ('was', 'VBD'),\n",
       "  ('truly', 'RB'),\n",
       "  ('boring', 'JJ')]]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from nltk.tokenize import word_tokenize\n",
    "#bu = []\n",
    "#for ind,item in enumerate(train_sentences):\n",
    "bu= [nltk.pos_tag(word_tokenize(train_sentences[ind])) for ind, item in enumerate(train_sentences)]\n",
    "\n",
    "bu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Combine all features for each sentence.\n",
    "\n",
    "Combine all the previous features, and generate a matrix encoding all previously mentioned features: unigrams, bigrams, trigrams and pos_tags. The resulting matrix should have the following dimensions: 3x31\n",
    "\n",
    "You could use the `sklearn.pipeline.FeatureUnion` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "All estimators should implement fit and transform. '[('I', 'PRP'), ('liked', 'VBD'), ('this', 'DT'), ('movie', 'NN')]' (type <class 'list'>) doesn't",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-114-2f9ff303a374>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m union = FeatureUnion([(\"uni\", vectorizer),\n\u001b[1;32m----> 4\u001b[1;33m (\"bi\", bigr), (\"tri\", trigr), (\"PoS\", nltk.pos_tag(word_tokenize(train_sentences[0])))])\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, transformer_list, n_jobs, transformer_weights)\u001b[0m\n\u001b[0;32m    620\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_jobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    621\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformer_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 622\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_transformers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    623\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_validate_transformers\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    664\u001b[0m                 raise TypeError(\"All estimators should implement fit and \"\n\u001b[0;32m    665\u001b[0m                                 \u001b[1;34m\"transform. '%s' (type %s) doesn't\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 666\u001b[1;33m                                 (t, type(t)))\n\u001b[0m\u001b[0;32m    667\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: All estimators should implement fit and transform. '[('I', 'PRP'), ('liked', 'VBD'), ('this', 'DT'), ('movie', 'NN')]' (type <class 'list'>) doesn't"
     ]
    }
   ],
   "source": [
    "#toks = sentence_tokenize(train_sentences)\n",
    "\n",
    "union = FeatureUnion([(\"uni\", vectorizer),(\"bi\", bigr), (\"tri\", trigr), (\"PoS\", nltk.pos_tag(word_tokenize(train_sentences[0])))])\n",
    "\n",
    "\n",
    "\n",
    "#union.fit_transform(train_sentences)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra to play with: Check this website and think about it. Do you think you can use this for something? (in the exam)\n",
    "\n",
    "http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHARE YOUR KNOWLEDGE!\n",
    "\n",
    "### Do you know any other way of representing the features of the training/testing set?\n",
    "\n",
    "Please share your knowledge using the forum from Absalon!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
