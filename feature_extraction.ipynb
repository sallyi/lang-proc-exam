{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction\n",
    "\n",
    "In this notebook we will learn how to extract different features from a text and how to combine them. It's pretty simple, but if you have this part well organized, it will be really useful in the near future. So, let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import argparse\n",
    "import time\n",
    "import codecs\n",
    "from collections import defaultdict\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import random\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_file(path):\n",
    "    with open(path, 'r+', encoding='utf8') as f:\n",
    "        return '\\n'.join([line.strip() for line in f])\n",
    "    \n",
    "def process_dir_files(path):\n",
    "    dir_files = []\n",
    "    for file in os.listdir(path):\n",
    "        current = os.path.join(path, file)\n",
    "        if os.path.isfile(current):\n",
    "            dir_files.append(open_file(current))\n",
    "    return dir_files\n",
    "                     \n",
    "\n",
    "#train_sents= process_dir_files('pan18-cross-domain-authorship-attribution-training-dataset-2017-12-02/problem00001/candidate00001')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def get_pos(text):\n",
    "    pos_tags= nltk.pos_tag(word_tokenize(text))\n",
    "    # print(len(pos_tags))\n",
    "    pos_tags = [word_tag[1] for word_tag in pos_tags]\n",
    "    pos_text = ' '.join(pos_tags)\n",
    "    return pos_tags\n",
    "\n",
    "#get_pos(train_sents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_ngrams(sents):\n",
    "    pos_tags= [nltk.pos_tag(word_tokenize(sents[ind])) for ind, item in enumerate(sents) if item != '']\n",
    "    pos_sents = []\n",
    "    for sent in pos_tags:\n",
    "        #print(sent)\n",
    "        pos = ' '.join([pos_tag[1] for pos_tag in sent])\n",
    "        #print(pos, '\\n')\n",
    "        pos_sents.append(pos)\n",
    "    vectorizer = CountVectorizer(ngram_range = (1,1))\n",
    "    vectorizer.fit(pos_sents)\n",
    "    return vectorizer\n",
    "\n",
    "\n",
    "#pos_vectorizer = get_pos_ngrams(train_sents)\n",
    "#pos_ngram = pos_vectorizer.transform(train_sents)\n",
    "#pos_ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    " A baseline authorship attribution method \n",
    " based on a character n-gram representation\n",
    " and a linear SVM classifier.\n",
    " It has a reject option to leave documents unattributed\n",
    " (when the probabilities of the two most likely training classes are too close)\n",
    " \n",
    " Questions/comments: stamatatos@aegean.gr\n",
    "\n",
    " It can be applied to datasets of PAN-19 cross-domain authorship attribution task\n",
    " See details here: http://pan.webis.de/clef19/pan19-web/author-identification.html\n",
    " Dependencies:\n",
    " - Python 2.7 or 3.6 (we recommend the Anaconda Python distribution)\n",
    " - scikit-learn\n",
    "\n",
    " Usage from command line: \n",
    "    > python pan19-cdaa-baseline.py -i EVALUATION-DIRECTORY -o OUTPUT-DIRECTORY [-n N-GRAM-ORDER] [-ft FREQUENCY-THRESHOLD] [-pt PROBABILITY-THRESHOLD]\n",
    " EVALUATION-DIRECTORY (str) is the main folder of a PAN-19 collection of attribution problems\n",
    " OUTPUT-DIRECTORY (str) is an existing folder where the predictions are saved in the PAN-19 format\n",
    " Optional parameters of the model:\n",
    "   N-GRAM-ORDER (int) is the length of character n-grams (default=3)\n",
    "   FREQUENCY-THRESHOLD (int) is the cutoff threshold used to filter out rare n-grams (default=5)\n",
    "   PROBABILITY-THRESHOLD (float) is the threshold for the reject option assigning test documents to the <UNK> class (default=0.1)\n",
    "                                 Let P1 and P2 be the two maximum probabilities of training classes for a test document. If P1-P2<pt then the test document is assigned to the <UNK> class.\n",
    "   \n",
    " Example:\n",
    "\n",
    "     >  python pan19-cdaa-baseline-svm.py -i \".\\pan19-cross-domain-authorship-attribution-training-dataset-2019-01-23\\\" -o \".\\a\n",
    "nswers-trigram\\\" -n 3\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import argparse\n",
    "import time\n",
    "import codecs\n",
    "from collections import defaultdict\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "def represent_text(text,n,pos=False):\n",
    "    # Extracts all character n-grams from  a 'text'\n",
    "    # if pos is True, extracts POS n-grams\n",
    "    if n>0:\n",
    "        if pos is True:\n",
    "            text = get_pos(text)\n",
    "            tokens = [' '.join(text[i:i+n]) for i in range(len(text)-n+1)]\n",
    "            #print(tokens)\n",
    "        else:\n",
    "            tokens = [text[i:i+n] for i in range(len(text)-n+1)]\n",
    "    frequency = defaultdict(int)\n",
    "    for token in tokens:\n",
    "        frequency[token] += 1\n",
    "    return frequency\n",
    "\n",
    "#represent_text(train_sents[0], 2)\n",
    "#represent_text(train_sents[0], 2, pos=True)\n",
    "                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nvocab = extract_vocabulary([(x,i) for i, x in enumerate(train_sents)], 2, 5, pos=True)\\nprint(len(vocab))\\nprint(vocab)\\nvectorizer = CountVectorizer(vocabulary=[x.lower() for  x in vocab])\\nprint([' '.join(get_pos(text)) for text in train_sents])\\n\\ntrain_data = vectorizer.fit_transform([' '.join(get_pos(text)) for text in train_sents])\\nprint(vectorizer.get_feature_names())\\ntrain_data = train_data.astype(float)\\nprint(train_data.shape)\\nprint(train_data.toarray())\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_files(path,label):\n",
    "    # Reads all text files located in the 'path' and assigns them to 'label' class\n",
    "    files = glob.glob(path+os.sep+label+os.sep+'*.txt')\n",
    "    texts=[]\n",
    "    for i,v in enumerate(files):\n",
    "        f=codecs.open(v,'r',encoding='utf-8')\n",
    "        texts.append((f.read(),label))\n",
    "        f.close()\n",
    "    return texts\n",
    "\n",
    "def extract_vocabulary(texts,n,ft,pos=False):\n",
    "    # Extracts all characer 'n'-grams occurring at least 'ft' times in a set of 'texts'\n",
    "    occurrences=defaultdict(int) \n",
    "    for (text,label) in texts:\n",
    "        text_occurrences = {}\n",
    "        if isinstance(n, int):\n",
    "            for x in range(1,n+1):\n",
    "                text_occurrences.update(represent_text(text,x,pos=pos))\n",
    "        else:\n",
    "            pass\n",
    "        for ngram in text_occurrences:\n",
    "            if ngram in occurrences:\n",
    "                occurrences[ngram]+=text_occurrences[ngram]\n",
    "            else:\n",
    "                occurrences[ngram]=text_occurrences[ngram]\n",
    "    vocabulary=[]\n",
    "    for i in occurrences.keys():\n",
    "        if occurrences[i]>=ft:\n",
    "            vocabulary.append(i)\n",
    "    return vocabulary\n",
    "\n",
    "'''\n",
    "vocab = extract_vocabulary([(x,i) for i, x in enumerate(train_sents)], 2, 5, pos=True)\n",
    "print(len(vocab))\n",
    "print(vocab)\n",
    "vectorizer = CountVectorizer(vocabulary=[x.lower() for  x in vocab])\n",
    "print([' '.join(get_pos(text)) for text in train_sents])\n",
    "\n",
    "train_data = vectorizer.fit_transform([' '.join(get_pos(text)) for text in train_sents])\n",
    "print(vectorizer.get_feature_names())\n",
    "train_data = train_data.astype(float)\n",
    "print(train_data.shape)\n",
    "print(train_data.toarray())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STOPWORDS = list(stopwords.words('english'))\n",
    "len(STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lex_div = np.zeros((5,1))\n",
    "lex_div = np.hstack((lex_div, np.ones(lex_div.shape)))\n",
    "lex_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin #gives fit_transform method for free\n",
    "\n",
    "class Feature_Extractor(TransformerMixin):\n",
    "    '''\n",
    "    Performs feature extraction on input docs.\n",
    "    '''\n",
    "    def __init__(self, char=True, pos=True, word=True, lexdiv=True, fnword=True):\n",
    "        self.char = char # uses character ngrams\n",
    "        self.pos = pos # uses pos ngrams\n",
    "        self.word = word # uses word ngrams\n",
    "        self.fnword = fnword # uses function word counts\n",
    "        self.lexdiv = lexdiv # uses lexical diversity score\n",
    "        \n",
    "    def set_params(self, char, pos, word, lexdiv, fnword):\n",
    "        self.char = char # uses character ngrams\n",
    "        self.pos = pos # uses pos ngrams\n",
    "        self.word = word # uses word ngrams\n",
    "        self.fnword = fnword # uses function word counts\n",
    "        self.lexdiv = lexdiv # uses lexical diversity score\n",
    "\n",
    "        \n",
    "    def fit_transform(self, docs, y=None):\n",
    "        ## Char-level n-grams ##\n",
    "        feature_list = []\n",
    "        self.texts = [text for i,(text,label) in enumerate(docs)]\n",
    "        print('Extracting features from train data')\n",
    "        if self.lexdiv is True or not any((self.char, self.pos, self.word, self.fnword, self.lexdiv)) :\n",
    "            ## Lexical Diversity\n",
    "            lex_div = self.lexical_diversity(docs)\n",
    "            if not any((self.char, self.pos, self.word, self.fnword)):\n",
    "                lex_div = np.hstack((lex_div, np.ones(lex_div.shape)))\n",
    "            feature_list.append(lex_div)\n",
    "            \n",
    "        if self.char is True:\n",
    "            char_vocab = extract_vocabulary(docs, 4, 3)\n",
    "            self.char_vectorizer = CountVectorizer(analyzer='char', ngram_range=(4,4),\n",
    "                                              lowercase=False, vocabulary=char_vocab\n",
    "                                                  )\n",
    "            char_data, self.char_vectorizer = self._fit_transform(self.char_vectorizer)\n",
    "            print('char vocabulary size:', len(char_vocab))\n",
    "            feature_list.append(char_data)\n",
    "            # print(char_data.toarray())\n",
    "        \n",
    "        ## POS n-grams ##\n",
    "        if self.pos is True:\n",
    "            pos_vocab = [x.lower() for x in extract_vocabulary(docs,2,3,pos=True)]\n",
    "            # print(pos_vocab)\n",
    "            self.pos_vectorizer = CountVectorizer(ngram_range=(1,3), vocabulary=pos_vocab\n",
    "                                                 )\n",
    "            print('\\t', 'pos vocabulary size:', len(pos_vocab))\n",
    "            pos_data, self.pos_vectorizer = self._fit_transform(self.pos_vectorizer, pos_replace=True)\n",
    "            feature_list.append(pos_data)\n",
    "\n",
    "        ## Word n-grams ##\n",
    "        if self.word is True:\n",
    "            self.word_vectorizer = CountVectorizer(ngram_range=(1,1))\n",
    "            word_data, self.word_vectorizer = self._fit_transform(self.word_vectorizer)\n",
    "            feature_list.append(word_data)\n",
    "        \n",
    "        ## Function Word counts ##\n",
    "        if self.fnword is True:\n",
    "            self.fnword_vectorizer = CountVectorizer(vocabulary=STOPWORDS)\n",
    "            fnword_data, self.fnword_vectorizer = self._fit_transform(self.fnword_vectorizer)\n",
    "            feature_list.append(fnword_data)\n",
    "\n",
    "        print('lexdiv: %s char: %s pos: %s word: %s fnword: %s'%(self.lexdiv, self.char, self.pos, \n",
    "                                                                          self.word, self.fnword))\n",
    "        feature_data = self.combine_features(tuple(feature_list)) \n",
    "        print('size of features:', feature_data.shape)   \n",
    "        return feature_data\n",
    "    \n",
    "    def combine_features(self, feat_tuple):\n",
    "        feature_data = hstack(feat_tuple)\n",
    "        return feature_data\n",
    "    \n",
    "    def replace_words_POS(self, texts):\n",
    "        return [' '.join(get_pos(text)) for text in texts]\n",
    "    \n",
    "    def lexical_diversity(self, docs):\n",
    "        lex_div = np.array([len(set(text)) / len(text) for (text,label) in docs]).reshape(len(docs), 1)\n",
    "        # print('lexical diversity:', lex_div.shape)\n",
    "        return lex_div\n",
    "    \n",
    "    def _fit_transform(self, vectorizer, pos_replace=False):\n",
    "        if pos_replace is True:\n",
    "            texts = self.replace_words_POS(self.texts) # replace words in text with POS\n",
    "        else:\n",
    "            texts = self.texts\n",
    "        vec_data = vectorizer.fit_transform(texts)\n",
    "        vec_data = vec_data.astype(float)\n",
    "        return vec_data, vectorizer\n",
    "    \n",
    "    def _transform(self, vectorizer, pos_replace=False):\n",
    "        if pos_replace is True:\n",
    "            texts = self.replace_words_POS(self.texts) # replace words in text with POS\n",
    "        else:\n",
    "            texts = self.texts\n",
    "        vec_data = vectorizer.transform(texts)\n",
    "        vec_data = vec_data.astype(float)\n",
    "        return vec_data\n",
    "    \n",
    "    def transform(self, docs):\n",
    "        feature_list = []\n",
    "        self.texts = [text for i,(text,label) in enumerate(docs)]\n",
    "        if self.lexdiv is True or not any((self.char, self.pos, self.word, self.fnword, self.lexdiv)):\n",
    "            lex_div = self.lexical_diversity(docs)\n",
    "            if not any((self.char, self.pos, self.word, self.fnword)):\n",
    "                lex_div = np.hstack((lex_div, np.ones(lex_div.shape)))\n",
    "            feature_list.append(lex_div)\n",
    "        if self.char is True:\n",
    "            feature_list.append(self._transform(self.char_vectorizer))\n",
    "        if self.word is True:\n",
    "            feature_list.append(self._transform(self.word_vectorizer))\n",
    "        if self.pos is True:\n",
    "            feature_list.append(self._transform(self.pos_vectorizer, pos_replace=True))\n",
    "        if self.fnword is True:\n",
    "            feature_list.append(self._transform(self.fnword_vectorizer))\n",
    "        print('Extracting features from test data')\n",
    "        print('lexdiv: %s char: %s pos: %s word: %s fnword: %s'%(self.lexdiv, self.char, self.pos, \n",
    "                                                                          self.word, self.fnword))\n",
    "        feature_data = self.combine_features(tuple(feature_list)) \n",
    "        print('size of features:', feature_data.shape)   \n",
    "        return feature_data\n",
    "        \n",
    "def write_results(path, problem, unk_folder, predictions):\n",
    "    # Saving output data\n",
    "    out_data=[]\n",
    "    unk_filelist = glob.glob(path+os.sep+problem+os.sep+unk_folder+os.sep+'*.txt')\n",
    "    pathlen=len(path+os.sep+problem+os.sep+unk_folder+os.sep)\n",
    "    for i,v in enumerate(predictions):\n",
    "        out_data.append({'unknown-text': unk_filelist[i][pathlen:], 'predicted-author': v})\n",
    "    with open(outpath+os.sep+'answers-'+problem+'.json', 'w') as f:\n",
    "        json.dump(out_data, f, indent=4)\n",
    "    print('\\t', 'answers saved to file','answers-'+problem+'.json')\n",
    "    \n",
    "\n",
    "def get_baseline_model_fn(num_in, num_out):\n",
    "    # for Keras: initialize baseline model fn with num features and num predicted categories\n",
    "    def baseline_model():\n",
    "        # create model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(8, input_dim=num_in, activation='relu'))\n",
    "        model.add(Dense(num_out, activation='softmax'))\n",
    "        # Compile model\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        return model\n",
    "    return baseline_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem00001\n",
      "\t language:  en\n",
      "\t 20 candidate authors\n",
      "\t 140 known texts\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   2 out of   5 | elapsed:    2.5s remaining:    3.8s\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   5 | elapsed:    3.0s remaining:    2.0s\n",
      "[Parallel(n_jobs=3)]: Done   5 out of   5 | elapsed:    4.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done   5 out of   5 | elapsed:    4.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from train data\n",
      "char vocabulary size: 28470\n",
      "lexdiv: True char: True pos: False word: False fnword: True\n",
      "size of features: (140, 28650)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [0 0 0 ... 0 0 0] are constant.\n",
      "  UserWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter (CV score=0.730):\n",
      "{'features__char': True, 'features__fnword': True, 'features__lexdiv': True, 'features__pos': False, 'features__word': False, 'sel__percentile': 70, 'svm__C': 0.1}\n",
      "problem00002\n",
      "\t language:  en\n",
      "\t 5 candidate authors\n",
      "\t 35 known texts\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   2 out of   5 | elapsed:    5.1s remaining:    7.6s\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   5 | elapsed:    5.8s remaining:    3.8s\n",
      "[Parallel(n_jobs=3)]: Done   5 out of   5 | elapsed:   10.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done   5 out of   5 | elapsed:   10.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from train data\n",
      "char vocabulary size: 14415\n",
      "\t pos vocabulary size: 619\n",
      "lexdiv: True char: True pos: True word: False fnword: True\n",
      "size of features: (35, 15214)\n",
      "Best parameter (CV score=0.980):\n",
      "{'features__char': True, 'features__fnword': True, 'features__lexdiv': True, 'features__pos': True, 'features__word': False, 'sel__percentile': 65, 'svm__C': 10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [0 0 0 ... 0 0 0] are constant.\n",
      "  UserWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'mean_fit_time': array([1.5294313]),\n",
       "  'std_fit_time': array([0.18031354]),\n",
       "  'mean_score_time': array([0.13267465]),\n",
       "  'std_score_time': array([0.04177476]),\n",
       "  'param_features__char': masked_array(data=[True],\n",
       "               mask=[False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'param_features__fnword': masked_array(data=[True],\n",
       "               mask=[False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'param_features__lexdiv': masked_array(data=[True],\n",
       "               mask=[False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'param_features__pos': masked_array(data=[False],\n",
       "               mask=[False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'param_features__word': masked_array(data=[False],\n",
       "               mask=[False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'param_sel__percentile': masked_array(data=[70],\n",
       "               mask=[False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'param_svm__C': masked_array(data=[0.1],\n",
       "               mask=[False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'params': [{'features__char': True,\n",
       "    'features__fnword': True,\n",
       "    'features__lexdiv': True,\n",
       "    'features__pos': False,\n",
       "    'features__word': False,\n",
       "    'sel__percentile': 70,\n",
       "    'svm__C': 0.1}],\n",
       "  'split0_test_score': array([0.625]),\n",
       "  'split1_test_score': array([0.675]),\n",
       "  'split2_test_score': array([0.85]),\n",
       "  'split3_test_score': array([0.7]),\n",
       "  'split4_test_score': array([0.8]),\n",
       "  'mean_test_score': array([0.73]),\n",
       "  'std_test_score': array([0.08276473]),\n",
       "  'rank_test_score': array([1], dtype=int32),\n",
       "  'split0_train_score': array([1.]),\n",
       "  'split1_train_score': array([1.]),\n",
       "  'split2_train_score': array([1.]),\n",
       "  'split3_train_score': array([1.]),\n",
       "  'split4_train_score': array([1.]),\n",
       "  'mean_train_score': array([1.]),\n",
       "  'std_train_score': array([0.])},\n",
       " {'mean_fit_time': array([3.82055721]),\n",
       "  'std_fit_time': array([0.29912318]),\n",
       "  'mean_score_time': array([0.31869812]),\n",
       "  'std_score_time': array([0.10571398]),\n",
       "  'param_features__char': masked_array(data=[True],\n",
       "               mask=[False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'param_features__fnword': masked_array(data=[True],\n",
       "               mask=[False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'param_features__lexdiv': masked_array(data=[True],\n",
       "               mask=[False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'param_features__pos': masked_array(data=[True],\n",
       "               mask=[False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'param_features__word': masked_array(data=[False],\n",
       "               mask=[False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'param_sel__percentile': masked_array(data=[65],\n",
       "               mask=[False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'param_svm__C': masked_array(data=[10],\n",
       "               mask=[False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'params': [{'features__char': True,\n",
       "    'features__fnword': True,\n",
       "    'features__lexdiv': True,\n",
       "    'features__pos': True,\n",
       "    'features__word': False,\n",
       "    'sel__percentile': 65,\n",
       "    'svm__C': 10}],\n",
       "  'split0_test_score': array([0.9]),\n",
       "  'split1_test_score': array([1.]),\n",
       "  'split2_test_score': array([1.]),\n",
       "  'split3_test_score': array([1.]),\n",
       "  'split4_test_score': array([1.]),\n",
       "  'mean_test_score': array([0.98]),\n",
       "  'std_test_score': array([0.04]),\n",
       "  'rank_test_score': array([1], dtype=int32),\n",
       "  'split0_train_score': array([1.]),\n",
       "  'split1_train_score': array([1.]),\n",
       "  'split2_train_score': array([1.]),\n",
       "  'split3_train_score': array([1.]),\n",
       "  'split4_train_score': array([1.]),\n",
       "  'mean_train_score': array([1.]),\n",
       "  'std_train_score': array([0.])}]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def pipeline_svm(path, outpath):\n",
    "    infocollection = path+os.sep+'collection-info.json'\n",
    "    problems = []\n",
    "    language = []\n",
    "    with open(infocollection, 'r') as f:\n",
    "        for attrib in json.load(f):\n",
    "            problems.append(attrib['problem-name'])\n",
    "            language.append(attrib['language'])\n",
    "    scores_all = []\n",
    "    param_grids = [{\n",
    "        'features__char': [True],\n",
    "        'features__pos': [False],\n",
    "        'features__word': [False],\n",
    "        'features__lexdiv': [True],\n",
    "        'features__fnword': [True],\n",
    "        'sel__percentile': [70],\n",
    "        'svm__C': [0.1],\n",
    "        },\n",
    "        {\n",
    "        'features__char': [True],\n",
    "        'features__pos': [True],\n",
    "        'features__word': [False],\n",
    "        'features__lexdiv': [True],\n",
    "        'features__fnword': [True],\n",
    "        'sel__percentile': [65],\n",
    "        'svm__C': [10],\n",
    "        }\n",
    "    ]\n",
    "    for index,problem in enumerate(problems):\n",
    "        print(problem)\n",
    "        # Reading information about the problem\n",
    "        infoproblem = path+os.sep+problem+os.sep+'problem-info.json'\n",
    "        candidates = []\n",
    "        with open(infoproblem, 'r') as f:\n",
    "            fj = json.load(f)\n",
    "            unk_folder = fj['unknown-folder']\n",
    "            for attrib in fj['candidate-authors']:\n",
    "                candidates.append(attrib['author-name'])\n",
    "        # Building training set\n",
    "        docs=[]\n",
    "        for candidate in candidates:\n",
    "            docs.extend(read_files(path+os.sep+problem,candidate))\n",
    "        train_labels = np.array([label for i,(text,label) in enumerate(docs)])\n",
    "        print('\\t', 'language: ', language[index])\n",
    "        print('\\t', len(candidates), 'candidate authors')\n",
    "        print('\\t', len(docs), 'known texts')\n",
    "        \n",
    "        ###### Applying Classifiers #####\n",
    "        svm = SVC(random_state=442)\n",
    "        feat_extractor = Feature_Extractor()\n",
    "        sel = SelectPercentile(f_classif)\n",
    "        scaler = preprocessing.MaxAbsScaler()\n",
    "        pipe = Pipeline(steps=[('features', feat_extractor), ('sel', sel), \n",
    "                               ('scale', scaler), ('svm', svm)])\n",
    "        param_grid = param_grids[index]\n",
    "        search = GridSearchCV(pipe, param_grid, iid=False, cv=5, n_jobs=3, verbose=10)\n",
    "        search.fit(docs, train_labels)\n",
    "        print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "        print(search.best_params_)\n",
    "        scores_all.append(search.cv_results_)\n",
    "    return scores_all\n",
    "\n",
    "\n",
    "%timeit\n",
    "\n",
    "base_dir='pan18-cross-domain-authorship-attribution-training-dataset-2017-12-02'\n",
    "out_dir = base_dir+os.sep+'output-dir'\n",
    "eval_dir = base_dir+os.sep+'eval-dir'\n",
    "svm_results = pipeline_svm(base_dir,out_dir)\n",
    "svm_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem00001\n",
      "\t language:  en\n",
      "\t 20 candidate authors\n",
      "\t 140 known texts\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   2 tasks      | elapsed:    5.0s\n",
      "[Parallel(n_jobs=3)]: Done   7 tasks      | elapsed:   16.1s\n",
      "[Parallel(n_jobs=3)]: Done  12 tasks      | elapsed:   23.8s\n",
      "[Parallel(n_jobs=3)]: Done  19 tasks      | elapsed:   36.9s\n",
      "[Parallel(n_jobs=3)]: Done  23 out of  25 | elapsed:  1.2min remaining:    6.5s\n",
      "[Parallel(n_jobs=3)]: Done  25 out of  25 | elapsed:  1.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from train data\n",
      "char vocabulary size: 28470\n",
      "lexdiv: False char: True pos: False word: False fnword: True\n",
      "size of features: (140, 28649)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [0 0 0 ... 0 0 0] are constant.\n",
      "  UserWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter (CV score=0.840):\n",
      "{'clf__alpha': 1, 'clf__learning_rate_init': 0.001, 'features__char': True, 'features__fnword': True, 'features__lexdiv': False, 'features__pos': False, 'features__word': False, 'sel__percentile': 55}\n",
      "problem00002\n",
      "\t language:  en\n",
      "\t 5 candidate authors\n",
      "\t 35 known texts\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   2 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=3)]: Done   7 tasks      | elapsed:    6.3s\n",
      "[Parallel(n_jobs=3)]: Done  12 tasks      | elapsed:    8.7s\n",
      "[Parallel(n_jobs=3)]: Done  19 tasks      | elapsed:   41.1s\n",
      "[Parallel(n_jobs=3)]: Done  23 out of  25 | elapsed:   57.8s remaining:    5.0s\n",
      "[Parallel(n_jobs=3)]: Done  25 out of  25 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from train data\n",
      "char vocabulary size: 14415\n",
      "lexdiv: True char: True pos: False word: True fnword: True\n",
      "size of features: (35, 18878)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [0 0 0 ... 0 0 0] are constant.\n",
      "  UserWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter (CV score=0.980):\n",
      "{'clf__alpha': 1, 'clf__learning_rate_init': 0.001, 'features__char': True, 'features__fnword': True, 'features__lexdiv': True, 'features__pos': False, 'features__word': True, 'sel__percentile': 55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'mean_fit_time': array([ 5.2441958 ,  5.0670495 ,  4.90210514,  4.47331686, 28.07843208]),\n",
       "  'std_fit_time': array([0.7838351 , 1.0539565 , 0.6981595 , 0.62890445, 3.08307084]),\n",
       "  'mean_score_time': array([0.14665532, 0.1347311 , 0.14142408, 0.13235531, 0.16557746]),\n",
       "  'std_score_time': array([0.0601638 , 0.06923629, 0.02966453, 0.04928331, 0.06388782]),\n",
       "  'param_clf__alpha': masked_array(data=[0.0001, 0.001, 0.01, 0.1, 1],\n",
       "               mask=[False, False, False, False, False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'param_clf__learning_rate_init': masked_array(data=[0.001, 0.001, 0.001, 0.001, 0.001],\n",
       "               mask=[False, False, False, False, False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'param_features__char': masked_array(data=[True, True, True, True, True],\n",
       "               mask=[False, False, False, False, False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'param_features__fnword': masked_array(data=[True, True, True, True, True],\n",
       "               mask=[False, False, False, False, False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'param_features__lexdiv': masked_array(data=[False, False, False, False, False],\n",
       "               mask=[False, False, False, False, False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'param_features__pos': masked_array(data=[False, False, False, False, False],\n",
       "               mask=[False, False, False, False, False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'param_features__word': masked_array(data=[False, False, False, False, False],\n",
       "               mask=[False, False, False, False, False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'param_sel__percentile': masked_array(data=[55, 55, 55, 55, 55],\n",
       "               mask=[False, False, False, False, False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'params': [{'clf__alpha': 0.0001,\n",
       "    'clf__learning_rate_init': 0.001,\n",
       "    'features__char': True,\n",
       "    'features__fnword': True,\n",
       "    'features__lexdiv': False,\n",
       "    'features__pos': False,\n",
       "    'features__word': False,\n",
       "    'sel__percentile': 55},\n",
       "   {'clf__alpha': 0.001,\n",
       "    'clf__learning_rate_init': 0.001,\n",
       "    'features__char': True,\n",
       "    'features__fnword': True,\n",
       "    'features__lexdiv': False,\n",
       "    'features__pos': False,\n",
       "    'features__word': False,\n",
       "    'sel__percentile': 55},\n",
       "   {'clf__alpha': 0.01,\n",
       "    'clf__learning_rate_init': 0.001,\n",
       "    'features__char': True,\n",
       "    'features__fnword': True,\n",
       "    'features__lexdiv': False,\n",
       "    'features__pos': False,\n",
       "    'features__word': False,\n",
       "    'sel__percentile': 55},\n",
       "   {'clf__alpha': 0.1,\n",
       "    'clf__learning_rate_init': 0.001,\n",
       "    'features__char': True,\n",
       "    'features__fnword': True,\n",
       "    'features__lexdiv': False,\n",
       "    'features__pos': False,\n",
       "    'features__word': False,\n",
       "    'sel__percentile': 55},\n",
       "   {'clf__alpha': 1,\n",
       "    'clf__learning_rate_init': 0.001,\n",
       "    'features__char': True,\n",
       "    'features__fnword': True,\n",
       "    'features__lexdiv': False,\n",
       "    'features__pos': False,\n",
       "    'features__word': False,\n",
       "    'sel__percentile': 55}],\n",
       "  'split0_test_score': array([0.65 , 0.65 , 0.65 , 0.675, 0.725]),\n",
       "  'split1_test_score': array([0.35 , 0.325, 0.325, 0.325, 0.775]),\n",
       "  'split2_test_score': array([0.7 , 0.75, 0.7 , 0.7 , 0.9 ]),\n",
       "  'split3_test_score': array([0.35, 0.35, 0.35, 0.35, 0.9 ]),\n",
       "  'split4_test_score': array([0.9 , 0.9 , 0.85, 0.85, 0.9 ]),\n",
       "  'mean_test_score': array([0.59 , 0.595, 0.575, 0.58 , 0.84 ]),\n",
       "  'std_test_score': array([0.21307276, 0.22494444, 0.20493902, 0.20700242, 0.07516648]),\n",
       "  'rank_test_score': array([3, 2, 5, 4, 1], dtype=int32),\n",
       "  'split0_train_score': array([1., 1., 1., 1., 1.]),\n",
       "  'split1_train_score': array([1., 1., 1., 1., 1.]),\n",
       "  'split2_train_score': array([1., 1., 1., 1., 1.]),\n",
       "  'split3_train_score': array([1., 1., 1., 1., 1.]),\n",
       "  'split4_train_score': array([1., 1., 1., 1., 1.]),\n",
       "  'mean_train_score': array([1., 1., 1., 1., 1.]),\n",
       "  'std_train_score': array([0., 0., 0., 0., 0.])},\n",
       " {'mean_fit_time': array([ 1.870191  ,  1.85795884,  1.86889739, 15.54664087, 13.72378373]),\n",
       "  'std_fit_time': array([0.11228668, 0.18120348, 0.16654955, 0.96063088, 3.43578717]),\n",
       "  'mean_score_time': array([0.07733531, 0.08533263, 0.07139044, 0.07550111, 0.09481101]),\n",
       "  'std_score_time': array([0.03638227, 0.03795959, 0.02882544, 0.03601872, 0.05083195]),\n",
       "  'param_clf__alpha': masked_array(data=[0.0001, 0.001, 0.01, 0.1, 1],\n",
       "               mask=[False, False, False, False, False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'param_clf__learning_rate_init': masked_array(data=[0.001, 0.001, 0.001, 0.001, 0.001],\n",
       "               mask=[False, False, False, False, False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'param_features__char': masked_array(data=[True, True, True, True, True],\n",
       "               mask=[False, False, False, False, False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'param_features__fnword': masked_array(data=[True, True, True, True, True],\n",
       "               mask=[False, False, False, False, False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'param_features__lexdiv': masked_array(data=[True, True, True, True, True],\n",
       "               mask=[False, False, False, False, False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'param_features__pos': masked_array(data=[False, False, False, False, False],\n",
       "               mask=[False, False, False, False, False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'param_features__word': masked_array(data=[True, True, True, True, True],\n",
       "               mask=[False, False, False, False, False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'param_sel__percentile': masked_array(data=[55, 55, 55, 55, 55],\n",
       "               mask=[False, False, False, False, False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'params': [{'clf__alpha': 0.0001,\n",
       "    'clf__learning_rate_init': 0.001,\n",
       "    'features__char': True,\n",
       "    'features__fnword': True,\n",
       "    'features__lexdiv': True,\n",
       "    'features__pos': False,\n",
       "    'features__word': True,\n",
       "    'sel__percentile': 55},\n",
       "   {'clf__alpha': 0.001,\n",
       "    'clf__learning_rate_init': 0.001,\n",
       "    'features__char': True,\n",
       "    'features__fnword': True,\n",
       "    'features__lexdiv': True,\n",
       "    'features__pos': False,\n",
       "    'features__word': True,\n",
       "    'sel__percentile': 55},\n",
       "   {'clf__alpha': 0.01,\n",
       "    'clf__learning_rate_init': 0.001,\n",
       "    'features__char': True,\n",
       "    'features__fnword': True,\n",
       "    'features__lexdiv': True,\n",
       "    'features__pos': False,\n",
       "    'features__word': True,\n",
       "    'sel__percentile': 55},\n",
       "   {'clf__alpha': 0.1,\n",
       "    'clf__learning_rate_init': 0.001,\n",
       "    'features__char': True,\n",
       "    'features__fnword': True,\n",
       "    'features__lexdiv': True,\n",
       "    'features__pos': False,\n",
       "    'features__word': True,\n",
       "    'sel__percentile': 55},\n",
       "   {'clf__alpha': 1,\n",
       "    'clf__learning_rate_init': 0.001,\n",
       "    'features__char': True,\n",
       "    'features__fnword': True,\n",
       "    'features__lexdiv': True,\n",
       "    'features__pos': False,\n",
       "    'features__word': True,\n",
       "    'sel__percentile': 55}],\n",
       "  'split0_test_score': array([0.9, 0.8, 0.8, 0.9, 0.9]),\n",
       "  'split1_test_score': array([1. , 1. , 1. , 0.9, 1. ]),\n",
       "  'split2_test_score': array([0.8, 0.8, 0.8, 0.8, 1. ]),\n",
       "  'split3_test_score': array([1., 1., 1., 1., 1.]),\n",
       "  'split4_test_score': array([1., 1., 1., 1., 1.]),\n",
       "  'mean_test_score': array([0.94, 0.92, 0.92, 0.92, 0.98]),\n",
       "  'std_test_score': array([0.08      , 0.09797959, 0.09797959, 0.07483315, 0.04      ]),\n",
       "  'rank_test_score': array([2, 3, 3, 3, 1], dtype=int32),\n",
       "  'split0_train_score': array([1., 1., 1., 1., 1.]),\n",
       "  'split1_train_score': array([1., 1., 1., 1., 1.]),\n",
       "  'split2_train_score': array([1., 1., 1., 1., 1.]),\n",
       "  'split3_train_score': array([1., 1., 1., 1., 1.]),\n",
       "  'split4_train_score': array([1., 1., 1., 1., 1.]),\n",
       "  'mean_train_score': array([1., 1., 1., 1., 1.]),\n",
       "  'std_train_score': array([0., 0., 0., 0., 0.])}]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def pipeline_perceptron(path, outpath):\n",
    "    infocollection = path+os.sep+'collection-info.json'\n",
    "    problems = []\n",
    "    language = []\n",
    "    with open(infocollection, 'r') as f:\n",
    "        for attrib in json.load(f):\n",
    "            problems.append(attrib['problem-name'])\n",
    "            language.append(attrib['language'])\n",
    "    scores_all = []\n",
    "    param_grids = [{\n",
    "        'features__char': [True],\n",
    "        'features__pos': [False],\n",
    "        'features__word': [False],\n",
    "        'features__lexdiv': [False],\n",
    "        'features__fnword': [True],\n",
    "        'sel__percentile': [55], # 0.59 score\n",
    "        'clf__alpha': [ 1], # .84 score\n",
    "        'clf__learning_rate_init': [ 0.001],\n",
    "        },\n",
    "        {\n",
    "        'features__char': [True],\n",
    "        'features__pos': [False],\n",
    "        'features__word': [True],\n",
    "        'features__lexdiv': [True],\n",
    "        'features__fnword': [True],\n",
    "        'sel__percentile': [55],  # 0.98 score\n",
    "        'clf__alpha': [ 1],\n",
    "        'clf__learning_rate_init': [0.001],\n",
    "        }\n",
    "    ]\n",
    "    for index,problem in enumerate(problems):\n",
    "        print(problem)\n",
    "        # Reading information about the problem\n",
    "        infoproblem = path+os.sep+problem+os.sep+'problem-info.json'\n",
    "        candidates = []\n",
    "        with open(infoproblem, 'r') as f:\n",
    "            fj = json.load(f)\n",
    "            unk_folder = fj['unknown-folder']\n",
    "            for attrib in fj['candidate-authors']:\n",
    "                candidates.append(attrib['author-name'])\n",
    "        # Building training set\n",
    "        docs=[]\n",
    "        for candidate in candidates:\n",
    "            docs.extend(read_files(path+os.sep+problem,candidate))\n",
    "        train_labels = np.array([label for i,(text,label) in enumerate(docs)])\n",
    "        print('\\t', 'language: ', language[index])\n",
    "        print('\\t', len(candidates), 'candidate authors')\n",
    "        print('\\t', len(docs), 'known texts')\n",
    "        \n",
    "        ###### Applying Classifiers #####\n",
    "        clf = MLPClassifier(random_state=442)\n",
    "        feat_extractor = Feature_Extractor()\n",
    "        sel = SelectPercentile(f_classif)\n",
    "        scaler = preprocessing.MaxAbsScaler()\n",
    "        pipe = Pipeline(steps=[('features', feat_extractor), ('sel', sel), \n",
    "                               ('scale', scaler), ('clf', clf)])\n",
    "        param_grid = param_grids[index]\n",
    "        search = GridSearchCV(pipe, param_grid, iid=False, cv=5, n_jobs=3, verbose=10)\n",
    "        search.fit(docs, train_labels)\n",
    "        print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "        print(search.best_params_)\n",
    "        scores_all.append(search.cv_results_)\n",
    "    return scores_all\n",
    "\n",
    "\n",
    "%timeit\n",
    "\n",
    "base_dir='pan18-cross-domain-authorship-attribution-training-dataset-2017-12-02'\n",
    "out_dir = base_dir+os.sep+'output-dir'\n",
    "eval_dir = base_dir+os.sep+'eval-dir'\n",
    "svm_results = pipeline_perceptron(base_dir,out_dir)\n",
    "svm_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_clf__alpha</th>\n",
       "      <th>param_clf__learning_rate_init</th>\n",
       "      <th>param_features__char</th>\n",
       "      <th>param_features__fnword</th>\n",
       "      <th>param_features__lexdiv</th>\n",
       "      <th>param_features__pos</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28.078432</td>\n",
       "      <td>3.083071</td>\n",
       "      <td>0.165577</td>\n",
       "      <td>0.063888</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.075166</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.067050</td>\n",
       "      <td>1.053957</td>\n",
       "      <td>0.134731</td>\n",
       "      <td>0.069236</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.224944</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.244196</td>\n",
       "      <td>0.783835</td>\n",
       "      <td>0.146655</td>\n",
       "      <td>0.060164</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.213073</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.473317</td>\n",
       "      <td>0.628904</td>\n",
       "      <td>0.132355</td>\n",
       "      <td>0.049283</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.207002</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.902105</td>\n",
       "      <td>0.698160</td>\n",
       "      <td>0.141424</td>\n",
       "      <td>0.029665</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.204939</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "4      28.078432      3.083071         0.165577        0.063888   \n",
       "1       5.067050      1.053957         0.134731        0.069236   \n",
       "0       5.244196      0.783835         0.146655        0.060164   \n",
       "3       4.473317      0.628904         0.132355        0.049283   \n",
       "2       4.902105      0.698160         0.141424        0.029665   \n",
       "\n",
       "  param_clf__alpha param_clf__learning_rate_init param_features__char  \\\n",
       "4                1                         0.001                 True   \n",
       "1            0.001                         0.001                 True   \n",
       "0           0.0001                         0.001                 True   \n",
       "3              0.1                         0.001                 True   \n",
       "2             0.01                         0.001                 True   \n",
       "\n",
       "  param_features__fnword param_features__lexdiv param_features__pos  ...  \\\n",
       "4                   True                  False               False  ...   \n",
       "1                   True                  False               False  ...   \n",
       "0                   True                  False               False  ...   \n",
       "3                   True                  False               False  ...   \n",
       "2                   True                  False               False  ...   \n",
       "\n",
       "  mean_test_score std_test_score rank_test_score  split0_train_score  \\\n",
       "4           0.840       0.075166               1                 1.0   \n",
       "1           0.595       0.224944               2                 1.0   \n",
       "0           0.590       0.213073               3                 1.0   \n",
       "3           0.580       0.207002               4                 1.0   \n",
       "2           0.575       0.204939               5                 1.0   \n",
       "\n",
       "   split1_train_score  split2_train_score  split3_train_score  \\\n",
       "4                 1.0                 1.0                 1.0   \n",
       "1                 1.0                 1.0                 1.0   \n",
       "0                 1.0                 1.0                 1.0   \n",
       "3                 1.0                 1.0                 1.0   \n",
       "2                 1.0                 1.0                 1.0   \n",
       "\n",
       "   split4_train_score  mean_train_score  std_train_score  \n",
       "4                 1.0               1.0              0.0  \n",
       "1                 1.0               1.0              0.0  \n",
       "0                 1.0               1.0              0.0  \n",
       "3                 1.0               1.0              0.0  \n",
       "2                 1.0               1.0              0.0  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(svm_results[0]).sort_values('mean_test_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_clf__alpha</th>\n",
       "      <th>param_clf__learning_rate_init</th>\n",
       "      <th>param_features__char</th>\n",
       "      <th>param_features__fnword</th>\n",
       "      <th>param_features__lexdiv</th>\n",
       "      <th>param_features__pos</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.723784</td>\n",
       "      <td>3.435787</td>\n",
       "      <td>0.094811</td>\n",
       "      <td>0.050832</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.870191</td>\n",
       "      <td>0.112287</td>\n",
       "      <td>0.077335</td>\n",
       "      <td>0.036382</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.857959</td>\n",
       "      <td>0.181203</td>\n",
       "      <td>0.085333</td>\n",
       "      <td>0.037960</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.097980</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.868897</td>\n",
       "      <td>0.166550</td>\n",
       "      <td>0.071390</td>\n",
       "      <td>0.028825</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.097980</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15.546641</td>\n",
       "      <td>0.960631</td>\n",
       "      <td>0.075501</td>\n",
       "      <td>0.036019</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.074833</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "4      13.723784      3.435787         0.094811        0.050832   \n",
       "0       1.870191      0.112287         0.077335        0.036382   \n",
       "1       1.857959      0.181203         0.085333        0.037960   \n",
       "2       1.868897      0.166550         0.071390        0.028825   \n",
       "3      15.546641      0.960631         0.075501        0.036019   \n",
       "\n",
       "  param_clf__alpha param_clf__learning_rate_init param_features__char  \\\n",
       "4                1                         0.001                 True   \n",
       "0           0.0001                         0.001                 True   \n",
       "1            0.001                         0.001                 True   \n",
       "2             0.01                         0.001                 True   \n",
       "3              0.1                         0.001                 True   \n",
       "\n",
       "  param_features__fnword param_features__lexdiv param_features__pos  ...  \\\n",
       "4                   True                   True               False  ...   \n",
       "0                   True                   True               False  ...   \n",
       "1                   True                   True               False  ...   \n",
       "2                   True                   True               False  ...   \n",
       "3                   True                   True               False  ...   \n",
       "\n",
       "  mean_test_score std_test_score rank_test_score  split0_train_score  \\\n",
       "4            0.98       0.040000               1                 1.0   \n",
       "0            0.94       0.080000               2                 1.0   \n",
       "1            0.92       0.097980               3                 1.0   \n",
       "2            0.92       0.097980               3                 1.0   \n",
       "3            0.92       0.074833               3                 1.0   \n",
       "\n",
       "   split1_train_score  split2_train_score  split3_train_score  \\\n",
       "4                 1.0                 1.0                 1.0   \n",
       "0                 1.0                 1.0                 1.0   \n",
       "1                 1.0                 1.0                 1.0   \n",
       "2                 1.0                 1.0                 1.0   \n",
       "3                 1.0                 1.0                 1.0   \n",
       "\n",
       "   split4_train_score  mean_train_score  std_train_score  \n",
       "4                 1.0               1.0              0.0  \n",
       "0                 1.0               1.0              0.0  \n",
       "1                 1.0               1.0              0.0  \n",
       "2                 1.0               1.0              0.0  \n",
       "3                 1.0               1.0              0.0  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(svm_results[1]).sort_values('mean_test_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(svm_results[0]).to_csv('problem1-clfparams_perceptron.csv')\n",
    "pd.DataFrame(svm_results[1]).to_csv('problem2-clfparams_perceptron.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.565 , 0.63  , 0.7925, 0.8225, 0.785 , 0.7975, 0.835 , 0.825 ])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_char_1 = pd.read_csv('problem1-charngrams.csv')\n",
    "df_char_1range = pd.read_csv('problem1-charngramsrange.csv')\n",
    "df_char_2 = pd.read_csv('problem2-charngrams.csv')\n",
    "df_char_2range = pd.read_csv('problem2-charngramsrange.csv')\n",
    "np.mean([pd.concat([df_char_1,df_char_1range])['mean_test_score'], pd.concat([df_char_2,df_char_2range])['mean_test_score']], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"{'features__char': True, 'features__fnword': False, 'features__ft': 3, 'features__lexdiv': False, 'features__n': 1, 'features__pos': False, 'features__word': False}\",\n",
       "       \"{'features__char': True, 'features__fnword': False, 'features__ft': 3, 'features__lexdiv': False, 'features__n': 2, 'features__pos': False, 'features__word': False}\",\n",
       "       \"{'features__char': True, 'features__fnword': False, 'features__ft': 3, 'features__lexdiv': False, 'features__n': 3, 'features__pos': False, 'features__word': False}\",\n",
       "       \"{'features__char': True, 'features__fnword': False, 'features__ft': 3, 'features__lexdiv': False, 'features__n': 4, 'features__pos': False, 'features__word': False}\",\n",
       "       \"{'features__char': True, 'features__fnword': False, 'features__ft': 3, 'features__lexdiv': False, 'features__n': 5, 'features__pos': False, 'features__word': False}\",\n",
       "       \"{'features__char': True, 'features__fnword': False, 'features__ft': 3, 'features__lexdiv': False, 'features__n': 3, 'features__pos': False, 'features__word': False}\",\n",
       "       \"{'features__char': True, 'features__fnword': False, 'features__ft': 3, 'features__lexdiv': False, 'features__n': 4, 'features__pos': False, 'features__word': False}\",\n",
       "       \"{'features__char': True, 'features__fnword': False, 'features__ft': 3, 'features__lexdiv': False, 'features__n': 5, 'features__pos': False, 'features__word': False}\"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([df_char_2,df_char_2range])['params'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem00001\n",
      "\t language:  en\n",
      "\t 20 candidate authors\n",
      "\t 140 known texts\n",
      "Testing fold  1\n",
      "(100, 52773)\n",
      "\t pos vocabulary size: 806 char vocabulary size: 52773\n",
      "[ 21.  25.   6.  37.  39.  52.  16.  48.  59.  49.  99.  85.  47.  55.\n",
      "  49.  50.  37.  31.  42.  61.  87.  31.  40.  24.  59.  46.  43.  68.\n",
      "  47.  85.  42. 120.  94. 110. 127. 107.  31.  59. 108.  95.  77.  80.\n",
      "  75. 118.  98.  32.  54.  46.  64.  46.  54.  47.  86.  52.  68.  78.\n",
      "  57.  77.  90.  77.  62.  54.  55.  74.  86.  55.  55.  34.  61.  28.\n",
      "  70.  57.  44.  40.  66.  16.  13.  42.  21.  68.  62.  42. 105.  84.\n",
      "  38.  92.  93.  91.  87. 107.  45.  88.  57. 103.  65.  67.  38.  49.\n",
      "  73.  76.]\n",
      "pos data: (100, 806) char data: (100, 52773) word data: (100, 116893)\n",
      "train shape: (100, 170473) test shape: (40, 170473)\n",
      "training before feature selection: (100, 170473)\n",
      "testing before feature selection: (40, 170473)\n",
      "training after feature selection: (100, 144902)\n",
      "testing after feature selection: (40, 144902)\n",
      "Fitting classifier with params: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [0 0 0 ... 0 0 0] are constant.\n",
      "  UserWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'problem': 'problem00001', 'accuracy': 0.175, 'precision': 0.16666666666666669, 'recall': 0.175, 'f1': 0.16333333333333333}\n",
      "Testing fold  2\n",
      "(100, 52929)\n",
      "\t pos vocabulary size: 807 char vocabulary size: 52929\n",
      "[ 63.  80.  74.  79.  72.  64.  76.  87.  74.  74. 101. 104. 103. 101.\n",
      "  95.  87.  79. 102. 103.  80.  77. 111. 115.  82. 111.  39.  45.  73.\n",
      "  73.  59.  92.  59.  73.  81.  76.  75.  72.  95.  85.  63.  66.  74.\n",
      "  89.  81.  67. 100.  75.  89.  78. 100.  79.  82.  73.  97.  71.  86.\n",
      "  85.  97.  87. 100. 100.  55.  62.  68.  74.  98.  84. 103.  74. 103.\n",
      "  98. 116.  62.  86.  78.  68.  73.  49.  69.  81.  58.  74.  75.  76.\n",
      "  63.  90. 101.  84.  94.  83.  80.  83.  61.  80.  88.  64.  78. 109.\n",
      "  71.  87.]\n",
      "pos data: (100, 807) char data: (100, 52929) word data: (100, 116940)\n",
      "train shape: (100, 170677) test shape: (40, 170677)\n",
      "training before feature selection: (100, 170677)\n",
      "testing before feature selection: (40, 170677)\n",
      "training after feature selection: (100, 144495)\n",
      "testing after feature selection: (40, 144495)\n",
      "Fitting classifier with params: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [0 0 0 ... 0 0 0] are constant.\n",
      "  UserWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'problem': 'problem00001', 'accuracy': 0.175, 'precision': 0.12944444444444442, 'recall': 0.175, 'f1': 0.13080086580086578}\n",
      "Testing fold  3\n",
      "(120, 58556)\n",
      "\t pos vocabulary size: 826 char vocabulary size: 58556\n",
      "[ 63.  80.  70.  71.  79.  72.  64.  76.  97.  98.  74.  74. 101. 104.\n",
      " 106. 112. 101.  95.  87.  79.  64.  69. 103.  80.  77. 111.  87.  85.\n",
      "  82. 111.  39.  45.  72.  61.  73.  59.  92.  59.  78.  64.  81.  76.\n",
      "  75.  72.  95.  67.  85.  63.  66.  74.  84.  73.  81.  67. 100.  75.\n",
      "  62.  49.  78. 100.  79.  82.  69.  89.  97.  71.  86.  85. 104.  99.\n",
      "  87. 100. 100.  55.  75.  67.  68.  74.  98.  84.  91.  96.  74. 103.\n",
      "  98. 116.  56.  77.  86.  78.  68.  73.  70.  76.  69.  81.  58.  74.\n",
      "  66.  87.  76.  63.  90. 101.  77.  79.  94.  83.  80.  83.  56.  85.\n",
      "  80.  88.  64.  78.  84.  95.  71.  87.]\n",
      "pos data: (120, 826) char data: (120, 58556) word data: (120, 137457)\n",
      "train shape: (120, 196840) test shape: (20, 196840)\n",
      "training before feature selection: (120, 196840)\n",
      "testing before feature selection: (20, 196840)\n",
      "training after feature selection: (120, 167314)\n",
      "testing after feature selection: (20, 167314)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [0 0 0 ... 0 0 0] are constant.\n",
      "  UserWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting classifier with params: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'problem': 'problem00001', 'accuracy': 0.25, 'precision': 0.13333333333333333, 'recall': 0.25, 'f1': 0.16666666666666666}\n",
      "Testing fold  4\n",
      "(120, 58489)\n",
      "\t pos vocabulary size: 831 char vocabulary size: 58489\n",
      "[ 63.  80.  70.  71.  74.  72.  64.  76.  97.  98.  87.  74. 101. 104.\n",
      " 106. 112. 103.  95.  87.  79.  64.  69. 102.  80.  77. 111.  87.  85.\n",
      " 115. 111.  39.  45.  72.  61.  73.  59.  92.  59.  78.  64.  73.  76.\n",
      "  75.  72.  95.  67.  95.  63.  66.  74.  84.  73.  89.  67. 100.  75.\n",
      "  62.  49.  89. 100.  79.  82.  69.  89.  73.  71.  86.  85. 104.  99.\n",
      "  97. 100. 100.  55.  75.  67.  62.  74.  98.  84.  91.  96. 103. 103.\n",
      "  98. 116.  56.  77.  62.  78.  68.  73.  70.  76.  49.  81.  58.  74.\n",
      "  66.  87.  75.  63.  90. 101.  77.  79.  84.  83.  80.  83.  56.  85.\n",
      "  61.  88.  64.  78.  84.  95. 109.  87.]\n",
      "pos data: (120, 831) char data: (120, 58489) word data: (120, 137690)\n",
      "train shape: (120, 197011) test shape: (20, 197011)\n",
      "training before feature selection: (120, 197011)\n",
      "testing before feature selection: (20, 197011)\n",
      "training after feature selection: (120, 167459)\n",
      "testing after feature selection: (20, 167459)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [0 0 0 ... 0 0 0] are constant.\n",
      "  UserWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting classifier with params: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'problem': 'problem00001', 'accuracy': 0.15, 'precision': 0.1, 'recall': 0.15, 'f1': 0.11666666666666665}\n",
      "Testing fold  5\n",
      "(120, 58686)\n",
      "\t pos vocabulary size: 835 char vocabulary size: 58686\n",
      "[ 63.  80.  70.  71.  74.  79.  64.  76.  97.  98.  87.  74. 101. 104.\n",
      " 106. 112. 103. 101.  87.  79.  64.  69. 102. 103.  77. 111.  87.  85.\n",
      " 115.  82.  39.  45.  72.  61.  73.  73.  92.  59.  78.  64.  73.  81.\n",
      "  75.  72.  95.  67.  95.  85.  66.  74.  84.  73.  89.  81. 100.  75.\n",
      "  62.  49.  89.  78.  79.  82.  69.  89.  73.  97.  86.  85. 104.  99.\n",
      "  97.  87. 100.  55.  75.  67.  62.  68.  98.  84.  91.  96. 103.  74.\n",
      "  98. 116.  56.  77.  62.  86.  68.  73.  70.  76.  49.  69.  58.  74.\n",
      "  66.  87.  75.  76.  90. 101.  77.  79.  84.  94.  80.  83.  56.  85.\n",
      "  61.  80.  64.  78.  84.  95. 109.  71.]\n",
      "pos data: (120, 835) char data: (120, 58686) word data: (120, 137858)\n",
      "train shape: (120, 197380) test shape: (20, 197380)\n",
      "training before feature selection: (120, 197380)\n",
      "testing before feature selection: (20, 197380)\n",
      "training after feature selection: (120, 167773)\n",
      "testing after feature selection: (20, 167773)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [0 0 0 ... 0 0 0] are constant.\n",
      "  UserWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting classifier with params: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'problem': 'problem00001', 'accuracy': 0.3, 'precision': 0.21000000000000002, 'recall': 0.3, 'f1': 0.23333333333333334}\n",
      "problem00002\n",
      "\t language:  en\n",
      "\t 5 candidate authors\n",
      "\t 35 known texts\n",
      "Testing fold  1\n",
      "(25, 20940)\n",
      "\t pos vocabulary size: 559 char vocabulary size: 20940\n",
      "[74. 71. 62. 51. 70. 36. 56. 62. 73. 68. 36. 43. 40. 38. 37. 79. 62. 54.\n",
      " 61. 53. 49. 65. 59. 84. 55.]\n",
      "pos data: (25, 559) char data: (25, 20940) word data: (25, 32487)\n",
      "train shape: (25, 53987) test shape: (10, 53987)\n",
      "training before feature selection: (25, 53987)\n",
      "testing before feature selection: (10, 53987)\n",
      "training after feature selection: (25, 45888)\n",
      "testing after feature selection: (10, 45888)\n",
      "Fitting classifier with params: {}\n",
      "{'problem': 'problem00002', 'accuracy': 0.5, 'precision': 0.6333333333333333, 'recall': 0.5, 'f1': 0.5266666666666666}\n",
      "Testing fold  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [0 0 0 ... 0 0 0] are constant.\n",
      "  UserWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 21107)\n",
      "\t pos vocabulary size: 573 char vocabulary size: 21107\n",
      "[29. 40. 41. 37. 29. 12. 28. 11. 25. 12. 40. 17. 43. 16. 31. 33. 18. 31.\n",
      " 38. 47. 28. 25. 18. 39. 24.]\n",
      "pos data: (25, 573) char data: (25, 21107) word data: (25, 32519)\n",
      "train shape: (25, 54200) test shape: (10, 54200)\n",
      "training before feature selection: (25, 54200)\n",
      "testing before feature selection: (10, 54200)\n",
      "training after feature selection: (25, 46070)\n",
      "testing after feature selection: (10, 46070)\n",
      "Fitting classifier with params: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [0 0 0 ... 0 0 0] are constant.\n",
      "  UserWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'problem': 'problem00002', 'accuracy': 0.7, 'precision': 0.6, 'recall': 0.7, 'f1': 0.6333333333333333}\n",
      "Testing fold  3\n",
      "(30, 24171)\n",
      "\t pos vocabulary size: 596 char vocabulary size: 24171\n",
      "[29. 40. 36. 24. 37. 29. 12. 28. 19. 12. 25. 12. 40. 17. 39. 13. 16. 31.\n",
      " 33. 18. 40. 43. 38. 47. 28. 25. 20. 33. 39. 24.]\n",
      "pos data: (30, 596) char data: (30, 24171) word data: (30, 38451)\n",
      "train shape: (30, 63219) test shape: (5, 63219)\n",
      "training before feature selection: (30, 63219)\n",
      "testing before feature selection: (5, 63219)\n",
      "training after feature selection: (30, 49363)\n",
      "testing after feature selection: (5, 49363)\n",
      "Fitting classifier with params: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [0 0 0 ... 0 0 0] are constant.\n",
      "  UserWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'problem': 'problem00002', 'accuracy': 0.6, 'precision': 0.6, 'recall': 0.6, 'f1': 0.6}\n",
      "Testing fold  4\n",
      "(30, 23884)\n",
      "\t pos vocabulary size: 592 char vocabulary size: 23884\n",
      "[29. 40. 36. 24. 41. 29. 12. 28. 19. 12. 11. 12. 40. 17. 39. 13. 43. 31.\n",
      " 33. 18. 40. 43. 31. 47. 28. 25. 20. 33. 18. 24.]\n",
      "pos data: (30, 592) char data: (30, 23884) word data: (30, 38443)\n",
      "train shape: (30, 62920) test shape: (5, 62920)\n",
      "training before feature selection: (30, 62920)\n",
      "testing before feature selection: (5, 62920)\n",
      "training after feature selection: (30, 53482)\n",
      "testing after feature selection: (5, 53482)\n",
      "Fitting classifier with params: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [0 0 0 ... 0 0 0] are constant.\n",
      "  UserWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'problem': 'problem00002', 'accuracy': 0.6, 'precision': 0.4666666666666667, 'recall': 0.6, 'f1': 0.5}\n",
      "Testing fold  5\n",
      "(30, 24084)\n",
      "\t pos vocabulary size: 601 char vocabulary size: 24084\n",
      "[29. 40. 36. 24. 41. 37. 12. 28. 19. 12. 11. 25. 40. 17. 39. 13. 43. 16.\n",
      " 33. 18. 40. 43. 31. 38. 28. 25. 20. 33. 18. 39.]\n",
      "pos data: (30, 601) char data: (30, 24084) word data: (30, 38670)\n",
      "train shape: (30, 63356) test shape: (5, 63356)\n",
      "training before feature selection: (30, 63356)\n",
      "testing before feature selection: (5, 63356)\n",
      "training after feature selection: (30, 53852)\n",
      "testing after feature selection: (5, 53852)\n",
      "Fitting classifier with params: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [0 0 0 ... 0 0 0] are constant.\n",
      "  UserWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'problem': 'problem00002', 'accuracy': 0.6, 'precision': 0.6, 'recall': 0.6, 'f1': 0.6}\n",
      "elapsed time: 527.2663791179657\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>problem</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.175</td>\n",
       "      <td>0.163333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>problem00001</td>\n",
       "      <td>0.175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.175</td>\n",
       "      <td>0.130801</td>\n",
       "      <td>0.129444</td>\n",
       "      <td>problem00001</td>\n",
       "      <td>0.175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.250</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>problem00001</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.150</td>\n",
       "      <td>0.116667</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>problem00001</td>\n",
       "      <td>0.150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.300</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>problem00001</td>\n",
       "      <td>0.300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.500</td>\n",
       "      <td>0.526667</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>problem00002</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.700</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>problem00002</td>\n",
       "      <td>0.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.600</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>problem00002</td>\n",
       "      <td>0.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.600</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>problem00002</td>\n",
       "      <td>0.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.600</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>problem00002</td>\n",
       "      <td>0.600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy        f1  precision       problem  recall\n",
       "0     0.175  0.163333   0.166667  problem00001   0.175\n",
       "1     0.175  0.130801   0.129444  problem00001   0.175\n",
       "2     0.250  0.166667   0.133333  problem00001   0.250\n",
       "3     0.150  0.116667   0.100000  problem00001   0.150\n",
       "4     0.300  0.233333   0.210000  problem00001   0.300\n",
       "5     0.500  0.526667   0.633333  problem00002   0.500\n",
       "6     0.700  0.633333   0.600000  problem00002   0.700\n",
       "7     0.600  0.600000   0.600000  problem00002   0.600\n",
       "8     0.600  0.500000   0.466667  problem00002   0.600\n",
       "9     0.600  0.600000   0.600000  problem00002   0.600"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def baseline_crossval(path, outpath, n=3, ft=5, feature_selection=False, \n",
    "             open_set=False, c=1, feat_sel_percent=None, classif=None, calibration=True, clf_params=[]):\n",
    "    start_time = time.time()\n",
    "    # Reading information about the collection\n",
    "    infocollection = path+os.sep+'collection-info.json'\n",
    "    problems = []\n",
    "    language = []\n",
    "    with open(infocollection, 'r') as f:\n",
    "        for attrib in json.load(f):\n",
    "            problems.append(attrib['problem-name'])\n",
    "            language.append(attrib['language'])\n",
    "    problem_scores = []\n",
    "    scores_all = []\n",
    "    for index,problem in enumerate(problems):\n",
    "        print(problem)\n",
    "        # Reading information about the problem\n",
    "        infoproblem = path+os.sep+problem+os.sep+'problem-info.json'\n",
    "        candidates = []\n",
    "        with open(infoproblem, 'r') as f:\n",
    "            fj = json.load(f)\n",
    "            unk_folder = fj['unknown-folder']\n",
    "            for attrib in fj['candidate-authors']:\n",
    "                candidates.append(attrib['author-name'])\n",
    "        # Building training set\n",
    "        docs=[]\n",
    "        for candidate in candidates:\n",
    "            docs.extend(read_files(path+os.sep+problem,candidate))\n",
    "        train_labels = np.array([label for i,(text,label) in enumerate(docs)])\n",
    "        print('\\t', 'language: ', language[index])\n",
    "        print('\\t', len(candidates), 'candidate authors')\n",
    "        print('\\t', len(docs), 'known texts')\n",
    "        \n",
    "        ###### Applying Classifiers #####\n",
    "        skf = StratifiedKFold(n_splits=5,random_state=442)\n",
    "        scores = []\n",
    "        k = 1\n",
    "        for train_index, test_index in skf.split(docs, train_labels):\n",
    "            print('Testing fold ', k)\n",
    "            k +=1\n",
    "            X_train_docs = [docs[i] for i in train_index]\n",
    "            X_test_docs = [docs[i] for i in test_index]\n",
    "            y_train, y_test = train_labels[train_index], train_labels[test_index]\n",
    "            feat_extractor = Feature_Extractor(n, ft)\n",
    "            X_train = feat_extractor.fit_transform(X_train_docs)\n",
    "            X_test = feat_extractor.transform(X_test_docs)\n",
    "            print('train shape:', X_train.shape, 'test shape:', X_test.shape)\n",
    "            if feature_selection is True:\n",
    "                ####### Feature Selection - Fit #######\n",
    "                print(\"training before feature selection:\", X_train.shape)\n",
    "                print(\"testing before feature selection:\", X_test.shape)\n",
    "                #sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "                #train_data = sel.fit_transform(train_data)\n",
    "                sel = SelectPercentile(f_classif, percentile=feat_sel_percent)\n",
    "                X_train = sel.fit_transform(X_train, y_train)\n",
    "                X_test = sel.transform(X_test)\n",
    "                #sel = SelectKBest(chi2, k=100000)\n",
    "                #train_data = sel.fit_transform(train_data, train_labels)\n",
    "                print(\"training after feature selection:\", X_train.shape)\n",
    "                print(\"testing after feature selection:\", X_test.shape)\n",
    "            max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "            X_train = max_abs_scaler.fit_transform(X_train)\n",
    "            X_test = max_abs_scaler.transform(X_test)\n",
    "            for c_params in clf_params:\n",
    "                print('Fitting classifier with params:', c_params)\n",
    "                if calibration is True:\n",
    "                    clf=CalibratedClassifierCV(OneVsRestClassifier(classif(**c_params)))\n",
    "                else:\n",
    "                    clf=OneVsRestClassifier(classif(**clf_params))\n",
    "                clf.fit(X_train, y_train)\n",
    "                y_pred = clf.predict(X_test)\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                precision = precision_score(y_test, y_pred, average='macro')\n",
    "                recall = recall_score(y_test, y_pred, average='macro')\n",
    "                f1 = f1_score(y_test, y_pred, average='macro')\n",
    "                fold_scores = {'problem': problem, 'accuracy': accuracy, 'precision': precision, 'recall': recall, \n",
    "                               'f1': f1, **c_params}\n",
    "                print(fold_scores)\n",
    "                scores.append(fold_scores)\n",
    "                #write_results(path, problem, unk_folder, y_pref)\n",
    "        '''\n",
    "        mean_accuracy = np.mean([x[0] for x in scores])\n",
    "        print(problem, 'MEAN ACCURACY SCORES:', mean_accuracy)\n",
    "        mean_prec = np.mean([x[1] for x in scores])\n",
    "        print(problem, 'MEAN PRECISION SCORES:', mean_prec)\n",
    "        mean_recall = np.mean([x[2] for x in scores])\n",
    "        print(problem, 'MEAN RECALL SCORES:', mean_recall)\n",
    "        mean_f1 = np.mean([x[3] for x in scores])\n",
    "        print(problem, 'MEAN F1 SCORES:', mean_f1)\n",
    "        problem_scores.append(mean_accuracy)\n",
    "        '''\n",
    "        scores_all.extend(scores)\n",
    "    #print('MEAN SCORES ACROSS PROBLEMS:', np.mean(problem_scores))\n",
    "    # todo: also add stdev of scores\n",
    "    print('elapsed time:', time.time() - start_time)\n",
    "    return pd.DataFrame(scores_all)\n",
    "\n",
    "base_dir='pan18-cross-domain-authorship-attribution-training-dataset-2017-12-02'\n",
    "out_dir = base_dir+os.sep+'output-dir'\n",
    "eval_dir = base_dir+os.sep+'eval-dir'\n",
    "#params = {'n': 5,'ft': 3, 'feature_selection': True, 'clf_params': [{'C': 0.1},{'C': 1}], 'feat_sel_percent': 85, 'classif': SVC, 'calibration': True}\n",
    "params = {'n': 5,'ft': 3, 'feature_selection': True, 'clf_params': [{}], 'feat_sel_percent': 85, 'classif': AdaBoostClassifier, 'calibration': True}\n",
    "\n",
    "\n",
    "#params = {'n': 5,'ft': 3,'pt': 0.05,'feature_selection': True, 'clf_params': [{'C': 0.1, 'solver': 'lbfgs','multi_class': 'multinomial','max_iter': 500}], 'feat_sel_percent': 85, 'classif': LogisticRegression, 'calibration': True}\n",
    "\n",
    "#params = {'n': 5,'ft': 3,'pt': 0.05,'feature_selection': True, 'clf_params': {'hidden_layer_sizes':(50,), 'max_iter':10, 'alpha':1e-4,\n",
    "#                    'solver':'sgd', 'verbose':10, 'tol':1e-4, 'random_state':1, 'learning_rate_init':.1},\n",
    "#          'feat_sel_percent': 85, 'classif': MLPClassifier, 'calibration': True}\n",
    "#params = {'n': 5,'ft': 3,'pt': 0.05,'feature_selection': True, 'clf_params': {'hidden_layer_sizes':(1,), 'random_state':1, 'learning_rate_init':.1},\n",
    "#          'feat_sel_percent': 85, 'classif': MLPClassifier, 'calibration': True}\n",
    "%timeit\n",
    "df = baseline_crossval(base_dir,out_dir,**params)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "problem\n",
       "problem00001    0.16216\n",
       "problem00002    0.57200\n",
       "Name: f1, dtype: float64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('problem')['f1'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem00001\n",
      "\t language:  en\n",
      "\t 20 candidate authors\n",
      "\t 140 known texts\n",
      "Do cross validation.\n",
      "(100, 52773)\n",
      "\t pos vocabulary size: 806 char vocabulary size: 52773\n",
      "lexical diversity: (100, 1)\n",
      "[ 21.  25.   6.  37.  39.  52.  16.  48.  59.  49.  99.  85.  47.  55.\n",
      "  49.  50.  37.  31.  42.  61.  87.  31.  40.  24.  59.  46.  43.  68.\n",
      "  47.  85.  42. 120.  94. 110. 127. 107.  31.  59. 108.  95.  77.  80.\n",
      "  75. 118.  98.  32.  54.  46.  64.  46.  54.  47.  86.  52.  68.  78.\n",
      "  57.  77.  90.  77.  62.  54.  55.  74.  86.  55.  55.  34.  61.  28.\n",
      "  70.  57.  44.  40.  66.  16.  13.  42.  21.  68.  62.  42. 105.  84.\n",
      "  38.  92.  93.  91.  87. 107.  45.  88.  57. 103.  65.  67.  38.  49.\n",
      "  73.  76.]\n",
      "pos data: (100, 806) char data: (100, 52773) word data: (100, 116893)\n",
      "lexical diversity: (40, 1)\n",
      "train shape: (100, 170473) test shape: (40, 170473)\n",
      "training before feature selection: (100, 170473)\n",
      "testing before feature selection: (40, 170473)\n",
      "training after feature selection: (100, 144902)\n",
      "testing after feature selection: (40, 144902)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [0 0 0 ... 0 0 0] are constant.\n",
      "  UserWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred y_test: (40,) (40,)\n",
      "['candidate00013' 'candidate00013' 'candidate00012' 'candidate00013'\n",
      " 'candidate00013' 'candidate00013' 'candidate00013' 'candidate00013'\n",
      " 'candidate00013' 'candidate00013' 'candidate00012' 'candidate00013'\n",
      " 'candidate00013' 'candidate00013' 'candidate00013' 'candidate00013'\n",
      " 'candidate00012' 'candidate00012' 'candidate00013' 'candidate00013'\n",
      " 'candidate00013' 'candidate00013' 'candidate00013' 'candidate00013'\n",
      " 'candidate00013' 'candidate00013' 'candidate00013' 'candidate00013'\n",
      " 'candidate00013' 'candidate00013' 'candidate00013' 'candidate00013'\n",
      " 'candidate00013' 'candidate00013' 'candidate00012' 'candidate00013'\n",
      " 'candidate00013' 'candidate00013' 'candidate00012' 'candidate00013']\n",
      "['candidate00001' 'candidate00001' 'candidate00002' 'candidate00002'\n",
      " 'candidate00003' 'candidate00003' 'candidate00004' 'candidate00004'\n",
      " 'candidate00005' 'candidate00005' 'candidate00006' 'candidate00006'\n",
      " 'candidate00007' 'candidate00007' 'candidate00008' 'candidate00008'\n",
      " 'candidate00009' 'candidate00009' 'candidate00010' 'candidate00010'\n",
      " 'candidate00011' 'candidate00011' 'candidate00012' 'candidate00012'\n",
      " 'candidate00013' 'candidate00013' 'candidate00014' 'candidate00014'\n",
      " 'candidate00015' 'candidate00015' 'candidate00016' 'candidate00016'\n",
      " 'candidate00017' 'candidate00017' 'candidate00018' 'candidate00018'\n",
      " 'candidate00019' 'candidate00019' 'candidate00020' 'candidate00020']\n",
      "accuracy: 0.05\n",
      "(100, 52929)\n",
      "\t pos vocabulary size: 807 char vocabulary size: 52929\n",
      "lexical diversity: (100, 1)\n",
      "[ 63.  80.  74.  79.  72.  64.  76.  87.  74.  74. 101. 104. 103. 101.\n",
      "  95.  87.  79. 102. 103.  80.  77. 111. 115.  82. 111.  39.  45.  73.\n",
      "  73.  59.  92.  59.  73.  81.  76.  75.  72.  95.  85.  63.  66.  74.\n",
      "  89.  81.  67. 100.  75.  89.  78. 100.  79.  82.  73.  97.  71.  86.\n",
      "  85.  97.  87. 100. 100.  55.  62.  68.  74.  98.  84. 103.  74. 103.\n",
      "  98. 116.  62.  86.  78.  68.  73.  49.  69.  81.  58.  74.  75.  76.\n",
      "  63.  90. 101.  84.  94.  83.  80.  83.  61.  80.  88.  64.  78. 109.\n",
      "  71.  87.]\n",
      "pos data: (100, 807) char data: (100, 52929) word data: (100, 116940)\n",
      "lexical diversity: (40, 1)\n",
      "train shape: (100, 170677) test shape: (40, 170677)\n",
      "training before feature selection: (100, 170677)\n",
      "testing before feature selection: (40, 170677)\n",
      "training after feature selection: (100, 144495)\n",
      "testing after feature selection: (40, 144495)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [0 0 0 ... 0 0 0] are constant.\n",
      "  UserWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred y_test: (40,) (40,)\n",
      "['candidate00005' 'candidate00005' 'candidate00005' 'candidate00005'\n",
      " 'candidate00005' 'candidate00005' 'candidate00005' 'candidate00005'\n",
      " 'candidate00005' 'candidate00005' 'candidate00005' 'candidate00005'\n",
      " 'candidate00005' 'candidate00005' 'candidate00014' 'candidate00005'\n",
      " 'candidate00005' 'candidate00005' 'candidate00005' 'candidate00005'\n",
      " 'candidate00005' 'candidate00005' 'candidate00005' 'candidate00005'\n",
      " 'candidate00005' 'candidate00005' 'candidate00005' 'candidate00005'\n",
      " 'candidate00005' 'candidate00005' 'candidate00005' 'candidate00005'\n",
      " 'candidate00005' 'candidate00005' 'candidate00005' 'candidate00005'\n",
      " 'candidate00005' 'candidate00005' 'candidate00005' 'candidate00005']\n",
      "['candidate00001' 'candidate00001' 'candidate00002' 'candidate00002'\n",
      " 'candidate00003' 'candidate00003' 'candidate00004' 'candidate00004'\n",
      " 'candidate00005' 'candidate00005' 'candidate00006' 'candidate00006'\n",
      " 'candidate00007' 'candidate00007' 'candidate00008' 'candidate00008'\n",
      " 'candidate00009' 'candidate00009' 'candidate00010' 'candidate00010'\n",
      " 'candidate00011' 'candidate00011' 'candidate00012' 'candidate00012'\n",
      " 'candidate00013' 'candidate00013' 'candidate00014' 'candidate00014'\n",
      " 'candidate00015' 'candidate00015' 'candidate00016' 'candidate00016'\n",
      " 'candidate00017' 'candidate00017' 'candidate00018' 'candidate00018'\n",
      " 'candidate00019' 'candidate00019' 'candidate00020' 'candidate00020']\n",
      "accuracy: 0.05\n",
      "(120, 58556)\n",
      "\t pos vocabulary size: 826 char vocabulary size: 58556\n",
      "lexical diversity: (120, 1)\n",
      "[ 63.  80.  70.  71.  79.  72.  64.  76.  97.  98.  74.  74. 101. 104.\n",
      " 106. 112. 101.  95.  87.  79.  64.  69. 103.  80.  77. 111.  87.  85.\n",
      "  82. 111.  39.  45.  72.  61.  73.  59.  92.  59.  78.  64.  81.  76.\n",
      "  75.  72.  95.  67.  85.  63.  66.  74.  84.  73.  81.  67. 100.  75.\n",
      "  62.  49.  78. 100.  79.  82.  69.  89.  97.  71.  86.  85. 104.  99.\n",
      "  87. 100. 100.  55.  75.  67.  68.  74.  98.  84.  91.  96.  74. 103.\n",
      "  98. 116.  56.  77.  86.  78.  68.  73.  70.  76.  69.  81.  58.  74.\n",
      "  66.  87.  76.  63.  90. 101.  77.  79.  94.  83.  80.  83.  56.  85.\n",
      "  80.  88.  64.  78.  84.  95.  71.  87.]\n",
      "pos data: (120, 826) char data: (120, 58556) word data: (120, 137457)\n",
      "lexical diversity: (20, 1)\n",
      "train shape: (120, 196840) test shape: (20, 196840)\n",
      "training before feature selection: (120, 196840)\n",
      "testing before feature selection: (20, 196840)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [0 0 0 ... 0 0 0] are constant.\n",
      "  UserWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training after feature selection: (120, 167314)\n",
      "testing after feature selection: (20, 167314)\n",
      "y_pred y_test: (20,) (20,)\n",
      "['candidate00018' 'candidate00017' 'candidate00020' 'candidate00017'\n",
      " 'candidate00018' 'candidate00020' 'candidate00017' 'candidate00018'\n",
      " 'candidate00017' 'candidate00010' 'candidate00017' 'candidate00020'\n",
      " 'candidate00017' 'candidate00018' 'candidate00017' 'candidate00013'\n",
      " 'candidate00020' 'candidate00018' 'candidate00017' 'candidate00018']\n",
      "['candidate00001' 'candidate00002' 'candidate00003' 'candidate00004'\n",
      " 'candidate00005' 'candidate00006' 'candidate00007' 'candidate00008'\n",
      " 'candidate00009' 'candidate00010' 'candidate00011' 'candidate00012'\n",
      " 'candidate00013' 'candidate00014' 'candidate00015' 'candidate00016'\n",
      " 'candidate00017' 'candidate00018' 'candidate00019' 'candidate00020']\n",
      "accuracy: 0.1\n",
      "(120, 58489)\n",
      "\t pos vocabulary size: 831 char vocabulary size: 58489\n",
      "lexical diversity: (120, 1)\n",
      "[ 63.  80.  70.  71.  74.  72.  64.  76.  97.  98.  87.  74. 101. 104.\n",
      " 106. 112. 103.  95.  87.  79.  64.  69. 102.  80.  77. 111.  87.  85.\n",
      " 115. 111.  39.  45.  72.  61.  73.  59.  92.  59.  78.  64.  73.  76.\n",
      "  75.  72.  95.  67.  95.  63.  66.  74.  84.  73.  89.  67. 100.  75.\n",
      "  62.  49.  89. 100.  79.  82.  69.  89.  73.  71.  86.  85. 104.  99.\n",
      "  97. 100. 100.  55.  75.  67.  62.  74.  98.  84.  91.  96. 103. 103.\n",
      "  98. 116.  56.  77.  62.  78.  68.  73.  70.  76.  49.  81.  58.  74.\n",
      "  66.  87.  75.  63.  90. 101.  77.  79.  84.  83.  80.  83.  56.  85.\n",
      "  61.  88.  64.  78.  84.  95. 109.  87.]\n",
      "pos data: (120, 831) char data: (120, 58489) word data: (120, 137690)\n",
      "lexical diversity: (20, 1)\n",
      "train shape: (120, 197011) test shape: (20, 197011)\n",
      "training before feature selection: (120, 197011)\n",
      "testing before feature selection: (20, 197011)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [0 0 0 ... 0 0 0] are constant.\n",
      "  UserWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training after feature selection: (120, 167459)\n",
      "testing after feature selection: (20, 167459)\n",
      "y_pred y_test: (20,) (20,)\n",
      "['candidate00020' 'candidate00016' 'candidate00020' 'candidate00020'\n",
      " 'candidate00016' 'candidate00020' 'candidate00020' 'candidate00020'\n",
      " 'candidate00020' 'candidate00016' 'candidate00015' 'candidate00016'\n",
      " 'candidate00020' 'candidate00020' 'candidate00020' 'candidate00020'\n",
      " 'candidate00020' 'candidate00016' 'candidate00016' 'candidate00016']\n",
      "['candidate00001' 'candidate00002' 'candidate00003' 'candidate00004'\n",
      " 'candidate00005' 'candidate00006' 'candidate00007' 'candidate00008'\n",
      " 'candidate00009' 'candidate00010' 'candidate00011' 'candidate00012'\n",
      " 'candidate00013' 'candidate00014' 'candidate00015' 'candidate00016'\n",
      " 'candidate00017' 'candidate00018' 'candidate00019' 'candidate00020']\n",
      "accuracy: 0.0\n",
      "(120, 58686)\n",
      "\t pos vocabulary size: 835 char vocabulary size: 58686\n",
      "lexical diversity: (120, 1)\n",
      "[ 63.  80.  70.  71.  74.  79.  64.  76.  97.  98.  87.  74. 101. 104.\n",
      " 106. 112. 103. 101.  87.  79.  64.  69. 102. 103.  77. 111.  87.  85.\n",
      " 115.  82.  39.  45.  72.  61.  73.  73.  92.  59.  78.  64.  73.  81.\n",
      "  75.  72.  95.  67.  95.  85.  66.  74.  84.  73.  89.  81. 100.  75.\n",
      "  62.  49.  89.  78.  79.  82.  69.  89.  73.  97.  86.  85. 104.  99.\n",
      "  97.  87. 100.  55.  75.  67.  62.  68.  98.  84.  91.  96. 103.  74.\n",
      "  98. 116.  56.  77.  62.  86.  68.  73.  70.  76.  49.  69.  58.  74.\n",
      "  66.  87.  75.  76.  90. 101.  77.  79.  84.  94.  80.  83.  56.  85.\n",
      "  61.  80.  64.  78.  84.  95. 109.  71.]\n",
      "pos data: (120, 835) char data: (120, 58686) word data: (120, 137858)\n",
      "lexical diversity: (20, 1)\n",
      "train shape: (120, 197380) test shape: (20, 197380)\n",
      "training before feature selection: (120, 197380)\n",
      "testing before feature selection: (20, 197380)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [0 0 0 ... 0 0 0] are constant.\n",
      "  UserWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training after feature selection: (120, 167773)\n",
      "testing after feature selection: (20, 167773)\n",
      "y_pred y_test: (20,) (20,)\n",
      "['candidate00004' 'candidate00004' 'candidate00004' 'candidate00014'\n",
      " 'candidate00014' 'candidate00014' 'candidate00004' 'candidate00004'\n",
      " 'candidate00009' 'candidate00004' 'candidate00004' 'candidate00004'\n",
      " 'candidate00014' 'candidate00004' 'candidate00014' 'candidate00004'\n",
      " 'candidate00004' 'candidate00004' 'candidate00004' 'candidate00004']\n",
      "['candidate00001' 'candidate00002' 'candidate00003' 'candidate00004'\n",
      " 'candidate00005' 'candidate00006' 'candidate00007' 'candidate00008'\n",
      " 'candidate00009' 'candidate00010' 'candidate00011' 'candidate00012'\n",
      " 'candidate00013' 'candidate00014' 'candidate00015' 'candidate00016'\n",
      " 'candidate00017' 'candidate00018' 'candidate00019' 'candidate00020']\n",
      "accuracy: 0.05\n",
      "problem00001 MEAN ACCURACY SCORES: 0.05\n",
      "problem00002\n",
      "\t language:  en\n",
      "\t 5 candidate authors\n",
      "\t 35 known texts\n",
      "Do cross validation.\n",
      "(25, 20940)\n",
      "\t pos vocabulary size: 559 char vocabulary size: 20940\n",
      "lexical diversity: (25, 1)\n",
      "[74. 71. 62. 51. 70. 36. 56. 62. 73. 68. 36. 43. 40. 38. 37. 79. 62. 54.\n",
      " 61. 53. 49. 65. 59. 84. 55.]\n",
      "pos data: (25, 559) char data: (25, 20940) word data: (25, 32487)\n",
      "lexical diversity: (10, 1)\n",
      "train shape: (25, 53987) test shape: (10, 53987)\n",
      "training before feature selection: (25, 53987)\n",
      "testing before feature selection: (10, 53987)\n",
      "training after feature selection: (25, 45888)\n",
      "testing after feature selection: (10, 45888)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [0 0 0 ... 0 0 0] are constant.\n",
      "  UserWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred y_test: (10,) (10,)\n",
      "['candidate00004' 'candidate00004' 'candidate00002' 'candidate00005'\n",
      " 'candidate00005' 'candidate00005' 'candidate00004' 'candidate00005'\n",
      " 'candidate00005' 'candidate00005']\n",
      "['candidate00001' 'candidate00001' 'candidate00002' 'candidate00002'\n",
      " 'candidate00003' 'candidate00003' 'candidate00004' 'candidate00004'\n",
      " 'candidate00005' 'candidate00005']\n",
      "accuracy: 0.4\n",
      "(25, 21107)\n",
      "\t pos vocabulary size: 573 char vocabulary size: 21107\n",
      "lexical diversity: (25, 1)\n",
      "[29. 40. 41. 37. 29. 12. 28. 11. 25. 12. 40. 17. 43. 16. 31. 33. 18. 31.\n",
      " 38. 47. 28. 25. 18. 39. 24.]\n",
      "pos data: (25, 573) char data: (25, 21107) word data: (25, 32519)\n",
      "lexical diversity: (10, 1)\n",
      "train shape: (25, 54200) test shape: (10, 54200)\n",
      "training before feature selection: (25, 54200)\n",
      "testing before feature selection: (10, 54200)\n",
      "training after feature selection: (25, 46070)\n",
      "testing after feature selection: (10, 46070)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [0 0 0 ... 0 0 0] are constant.\n",
      "  UserWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred y_test: (10,) (10,)\n",
      "['candidate00001' 'candidate00001' 'candidate00005' 'candidate00004'\n",
      " 'candidate00005' 'candidate00004' 'candidate00005' 'candidate00005'\n",
      " 'candidate00005' 'candidate00005']\n",
      "['candidate00001' 'candidate00001' 'candidate00002' 'candidate00002'\n",
      " 'candidate00003' 'candidate00003' 'candidate00004' 'candidate00004'\n",
      " 'candidate00005' 'candidate00005']\n",
      "accuracy: 0.4\n",
      "(30, 24171)\n",
      "\t pos vocabulary size: 596 char vocabulary size: 24171\n",
      "lexical diversity: (30, 1)\n",
      "[29. 40. 36. 24. 37. 29. 12. 28. 19. 12. 25. 12. 40. 17. 39. 13. 16. 31.\n",
      " 33. 18. 40. 43. 38. 47. 28. 25. 20. 33. 39. 24.]\n",
      "pos data: (30, 596) char data: (30, 24171) word data: (30, 38451)\n",
      "lexical diversity: (5, 1)\n",
      "train shape: (30, 63219) test shape: (5, 63219)\n",
      "training before feature selection: (30, 63219)\n",
      "testing before feature selection: (5, 63219)\n",
      "training after feature selection: (30, 49363)\n",
      "testing after feature selection: (5, 49363)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [0 0 0 ... 0 0 0] are constant.\n",
      "  UserWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred y_test: (5,) (5,)\n",
      "['candidate00001' 'candidate00005' 'candidate00003' 'candidate00001'\n",
      " 'candidate00005']\n",
      "['candidate00001' 'candidate00002' 'candidate00003' 'candidate00004'\n",
      " 'candidate00005']\n",
      "accuracy: 0.6\n",
      "(30, 23884)\n",
      "\t pos vocabulary size: 592 char vocabulary size: 23884\n",
      "lexical diversity: (30, 1)\n",
      "[29. 40. 36. 24. 41. 29. 12. 28. 19. 12. 11. 12. 40. 17. 39. 13. 43. 31.\n",
      " 33. 18. 40. 43. 31. 47. 28. 25. 20. 33. 18. 24.]\n",
      "pos data: (30, 592) char data: (30, 23884) word data: (30, 38443)\n",
      "lexical diversity: (5, 1)\n",
      "train shape: (30, 62920) test shape: (5, 62920)\n",
      "training before feature selection: (30, 62920)\n",
      "testing before feature selection: (5, 62920)\n",
      "training after feature selection: (30, 53482)\n",
      "testing after feature selection: (5, 53482)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [0 0 0 ... 0 0 0] are constant.\n",
      "  UserWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred y_test: (5,) (5,)\n",
      "['candidate00004' 'candidate00004' 'candidate00004' 'candidate00004'\n",
      " 'candidate00004']\n",
      "['candidate00001' 'candidate00002' 'candidate00003' 'candidate00004'\n",
      " 'candidate00005']\n",
      "accuracy: 0.2\n",
      "(30, 24084)\n",
      "\t pos vocabulary size: 601 char vocabulary size: 24084\n",
      "lexical diversity: (30, 1)\n",
      "[29. 40. 36. 24. 41. 37. 12. 28. 19. 12. 11. 25. 40. 17. 39. 13. 43. 16.\n",
      " 33. 18. 40. 43. 31. 38. 28. 25. 20. 33. 18. 39.]\n",
      "pos data: (30, 601) char data: (30, 24084) word data: (30, 38670)\n",
      "lexical diversity: (5, 1)\n",
      "train shape: (30, 63356) test shape: (5, 63356)\n",
      "training before feature selection: (30, 63356)\n",
      "testing before feature selection: (5, 63356)\n",
      "training after feature selection: (30, 53852)\n",
      "testing after feature selection: (5, 53852)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [0 0 0 ... 0 0 0] are constant.\n",
      "  UserWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred y_test: (5,) (5,)\n",
      "['candidate00001' 'candidate00003' 'candidate00003' 'candidate00001'\n",
      " 'candidate00005']\n",
      "['candidate00001' 'candidate00002' 'candidate00003' 'candidate00004'\n",
      " 'candidate00005']\n",
      "accuracy: 0.6\n",
      "problem00002 MEAN ACCURACY SCORES: 0.43999999999999995\n",
      "MEAN SCORES ACCROSS PROBLEMS: 0.24499999999999997\n",
      "elapsed time: 484.9901819229126\n"
     ]
    }
   ],
   "source": [
    "def baseline_keras(path, outpath, n=3, ft=5, pt=0.1, feature_selection=False, \n",
    "             open_set=False, c=1, feat_sel_percent=None, clf=None, calibration=True):\n",
    "    start_time = time.time()\n",
    "    # Reading information about the collection\n",
    "    infocollection = path+os.sep+'collection-info.json'\n",
    "    problems = []\n",
    "    language = []\n",
    "    with open(infocollection, 'r') as f:\n",
    "        for attrib in json.load(f):\n",
    "            problems.append(attrib['problem-name'])\n",
    "            language.append(attrib['language'])\n",
    "    problem_scores = []\n",
    "    for index,problem in enumerate(problems):\n",
    "        print(problem)\n",
    "        # Reading information about the problem\n",
    "        infoproblem = path+os.sep+problem+os.sep+'problem-info.json'\n",
    "        candidates = []\n",
    "        with open(infoproblem, 'r') as f:\n",
    "            fj = json.load(f)\n",
    "            unk_folder = fj['unknown-folder']\n",
    "            for attrib in fj['candidate-authors']:\n",
    "                candidates.append(attrib['author-name'])\n",
    "        # Building training set\n",
    "        docs=[]\n",
    "        for candidate in candidates:\n",
    "            docs.extend(read_files(path+os.sep+problem,candidate))\n",
    "        train_labels = np.array([label for i,(text,label) in enumerate(docs)])\n",
    "        print('\\t', 'language: ', language[index])\n",
    "        print('\\t', len(candidates), 'candidate authors')\n",
    "        print('\\t', len(docs), 'known texts')\n",
    "        \n",
    "        ###### Applying Classifiers #####\n",
    "        if calibration is True:\n",
    "            clf=CalibratedClassifierCV(OneVsRestClassifier(SVC(C=c)))\n",
    "        else:\n",
    "            clf=OneVsRestClassifier(SVC(C=c))\n",
    "        skf = StratifiedKFold(n_splits=5,random_state=442)\n",
    "        scores = []\n",
    "        print(\"Do cross validation.\")\n",
    "        for train_index, test_index in skf.split(docs, train_labels):\n",
    "            X_train_docs = [docs[i] for i in train_index]\n",
    "            X_test_docs = [docs[i] for i in test_index]\n",
    "            y_train, y_test = train_labels[train_index], train_labels[test_index]\n",
    "            ##### Extract X features ####\n",
    "            feat_extractor = Feature_Extractor(n, ft)\n",
    "            X_train = feat_extractor.fit_transform(X_train_docs)\n",
    "            X_test = feat_extractor.transform(X_test_docs)\n",
    "            print('train shape:', X_train.shape, 'test shape:', X_test.shape)\n",
    "            if feature_selection is True:\n",
    "                ####### Feature Selection - Fit #######\n",
    "                print(\"training before feature selection:\", X_train.shape)\n",
    "                print(\"testing before feature selection:\", X_test.shape)\n",
    "                #sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "                #train_data = sel.fit_transform(train_data)\n",
    "                # We use the default selection function: the 10% most significant features\n",
    "                sel = SelectPercentile(f_classif, percentile=feat_sel_percent)\n",
    "                X_train = sel.fit_transform(X_train, y_train)\n",
    "                X_test = sel.transform(X_test)\n",
    "                #sel = SelectKBest(chi2, k=100000)\n",
    "                #train_data = sel.fit_transform(train_data, train_labels)\n",
    "                print(\"training after feature selection:\", X_train.shape)\n",
    "                print(\"testing after feature selection:\", X_test.shape)\n",
    "            max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "            X_train = max_abs_scaler.fit_transform(X_train)\n",
    "            X_test = max_abs_scaler.transform(X_test)\n",
    "            baseline_model = get_baseline_model_fn(X_train.shape[1], len(candidates))\n",
    "            clf = KerasClassifier(build_fn=baseline_model, epochs=200, batch_size=5, verbose=0)\n",
    "            ##### Convert y to 1-hot encoding ####\n",
    "            y_encoder = LabelEncoder()\n",
    "            y_train = y_encoder.fit_transform(y_train)\n",
    "            # convert integers to dummy variables (i.e. one hot encoded)\n",
    "            y_train = np_utils.to_categorical(y_train)\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred = clf.predict(X_test)\n",
    "            print(\"y_pred y_test:\", y_pred.shape, y_test.shape)\n",
    "            y_pred = y_encoder.inverse_transform(y_pred)\n",
    "            print(y_pred)\n",
    "            print(y_test)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            scores.append(accuracy)\n",
    "            print('accuracy:', accuracy)\n",
    "            #write_results(path, problem, unk_folder, predictions)\n",
    "        mean_score = np.mean(scores)\n",
    "        print(problem, 'MEAN ACCURACY SCORES:', mean_score)\n",
    "        problem_scores.append(mean_score)\n",
    "    print('MEAN SCORES ACCROSS PROBLEMS:', np.mean(problem_scores))\n",
    "    # todo: also add stdev of scores\n",
    "    print('elapsed time:', time.time() - start_time)\n",
    "\n",
    "base_dir='pan18-cross-domain-authorship-attribution-training-dataset-2017-12-02'\n",
    "out_dir = base_dir+os.sep+'output-dir'\n",
    "eval_dir = base_dir+os.sep+'eval-dir'\n",
    "#params = {'n': 5,'ft': 3,'pt': 0.05,'feature_selection': False, 'c':0.1, 'feat_sel_percent': None, 'clf': 'SVC'}\n",
    "params = {'n': 5,'ft': 3,'pt': 0.05,'feature_selection': True, 'c':0.1, 'feat_sel_percent': 85, 'clf': 'SVC', 'calibration': True}\n",
    "#params = {'n': 5,'ft': 3,'pt': 0.05,'feature_selection': True, 'c':1, 'feat_sel_percent': 85, 'clf': 'SVC'}\n",
    "#params = {'n': 5,'ft': 3,'pt': 0.05,'feature_selection': True, 'c':10, 'feat_sel_percent': 85, 'clf': 'SVC'}\n",
    "baseline_keras(base_dir,out_dir,**params)\n",
    "%timeit\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#params = {'n': 5,'ft': 3,'pt': 0.05,'feature_selection': False, 'c':0.1, 'feat_sel_percent': None, 'clf': 'SVC'}\n",
    "params = {'n': 5,'ft': 3,'pt': 0.05,'feature_selection': True, 'c':0.1, 'feat_sel_percent': 85, 'clf': 'SVC', 'calibration': True}\n",
    "#params = {'n': 5,'ft': 3,'pt': 0.05,'feature_selection': True, 'c':1, 'feat_sel_percent': 85, 'clf': 'SVC'}\n",
    "#params = {'n': 5,'ft': 3,'pt': 0.05,'feature_selection': True, 'c':10, 'feat_sel_percent': 85, 'clf': 'SVC'}\n",
    "baseline(base_dir,out_dir,**params)\n",
    "%timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem00001\n",
      "(140, 64239)\n",
      "\t pos vocabulary size: 861 char vocabulary size: 64239\n",
      "pos data: (140, 861) char data: (140, 64239) word data: (140, 158555) fnword data: (140, 179)\n",
      "training before feature selection: (140, 223835)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [0 0 0 ... 0 0 0] are constant.\n",
      "  UserWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training after feature selection: (140, 190259)\n",
      "\t language:  en\n",
      "\t 20 candidate authors\n",
      "\t 140 known texts\n",
      "test before feature selection: (105, 223835)\n",
      "test after feature selection: (105, 190259)\n",
      "\t 105 unknown texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t answers saved to file answers-problem00001.json\n",
      "problem00002\n",
      "(35, 26761)\n",
      "\t pos vocabulary size: 619 char vocabulary size: 26761\n",
      "pos data: (35, 619) char data: (35, 26761) word data: (35, 44504) fnword data: (35, 179)\n",
      "training before feature selection: (35, 72064)\n",
      "training after feature selection: (35, 56984)\n",
      "\t language:  en\n",
      "\t 5 candidate authors\n",
      "\t 35 known texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [0 0 0 ... 0 0 0] are constant.\n",
      "  UserWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test before feature selection: (21, 72064)\n",
      "test after feature selection: (21, 56984)\n",
      "\t 21 unknown texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t answers saved to file answers-problem00002.json\n",
      "elapsed time: 41.05219507217407\n"
     ]
    }
   ],
   "source": [
    "def baseline_old(path, outpath, n=3, ft=5, pt=0.1, feature_selection=False, \n",
    "             open_set=False, c=1, feat_sel_percent=80, clf=None):\n",
    "    start_time = time.time()\n",
    "    # Reading information about the collection\n",
    "    infocollection = path+os.sep+'collection-info.json'\n",
    "    problems = []\n",
    "    language = []\n",
    "    with open(infocollection, 'r') as f:\n",
    "        for attrib in json.load(f):\n",
    "            problems.append(attrib['problem-name'])\n",
    "            language.append(attrib['language'])\n",
    "    for index,problem in enumerate(problems):\n",
    "        print(problem)\n",
    "        # Reading information about the problem\n",
    "        infoproblem = path+os.sep+problem+os.sep+'problem-info.json'\n",
    "        candidates = []\n",
    "        with open(infoproblem, 'r') as f:\n",
    "            fj = json.load(f)\n",
    "            unk_folder = fj['unknown-folder']\n",
    "            for attrib in fj['candidate-authors']:\n",
    "                candidates.append(attrib['author-name'])\n",
    "        # Building training set\n",
    "        train_docs=[]\n",
    "        for candidate in candidates:\n",
    "            train_docs.extend(read_files(path+os.sep+problem,candidate))\n",
    "        train_labels = [label for i,(text,label) in enumerate(train_docs)]\n",
    "        #### Feature Extraction ###\n",
    "        ###### Fit-Transform Training Set #######\n",
    "        feat_extractor = Feature_Extractor(n, ft)\n",
    "        train_data = feat_extractor.fit_transform(train_docs)\n",
    "        if feature_selection is True:\n",
    "            ####### Feature Selection - Fit #######\n",
    "            print(\"training before feature selection:\", train_data.shape)\n",
    "            #sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "            #train_data = sel.fit_transform(train_data)\n",
    "            # We use the default selection function: the 10% most significant features\n",
    "            sel = SelectPercentile(f_classif, percentile=feat_sel_percent)\n",
    "            train_data = sel.fit_transform(train_data, train_labels)\n",
    "            #sel = SelectKBest(chi2, k=100000)\n",
    "            #train_data = sel.fit_transform(train_data, train_labels)\n",
    "            print(\"training after feature selection:\", train_data.shape)\n",
    "        print('\\t', 'language: ', language[index])\n",
    "        print('\\t', len(candidates), 'candidate authors')\n",
    "        print('\\t', len(train_docs), 'known texts')\n",
    "        \n",
    "        ###### Transform Test Set #######\n",
    "        test_docs=read_files(path+os.sep+problem,unk_folder)\n",
    "        test_data = feat_extractor.transform(test_docs)\n",
    "        if feature_selection is True:\n",
    "            ####### Feature Selection #######\n",
    "            print(\"test before feature selection:\", test_data.shape)\n",
    "            test_data = sel.transform(test_data)\n",
    "            print(\"test after feature selection:\", test_data.shape)\n",
    "        print('\\t', len(test_docs), 'unknown texts')\n",
    "        \n",
    "        ###### Applying Classifiers #####\n",
    "        max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "        scaled_train_data = max_abs_scaler.fit_transform(train_data)\n",
    "        scaled_test_data = max_abs_scaler.transform(test_data)\n",
    "        clf=CalibratedClassifierCV(OneVsRestClassifier(SVC(C=c)))\n",
    "        clf.fit(scaled_train_data, train_labels)\n",
    "        predictions=clf.predict(scaled_test_data)\n",
    "        if open_set is True:\n",
    "            # Reject option (used in open-set cases)\n",
    "            count=0\n",
    "            for i,p in enumerate(predictions):\n",
    "                sproba=sorted(proba[i],reverse=True)\n",
    "                if sproba[0]-sproba[1]<pt:\n",
    "                    predictions[i]=u'<UNK>'\n",
    "                    count=count+1\n",
    "            print('\\t',count,'texts left unattributed')\n",
    "        # Saving output data\n",
    "        out_data=[]\n",
    "        unk_filelist = glob.glob(path+os.sep+problem+os.sep+unk_folder+os.sep+'*.txt')\n",
    "        pathlen=len(path+os.sep+problem+os.sep+unk_folder+os.sep)\n",
    "        for i,v in enumerate(predictions):\n",
    "            out_data.append({'unknown-text': unk_filelist[i][pathlen:], 'predicted-author': v})\n",
    "        with open(outpath+os.sep+'answers-'+problem+'.json', 'w') as f:\n",
    "            json.dump(out_data, f, indent=4)\n",
    "        print('\\t', 'answers saved to file','answers-'+problem+'.json')\n",
    "    print('elapsed time:', time.time() - start_time)\n",
    "\n",
    "base_dir='pan18-cross-domain-authorship-attribution-training-dataset-2017-12-02'\n",
    "out_dir = base_dir+os.sep+'output-dir'\n",
    "eval_dir = base_dir+os.sep+'eval-dir'\n",
    "params = {'n': 5,'ft': 3,'pt': 0.05,'feature_selection': True, 'c':0.1, 'feat_sel_percent': 85, 'clf': 'SVC'}\n",
    "#params = {'n': 5,'ft': 3,'pt': 0.05,'feature_selection': True, 'c':1, 'feat_sel_percent': 85, 'clf': 'SVC'}\n",
    "baseline_old(base_dir,out_dir,**params)\n",
    "%timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'n': 5,'ft': 3,'pt': 0.05,'feature_selection': True, 'c':0.1, 'feat_sel_percent': 85, 'clf': 'Keras Neural Network'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'n': 5,'ft': 3,'pt': 0.05,'feature_selection': True, 'c':0.1, 'feat_sel_percent': 85, 'clf': 'Keras Neural Network'}\n",
    "#params = {'n': 5,'ft': 3,'pt': 0.05,'feature_selection': False, 'c':0.1, 'feat_sel_percent': None, 'clf': 'SVC'}\n",
    "#params = {'n': 5,'ft': 3,'pt': 0.05,'feature_selection': True, 'c':1, 'feat_sel_percent': 85, 'clf': 'SVC'}\n",
    "baseline_old(base_dir,out_dir,**params)\n",
    "%timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem00001 Macro-F1: 0.585\n",
      "problem00002 Macro-F1: 0.734\n",
      "Overall score: 0.66\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>ft</th>\n",
       "      <th>pt</th>\n",
       "      <th>feature_selection</th>\n",
       "      <th>c</th>\n",
       "      <th>feat_sel_percent</th>\n",
       "      <th>clf</th>\n",
       "      <th>problem-name</th>\n",
       "      <th>macro-f1</th>\n",
       "      <th>macro-precision</th>\n",
       "      <th>macro-recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.05</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>85</td>\n",
       "      <td>SVC</td>\n",
       "      <td>problem00001</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.05</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>85</td>\n",
       "      <td>SVC</td>\n",
       "      <td>problem00002</td>\n",
       "      <td>0.734</td>\n",
       "      <td>0.715</td>\n",
       "      <td>0.767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.05</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>85</td>\n",
       "      <td>SVC</td>\n",
       "      <td>problem00001</td>\n",
       "      <td>0.585</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.05</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>85</td>\n",
       "      <td>SVC</td>\n",
       "      <td>problem00002</td>\n",
       "      <td>0.734</td>\n",
       "      <td>0.715</td>\n",
       "      <td>0.767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.05</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>85</td>\n",
       "      <td>SVC</td>\n",
       "      <td>problem00001</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.05</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>85</td>\n",
       "      <td>SVC</td>\n",
       "      <td>problem00002</td>\n",
       "      <td>0.734</td>\n",
       "      <td>0.715</td>\n",
       "      <td>0.767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.05</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>85</td>\n",
       "      <td>SVC</td>\n",
       "      <td>problem00001</td>\n",
       "      <td>0.585</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.05</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>85</td>\n",
       "      <td>SVC</td>\n",
       "      <td>problem00002</td>\n",
       "      <td>0.734</td>\n",
       "      <td>0.715</td>\n",
       "      <td>0.767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n  ft    pt  feature_selection    c  feat_sel_percent  clf  problem-name  \\\n",
       "0  5   3  0.05               True  0.1                85  SVC  problem00001   \n",
       "1  5   3  0.05               True  0.1                85  SVC  problem00002   \n",
       "2  5   3  0.05               True  0.1                85  SVC  problem00001   \n",
       "3  5   3  0.05               True  0.1                85  SVC  problem00002   \n",
       "4  5   3  0.05               True  0.1                85  SVC  problem00001   \n",
       "5  5   3  0.05               True  0.1                85  SVC  problem00002   \n",
       "6  5   3  0.05               True  0.1                85  SVC  problem00001   \n",
       "7  5   3  0.05               True  0.1                85  SVC  problem00002   \n",
       "\n",
       "   macro-f1  macro-precision  macro-recall  \n",
       "0     0.573            0.578         0.679  \n",
       "1     0.734            0.715         0.767  \n",
       "2     0.585            0.582         0.713  \n",
       "3     0.734            0.715         0.767  \n",
       "4     0.573            0.578         0.679  \n",
       "5     0.734            0.715         0.767  \n",
       "6     0.585            0.582         0.713  \n",
       "7     0.734            0.715         0.767  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "# Evaluation script for the Cross-Domain Authorship Attribution task @PAN2019.\n",
    "We use the F1 metric (macro-average) as implemented in scikit-learn:\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "We include the following ad hoc rules:\n",
    "- If authors are predicted which were not seen during training,\n",
    "  these predictions will count as false predictions ('<UNK>' class)\n",
    "  and they will negatively effect performance.\n",
    "- If texts are left unattributed they will assigned to the ('<UNK>'\n",
    "  class) and they will negatively effect performance.\n",
    "- The <UNK> class is excluded from the macro-average across classes.\n",
    "- If multiple test attributions are given for a single unknown document,\n",
    "  only the first one will be taken into consideration.\n",
    "\n",
    "Dependencies:\n",
    "- Python 2.7 or 3.6 (we recommend the Anaconda Python distribution)\n",
    "- scikit-learn\n",
    "\n",
    "Usage from the command line:\n",
    ">>> python pan19-cdaa-evaluator.py -i COLLECTION -a ANSWERS -o OUTPUT\n",
    "where\n",
    "    COLLECTION is the path to the main folder of the evaluation collection\n",
    "    ANSWERS is the path to the answers folder of a submitted method\n",
    "    OUTPUT is the path to the folder where the results of the evaluation will be saved\n",
    "\n",
    "Example: \n",
    ">>>  python pan19-cdaa-evaluator.py -i \".\\pan19-cross-domain-authorship-attribution-training-dataset-2019-01-23\\\" -a \".\\answ\n",
    "ers-unigram\" -o \".\\eval-unigram\\\"\n",
    "\n",
    "# References:\n",
    "@article{scikit-learn,\n",
    " title={Scikit-learn: Machine Learning in {P}ython},\n",
    " author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.\n",
    "         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.\n",
    "         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and\n",
    "         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},\n",
    " journal={Journal of Machine Learning Research},\n",
    " volume={12},\n",
    " pages={2825--2830},\n",
    " year={2011}\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "def eval_measures(gt, pred):\n",
    "    \"\"\"Compute macro-averaged F1-scores, macro-averaged precision, \n",
    "    macro-averaged recall, and micro-averaged accuracy according the ad hoc\n",
    "    rules discussed at the top of this file.\n",
    "    Parameters\n",
    "    ----------\n",
    "    gt : dict\n",
    "        Ground truth, where keys indicate text file names\n",
    "        (e.g. `unknown00002.txt`), and values represent\n",
    "        author labels (e.g. `candidate00003`)\n",
    "    pred : dict\n",
    "        Predicted attribution, where keys indicate text file names\n",
    "        (e.g. `unknown00002.txt`), and values represent\n",
    "        author labels (e.g. `candidate00003`)\n",
    "    Returns\n",
    "    -------\n",
    "    f1 : float\n",
    "        Macro-averaged F1-score\n",
    "    precision : float\n",
    "        Macro-averaged precision\n",
    "    recall : float\n",
    "        Macro-averaged recall\n",
    "    accuracy : float\n",
    "        Micro-averaged F1-score\n",
    "    \"\"\"\n",
    "\n",
    "    actual_authors = list(gt.values())\n",
    "    encoder = LabelEncoder().fit(['<UNK>'] + actual_authors)\n",
    "\n",
    "    text_ids, gold_authors, silver_authors = [], [], []\n",
    "    for text_id in sorted(gt):\n",
    "        text_ids.append(text_id)\n",
    "        gold_authors.append(gt[text_id])\n",
    "        try:\n",
    "            silver_authors.append(pred[text_id])\n",
    "        except KeyError:\n",
    "            # missing attributions get <UNK>:\n",
    "            silver_authors.append('<UNK>')\n",
    "\n",
    "    assert len(text_ids) == len(gold_authors)\n",
    "    assert len(text_ids) == len(silver_authors)\n",
    "\n",
    "    # replace non-existent silver authors with '<UNK>':\n",
    "    silver_authors = [a if a in encoder.classes_ else '<UNK>' \n",
    "                      for a in silver_authors]\n",
    "\n",
    "    gold_author_ints = encoder.transform(gold_authors)\n",
    "    silver_author_ints = encoder.transform(silver_authors)\n",
    "\n",
    "    # get F1 for individual classes (and suppress warnings):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        labels=list(set(gold_author_ints))\n",
    "        # Exclude the <UNK> class\n",
    "        for x in labels:\n",
    "            if encoder.inverse_transform(np.array([x]))=='<UNK>':\n",
    "                labels.remove(x)\n",
    "        f1 = f1_score(gold_author_ints,\n",
    "                  silver_author_ints,\n",
    "                  labels,\n",
    "                  average='macro')\n",
    "        precision = precision_score(gold_author_ints,\n",
    "                  silver_author_ints,\n",
    "                  labels,\n",
    "                  average='macro')\n",
    "        recall = recall_score(gold_author_ints,\n",
    "                  silver_author_ints,\n",
    "                  labels,\n",
    "                  average='macro')\n",
    "        accuracy = accuracy_score(gold_author_ints,\n",
    "                  silver_author_ints)\n",
    "\n",
    "    return f1,precision,recall\n",
    "\n",
    "def evaluate(ground_truth_file,predictions_file):\n",
    "    # Calculates evaluation measures for a single attribution problem\n",
    "    gt = {}\n",
    "    with open(ground_truth_file, 'r') as f:\n",
    "        for attrib in json.load(f)['ground_truth']:\n",
    "            gt[attrib['unknown-text']] = attrib['true-author']\n",
    "\n",
    "    pred = {}\n",
    "    with open(predictions_file, 'r') as f:\n",
    "        for attrib in json.load(f):\n",
    "            if attrib['unknown-text'] not in pred:\n",
    "                pred[attrib['unknown-text']] = attrib['predicted-author']\n",
    "    f1,precision,recall =  eval_measures(gt,pred)\n",
    "    return round(f1,3), round(precision,3), round(recall,3)\n",
    "\n",
    "def evaluate_all(path_collection,path_answers,path_out,params):\n",
    "    # Calculates evaluation measures for a PAN-18 collection of attribution problems\n",
    "    infocollection = path_collection+os.sep+'collection-info.json'\n",
    "    problems = []\n",
    "    data = []\n",
    "    with open(infocollection, 'r') as f:\n",
    "        for attrib in json.load(f):\n",
    "            problems.append(attrib['problem-name'])\n",
    "    scores=[];\n",
    "    for problem in problems:\n",
    "        prob_data = deepcopy(params)\n",
    "        f1,precision,recall=evaluate(path_collection+os.sep+problem+os.sep+'ground-truth.json',path_answers+os.sep+'answers-'+problem+'.json')\n",
    "        scores.append(f1)\n",
    "        prob_data.update({'problem-name': problem, 'macro-f1': round(f1,3), 'macro-precision': round(precision,3), 'macro-recall': round(recall,3)})\n",
    "        if os.path.isfile('metrics.csv'):\n",
    "            with open('metrics.csv', 'a') as f:  # Just use 'w' mode in 3.x\n",
    "                w = csv.DictWriter(f, prob_data.keys())\n",
    "                w.writerow(prob_data)\n",
    "        else:\n",
    "            with open('metrics.csv', 'w') as f:  # Just use 'w' mode in 3.x\n",
    "                w = csv.DictWriter(f, prob_data.keys())\n",
    "                w.writeheader()\n",
    "                w.writerow(prob_data)\n",
    "        data.append(prob_data)\n",
    "        print(str(problem),'Macro-F1:',round(f1,3))\n",
    "    overall_score=sum(scores)/len(scores)\n",
    "    # Saving data to output files (out.json and evaluation.prototext)\n",
    "    with open(path_out+os.sep+'out.json', 'w') as f:\n",
    "        json.dump({'problems': data, 'overall_score': round(overall_score,3)}, f, indent=4, sort_keys=True)\n",
    "    print('Overall score:', round(overall_score,3))\n",
    "    prototext='measure {\\n key: \"mean macro-f1\"\\n value: \"'+str(round(overall_score,3))+'\"\\n}\\n'\n",
    "    with open(path_out+os.sep+'evaluation.prototext', 'w') as f:\n",
    "        f.write(prototext)\n",
    "    return pd.read_csv('metrics.csv')\n",
    "        \n",
    "evaluate_all(base_dir,out_dir,eval_dir, params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('metrics.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('c', inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['feature_selection'] == True].groupby(['problem-name'])['macro-recall', 'macro-precision'].plot(legend=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
