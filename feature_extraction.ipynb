{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction\n",
    "\n",
    "In this notebook we will learn how to extract different features from a text and how to combine them. It's pretty simple, but if you have this part well organized, it will be really useful in the near future. So, let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import argparse\n",
    "import time\n",
    "import codecs\n",
    "from collections import defaultdict\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/sallyisa/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sallyisa/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_file(path):\n",
    "    with open(path, 'r+') as f:\n",
    "        return '\\n'.join([line.strip() for line in f])\n",
    "    \n",
    "def process_dir_files(path):\n",
    "    dir_files = []\n",
    "    for file in os.listdir(path):\n",
    "        current = os.path.join(path, file)\n",
    "        if os.path.isfile(current):\n",
    "            dir_files.append(open_file(current))\n",
    "    return dir_files\n",
    "                     \n",
    "\n",
    "train_sents= process_dir_files('pan18-cross-domain-authorship-attribution-training-dataset-2017-12-02/problem00001/candidate00001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def get_pos(text):\n",
    "    pos_tags= nltk.pos_tag(word_tokenize(text))\n",
    "    # print(len(pos_tags))\n",
    "    pos_tags = [word_tag[1] for word_tag in pos_tags]\n",
    "    pos_text = ' '.join(pos_tags)\n",
    "    return pos_tags\n",
    "\n",
    "#get_pos(train_sents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_ngrams(sents):\n",
    "    pos_tags= [nltk.pos_tag(word_tokenize(sents[ind])) for ind, item in enumerate(sents) if item != '']\n",
    "    pos_sents = []\n",
    "    for sent in pos_tags:\n",
    "        #print(sent)\n",
    "        pos = ' '.join([pos_tag[1] for pos_tag in sent])\n",
    "        #print(pos, '\\n')\n",
    "        pos_sents.append(pos)\n",
    "    vectorizer = CountVectorizer(ngram_range = (1,1))\n",
    "    vectorizer.fit(pos_sents)\n",
    "    return vectorizer\n",
    "\n",
    "\n",
    "#pos_vectorizer = get_pos_ngrams(train_sents)\n",
    "#pos_ngram = pos_vectorizer.transform(train_sents)\n",
    "#pos_ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'gr': 7,\n",
       "             'ra': 6,\n",
       "             'ac': 10,\n",
       "             'ce': 16,\n",
       "             'ef': 5,\n",
       "             'fu': 11,\n",
       "             'ul': 8,\n",
       "             'l ': 9,\n",
       "             ' o': 29,\n",
       "             'on': 25,\n",
       "             'ne': 27,\n",
       "             'es': 13,\n",
       "             's.': 4,\n",
       "             '.\\n': 18,\n",
       "             '\\n\\n': 34,\n",
       "             '\\n\"': 9,\n",
       "             '\"O': 1,\n",
       "             'On': 1,\n",
       "             'e ': 131,\n",
       "             ' m': 20,\n",
       "             'mo': 6,\n",
       "             'or': 26,\n",
       "             're': 41,\n",
       "             'e,': 9,\n",
       "             ',\"': 6,\n",
       "             '\" ': 8,\n",
       "             ' M': 7,\n",
       "             'Ma': 11,\n",
       "             'ar': 27,\n",
       "             'rv': 11,\n",
       "             've': 27,\n",
       "             'el': 28,\n",
       "             'lo': 22,\n",
       "             'ou': 44,\n",
       "             'us': 28,\n",
       "             's ': 68,\n",
       "             ' s': 72,\n",
       "             'sa': 13,\n",
       "             'ai': 18,\n",
       "             'id': 12,\n",
       "             'd,': 13,\n",
       "             ', ': 42,\n",
       "             'so': 5,\n",
       "             'un': 9,\n",
       "             'nd': 46,\n",
       "             'di': 6,\n",
       "             'in': 76,\n",
       "             'ng': 49,\n",
       "             'g ': 39,\n",
       "             ' r': 9,\n",
       "             'ro': 17,\n",
       "             'oy': 1,\n",
       "             'ya': 4,\n",
       "             'al': 7,\n",
       "             'll': 12,\n",
       "             'ly': 11,\n",
       "             'y ': 17,\n",
       "             ' b': 30,\n",
       "             'bo': 6,\n",
       "             'ed': 46,\n",
       "             'd ': 90,\n",
       "             ' f': 32,\n",
       "             'fr': 2,\n",
       "             'om': 6,\n",
       "             'm ': 8,\n",
       "             ' h': 76,\n",
       "             'hi': 24,\n",
       "             'is': 13,\n",
       "             'se': 18,\n",
       "             'ea': 29,\n",
       "             'at': 39,\n",
       "             't.': 5,\n",
       "             '\"S': 1,\n",
       "             'Sh': 6,\n",
       "             'he': 124,\n",
       "             'e’': 9,\n",
       "             '’s': 12,\n",
       "             ' t': 102,\n",
       "             'ti': 13,\n",
       "             'ir': 9,\n",
       "             ' J': 11,\n",
       "             'Jo': 12,\n",
       "             'oe': 12,\n",
       "             'th': 64,\n",
       "             'ho': 8,\n",
       "             'ug': 6,\n",
       "             'gh': 12,\n",
       "             'h ': 15,\n",
       "             ' n': 17,\n",
       "             'no': 11,\n",
       "             'ot': 12,\n",
       "             't ': 75,\n",
       "             ' u': 13,\n",
       "             'nk': 1,\n",
       "             'ki': 15,\n",
       "             'dl': 3,\n",
       "             'y.': 3,\n",
       "             '. ': 22,\n",
       "             ' (': 3,\n",
       "             '(t': 3,\n",
       "             'uc': 9,\n",
       "             'ck': 14,\n",
       "             ' j': 7,\n",
       "             'je': 2,\n",
       "             'er': 62,\n",
       "             'rk': 4,\n",
       "             'k)': 1,\n",
       "             ').': 1,\n",
       "             '\\nH': 2,\n",
       "             'He': 6,\n",
       "             ' w': 49,\n",
       "             'wa': 25,\n",
       "             'as': 28,\n",
       "             'ri': 7,\n",
       "             'ig': 8,\n",
       "             'ht': 10,\n",
       "             't;': 1,\n",
       "             '; ': 2,\n",
       "             'r ': 56,\n",
       "             'mu': 2,\n",
       "             'sc': 1,\n",
       "             'cl': 2,\n",
       "             'le': 14,\n",
       "             'ha': 22,\n",
       "             'av': 6,\n",
       "             ' l': 19,\n",
       "             'si': 5,\n",
       "             'nc': 8,\n",
       "             'tu': 2,\n",
       "             'ur': 6,\n",
       "             'rn': 4,\n",
       "             'to': 44,\n",
       "             'o ': 43,\n",
       "             ' c': 16,\n",
       "             'co': 8,\n",
       "             'tt': 9,\n",
       "             'n ': 36,\n",
       "             'wi': 10,\n",
       "             'it': 22,\n",
       "             ' e': 18,\n",
       "             'ex': 6,\n",
       "             'xh': 1,\n",
       "             'au': 6,\n",
       "             'st': 20,\n",
       "             'io': 4,\n",
       "             ' a': 76,\n",
       "             'an': 49,\n",
       "             ' k': 7,\n",
       "             'kn': 2,\n",
       "             'ee': 9,\n",
       "             'su': 4,\n",
       "             'up': 8,\n",
       "             'pp': 6,\n",
       "             'po': 4,\n",
       "             'rt': 4,\n",
       "             'pr': 7,\n",
       "             ' B': 2,\n",
       "             'Bu': 2,\n",
       "             'ut': 9,\n",
       "             ' d': 15,\n",
       "             'da': 5,\n",
       "             'am': 3,\n",
       "             'mn': 3,\n",
       "             ' i': 35,\n",
       "             'if': 10,\n",
       "             'f ': 27,\n",
       "             'sh': 31,\n",
       "             ' g': 18,\n",
       "             'go': 8,\n",
       "             'oi': 8,\n",
       "             'ta': 11,\n",
       "             'ak': 5,\n",
       "             'ke': 9,\n",
       "             'of': 14,\n",
       "             'ff': 4,\n",
       "             'fe': 6,\n",
       "             'lp': 1,\n",
       "             'p ': 9,\n",
       "             'p,': 1,\n",
       "             'ma': 9,\n",
       "             'rs': 8,\n",
       "             'lf': 5,\n",
       "             'oo': 16,\n",
       "             'ok': 6,\n",
       "             'k ': 9,\n",
       "             'li': 11,\n",
       "             'ik': 3,\n",
       "             'a ': 19,\n",
       "             'we': 6,\n",
       "             'kl': 1,\n",
       "             'nt': 9,\n",
       "             's,': 3,\n",
       "             'wh': 10,\n",
       "             ' -': 4,\n",
       "             '- ': 4,\n",
       "             'et': 12,\n",
       "             'ca': 8,\n",
       "             'ap': 6,\n",
       "             'pt': 2,\n",
       "             'n.': 3,\n",
       "             '\\nL': 4,\n",
       "             'Lu': 8,\n",
       "             'uk': 8,\n",
       "             'ka': 8,\n",
       "             'de': 11,\n",
       "             'te': 23,\n",
       "             'rm': 3,\n",
       "             'mi': 2,\n",
       "             ' p': 15,\n",
       "             'ov': 4,\n",
       "             'wo': 9,\n",
       "             'em': 4,\n",
       "             'm,': 2,\n",
       "             'k.': 2,\n",
       "             '\\nS': 5,\n",
       "             'So': 1,\n",
       "             'sl': 2,\n",
       "             'la': 11,\n",
       "             'pe': 9,\n",
       "             'aw': 6,\n",
       "             'ay': 10,\n",
       "             'pu': 3,\n",
       "             'fl': 1,\n",
       "             'r,': 2,\n",
       "             'im': 11,\n",
       "             'me': 16,\n",
       "             'sw': 4,\n",
       "             'rd': 6,\n",
       "             'ec': 7,\n",
       "             'k,': 1,\n",
       "             'il': 8,\n",
       "             'be': 19,\n",
       "             'tr': 7,\n",
       "             'r.': 3,\n",
       "             '\"T': 2,\n",
       "             'Th': 5,\n",
       "             'hr': 5,\n",
       "             ' v': 3,\n",
       "             'vo': 2,\n",
       "             'ic': 4,\n",
       "             'mb': 1,\n",
       "             'bl': 5,\n",
       "             'g,': 2,\n",
       "             'sp': 6,\n",
       "             'sm': 3,\n",
       "             '\\nM': 4,\n",
       "             ' y': 12,\n",
       "             'wn': 2,\n",
       "             'd.': 6,\n",
       "             '\\n(': 4,\n",
       "             '(f': 3,\n",
       "             'ks': 2,\n",
       "             'm.': 3,\n",
       "             '.)': 4,\n",
       "             ')\\n': 4,\n",
       "             '\\n-': 1,\n",
       "             '-\\n': 1,\n",
       "             '\\nA': 1,\n",
       "             'A ': 1,\n",
       "             'ew': 3,\n",
       "             'w ': 4,\n",
       "             'ek': 1,\n",
       "             'ei': 7,\n",
       "             'ni': 7,\n",
       "             ' L': 6,\n",
       "             'iz': 1,\n",
       "             'ze': 1,\n",
       "             'ju': 4,\n",
       "             'od': 5,\n",
       "             '\"Y': 2,\n",
       "             'Yo': 5,\n",
       "             'u ': 4,\n",
       "             'ow': 8,\n",
       "             'ad': 10,\n",
       "             'yo': 10,\n",
       "             'op': 4,\n",
       "             'en': 23,\n",
       "             't’': 3,\n",
       "             'ts': 2,\n",
       "             'ci': 2,\n",
       "             'ip': 2,\n",
       "             'pa': 3,\n",
       "             'xt': 4,\n",
       "             'nb': 1,\n",
       "             'br': 2,\n",
       "             'mp': 2,\n",
       "             'ie': 3,\n",
       "             'w.': 3,\n",
       "             '(m': 1,\n",
       "             'yb': 1,\n",
       "             'dn': 5,\n",
       "             'n’': 9,\n",
       "             '’t': 9,\n",
       "             'n,': 3,\n",
       "             'ld': 6,\n",
       "             '’d': 3,\n",
       "             'bu': 3,\n",
       "             'ms': 3,\n",
       "             'os': 4,\n",
       "             'lt': 4,\n",
       "             'gu': 2,\n",
       "             'ui': 1,\n",
       "             'ty': 2,\n",
       "             'fo': 14,\n",
       "             'ol': 1,\n",
       "             'ey': 2,\n",
       "             'ye': 2,\n",
       "             ' “': 11,\n",
       "             '“Y': 3,\n",
       "             'u’': 6,\n",
       "             '’r': 6,\n",
       "             'lk': 2,\n",
       "             'ch': 7,\n",
       "             'h,': 3,\n",
       "             ',”': 2,\n",
       "             '” ': 3,\n",
       "             'rg': 1,\n",
       "             'ge': 8,\n",
       "             'oc': 2,\n",
       "             'ba': 3,\n",
       "             'fa': 3,\n",
       "             'ag': 5,\n",
       "             'ga': 4,\n",
       "             ' T': 1,\n",
       "             'ss': 3,\n",
       "             'cu': 1,\n",
       "             'ua': 1,\n",
       "             'ru': 4,\n",
       "             'ab': 5,\n",
       "             'e.': 11,\n",
       "             ' H': 4,\n",
       "             'pl': 6,\n",
       "             'pi': 2,\n",
       "             'nl': 2,\n",
       "             'rp': 1,\n",
       "             'ns': 3,\n",
       "             't)': 1,\n",
       "             ') ': 2,\n",
       "             '“I': 5,\n",
       "             'I’': 6,\n",
       "             '’m': 5,\n",
       "             'eg': 4,\n",
       "             'gi': 2,\n",
       "             'nn': 3,\n",
       "             'y,': 1,\n",
       "             'a,': 1,\n",
       "             'tc': 3,\n",
       "             '.”': 6,\n",
       "             '”\\n': 8,\n",
       "             'u.': 3,\n",
       "             '\\nJ': 1,\n",
       "             'ja': 1,\n",
       "             'ds': 3,\n",
       "             'gt': 1,\n",
       "             'fi': 6,\n",
       "             'cr': 2,\n",
       "             'ue': 1,\n",
       "             'h.': 3,\n",
       "             '\"I': 1,\n",
       "             'I ': 4,\n",
       "             'dg': 1,\n",
       "             ' \"': 2,\n",
       "             ' Z': 1,\n",
       "             'Za': 1,\n",
       "             'gy': 1,\n",
       "             'rc': 1,\n",
       "             'ev': 6,\n",
       "             'ry': 2,\n",
       "             ' S': 1,\n",
       "             ' I': 5,\n",
       "             'If': 1,\n",
       "             '.\"': 2,\n",
       "             'gg': 1,\n",
       "             '\"M': 1,\n",
       "             'Mi': 1,\n",
       "             'eh': 1,\n",
       "             'p.': 1,\n",
       "             '\"\\n': 3,\n",
       "             '“A': 1,\n",
       "             'Ar': 2,\n",
       "             'yi': 5,\n",
       "             'k?': 1,\n",
       "             '?”': 2,\n",
       "             '\"A': 1,\n",
       "             'u?': 1,\n",
       "             '?\"': 2,\n",
       "             'wr': 1,\n",
       "             '’l': 1,\n",
       "             't,': 5,\n",
       "             'ws': 1,\n",
       "             ' N': 1,\n",
       "             'No': 1,\n",
       "             'o,': 2,\n",
       "             'ct': 1,\n",
       "             'uy': 1,\n",
       "             'dr': 1,\n",
       "             'n—': 1,\n",
       "             '— ': 2,\n",
       "             '“J': 1,\n",
       "             'Ju': 1,\n",
       "             't—': 1,\n",
       "             's—': 1,\n",
       "             '—”': 1,\n",
       "             'gn': 3,\n",
       "             'a’': 1,\n",
       "             'sn': 3,\n",
       "             'na': 4,\n",
       "             ' A': 1,\n",
       "             'Al': 1,\n",
       "             'oa': 1,\n",
       "             's)': 1,\n",
       "             'tl': 1,\n",
       "             'g;': 1,\n",
       "             '“W': 1,\n",
       "             'We': 1,\n",
       "             'l,': 1,\n",
       "             'o’': 1,\n",
       "             'tw': 1,\n",
       "             'e?': 1,\n",
       "             '\\nI': 1,\n",
       "             'It': 1,\n",
       "             'ny': 1,\n",
       "             'ep': 2,\n",
       "             'd’': 1,\n",
       "             's’': 1,\n",
       "             '’ ': 1,\n",
       "             '\"W': 1,\n",
       "             'Wh': 1,\n",
       "             'do': 3,\n",
       "             'n?': 1,\n",
       "             'xp': 1,\n",
       "             'gl': 1,\n",
       "             'g.': 1,\n",
       "             'ia': 2,\n",
       "             'f,': 1,\n",
       "             'Le': 2,\n",
       "             '\\nT': 2,\n",
       "             'bb': 1,\n",
       "             'df': 1,\n",
       "             '\"C': 1,\n",
       "             'Cl': 1,\n",
       "             'p!': 1,\n",
       "             '!\"': 1,\n",
       "             'um': 1,\n",
       "             'ls': 1,\n",
       "             'vi': 1})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    " A baseline authorship attribution method \n",
    " based on a character n-gram representation\n",
    " and a linear SVM classifier.\n",
    " It has a reject option to leave documents unattributed\n",
    " (when the probabilities of the two most likely training classes are too close)\n",
    " \n",
    " Questions/comments: stamatatos@aegean.gr\n",
    "\n",
    " It can be applied to datasets of PAN-19 cross-domain authorship attribution task\n",
    " See details here: http://pan.webis.de/clef19/pan19-web/author-identification.html\n",
    " Dependencies:\n",
    " - Python 2.7 or 3.6 (we recommend the Anaconda Python distribution)\n",
    " - scikit-learn\n",
    "\n",
    " Usage from command line: \n",
    "    > python pan19-cdaa-baseline.py -i EVALUATION-DIRECTORY -o OUTPUT-DIRECTORY [-n N-GRAM-ORDER] [-ft FREQUENCY-THRESHOLD] [-pt PROBABILITY-THRESHOLD]\n",
    " EVALUATION-DIRECTORY (str) is the main folder of a PAN-19 collection of attribution problems\n",
    " OUTPUT-DIRECTORY (str) is an existing folder where the predictions are saved in the PAN-19 format\n",
    " Optional parameters of the model:\n",
    "   N-GRAM-ORDER (int) is the length of character n-grams (default=3)\n",
    "   FREQUENCY-THRESHOLD (int) is the cutoff threshold used to filter out rare n-grams (default=5)\n",
    "   PROBABILITY-THRESHOLD (float) is the threshold for the reject option assigning test documents to the <UNK> class (default=0.1)\n",
    "                                 Let P1 and P2 be the two maximum probabilities of training classes for a test document. If P1-P2<pt then the test document is assigned to the <UNK> class.\n",
    "   \n",
    " Example:\n",
    "\n",
    "     >  python pan19-cdaa-baseline-svm.py -i \".\\pan19-cross-domain-authorship-attribution-training-dataset-2019-01-23\\\" -o \".\\a\n",
    "nswers-trigram\\\" -n 3\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import argparse\n",
    "import time\n",
    "import codecs\n",
    "from collections import defaultdict\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "def represent_text(text,n,pos=False):\n",
    "    # Extracts all character n-grams from  a 'text'\n",
    "    # if pos is True, extracts POS n-grams\n",
    "    if n>0:\n",
    "        if pos is True:\n",
    "            text = get_pos(text)\n",
    "            tokens = [' '.join(text[i:i+n]) for i in range(len(text)-n+1)]\n",
    "            #print(tokens)\n",
    "        else:\n",
    "            tokens = [text[i:i+n] for i in range(len(text)-n+1)]\n",
    "    frequency = defaultdict(int)\n",
    "    for token in tokens:\n",
    "        frequency[token] += 1\n",
    "    return frequency\n",
    "\n",
    "represent_text(train_sents[0], 2)\n",
    "#represent_text(train_sents[0], 2, pos=True)\n",
    "                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "287\n",
      "['JJ', 'NNS', '.', '``', 'CD', 'JJR', ',', \"''\", 'NNP', 'VBD', 'VBG', 'RB', 'VBN', 'IN', 'PRP$', 'NN', 'PRP', '(', 'DT', ')', ':', 'VBP', 'TO', 'CC', 'VB', 'WP', 'RP', 'WRB', 'EX', 'VBZ', 'JJS', 'MD', 'PDT', 'WDT', 'JJ NNS', 'NNS .', '. ``', \", ''\", \"'' NNP\", 'NNP VBD', 'VBD ,', ', VBG', 'VBG RB', 'RB VBN', 'VBN IN', 'IN PRP$', 'PRP$ NN', 'NN .', '`` PRP', 'PRP VBD', 'VBD RB', 'VBN ,', ', IN', 'IN RB', 'RB RB', 'RB .', '. (', 'DT NN', 'NN NN', '. PRP', 'RB :', 'PRP$ NNS', 'NNS VBP', 'VBP RB', 'RB IN', 'VBN TO', 'NN IN', 'IN NN', 'NN CC', 'CC PRP$', 'NNS VBD', 'VBD TO', 'TO VB', 'VB PRP$', 'PRP$ JJ', 'JJ .', '. CC', 'CC VBD', 'VBD IN', 'IN PRP', 'VBD VBG', 'VBG TO', 'VB NNP', 'NNP NNP', 'NNP NN', 'NN VBD', 'VBD NN', 'NN TO', 'NN ,', ', CC', 'VB PRP', 'PRP VB', 'VB IN', 'IN DT', 'IN NNP', 'NNP ,', ': IN', 'VBD PRP', 'PRP CC', 'CC RB', 'VBD DT', '. NNP', 'VBD VBN', 'TO DT', 'DT IN', 'PRP ,', ', DT', 'NN WP', 'WP PRP', 'VBD .', '. IN', 'VBD NNP', 'JJ NN', 'NN RB', 'RB CC', 'PRP RP', ', JJ', 'RB TO', 'PRP .', ', NN', 'CC NNP', 'VBN .', 'NNS ,', '. )', ': DT', 'DT JJ', 'NNS IN', 'RB JJ', 'JJ IN', 'IN VBG', 'VBG PRP$', 'PRP VBP', 'VBP TO', 'VB WRB', 'JJ JJ', 'NN NNS', 'NNS CC', 'VBP PRP$', 'EX VBD', 'RB PRP', 'PRP VBZ', 'VBZ NN', 'VBD JJ', 'NNP IN', 'RB ,', 'RB DT', 'PRP TO', 'VB RB', 'PRP MD', 'VBG NN', ') PRP', 'PRP NNS', '. NN', 'NN PRP', 'VBP VB', 'VB VBG', 'JJ ,', ', NNP', ', RB', 'JJ RB', 'RB VBD', 'PRP NN', 'VBD PRP$', 'CC PRP', '. DT', 'JJ PRP', 'VBG IN', 'RP IN', 'NNP CC', 'CC VBG', 'VBG PRP', 'PRP IN', 'PDT DT', 'DT NNS', 'NNS PRP', 'VBD CC', 'IN JJ', 'NN (', 'VBP JJ', ') CC', 'VB DT', 'CC IN', 'RB VBG', 'CC VB', 'WRB PRP', 'NNP PRP', 'NNP .', 'NNP VBZ', 'VBZ PRP$', 'VB NN', 'VBZ JJ', 'CC NN', 'CC DT', 'DT .', 'VB ,', 'TO PRP$', 'JJ TO', 'TO PRP', 'PRP JJ', 'MD VB', 'VB JJ', \". ''\", 'RB VB', 'VBP PRP', 'JJ CC', 'NN VBG', 'RP PRP$', 'DT ,', 'VBG DT', 'NN NNP', 'PRP$ CC', 'NNS RB', 'MD RB', 'VBP NN', 'DT CD', 'CD IN', 'VBP .', 'VBZ DT', 'RB WRB', ', WRB', 'PRP$ IN', 'VB .', '. VB', 'DT RB', 'PRP RB', 'VB VBN', 'NN WDT', 'NN VBP', ', PRP', 'NNS DT', 'NN :', ': RB', '. VBG', 'RBR', 'IN .', '( PRP', 'RB VBP', 'RB VBZ', 'NN JJ', ', VB', 'VBD NNS', 'IN NNS', '. CD', 'CD .', '. WP', 'VBZ IN', '. WRB', 'VBZ PRP', 'VB CC', '. PRP$', 'CC VBZ', 'VBZ TO', 'VBZ RB', 'PRP :', ': CC', '. :', 'VBZ ,', 'PRP DT', 'VBP WRB', 'NNS VBG', 'VBG CC', '. RB', 'NN VBZ', 'IN IN', 'VB NNS', ': PRP', 'DT VBD', 'VBP IN', 'DT VBP', 'NN WRB', 'IN ,', 'CC JJ', 'NN DT', 'VBN DT', 'NNS TO', 'NN MD', 'WRB DT', 'JJ NNP', 'CC WRB', 'VB TO', 'VBP DT', 'CC NNS', 'IN WP', 'POS', 'NN POS', ', VBZ', 'IN EX']\n",
      "[\"JJ NNS . `` CD JJR , '' NNP VBD , VBG RB VBN IN PRP$ NN . `` PRP VBD RB VBN , '' NNP VBD , IN RB RB . ( DT NN NN ) . PRP VBD RB : PRP$ NNS VBP RB IN VBN TO NN IN NN CC PRP$ NNS VBD TO VB PRP$ JJ . CC VBD IN PRP VBD VBG TO VB NNP NNP NN VBD NN TO VB PRP$ NN , CC VBD IN PRP VBD VBG TO VB PRP VB IN DT NN IN NN IN NNP , WP : IN PRP VBD PRP CC RB : VBD DT NN NN . NNP VBD VBN TO VB PRP$ NN TO DT IN PRP , DT NN WP PRP VBD . IN PRP VBD NNP NNP JJ NN RB CC VBD PRP RP DT NN NN , CC VBD DT NN IN PRP$ NN , JJ PRP$ NN RB TO VB PRP . `` CD JJR , '' PRP VBD , NN NN , CC NNP VBD IN DT NN . JJ VBN . ( VBG NNS , DT IN PRP . ) : DT JJ NNS IN PRP$ NN , CC NNP VBD NNP VBD RB RB JJ IN VBG PRP$ NN . `` PRP VBP TO VB WRB TO VB PRP$ JJ JJ NN NNS CC VBP PRP$ JJ NN , '' NNP VBD , CC EX VBD DT JJ NN IN PRP$ NN RB . ( RB PRP VBZ NN IN PRP VBD JJ NN NN , IN NNP VBD PRP VBD NNP JJ NN IN PRP VBD RB VBN NNP IN JJS RB , CC NNP VBD RB DT NN TO VB PRP TO VB RB IN PRP MD VBG NN CC VBD JJ IN PRP . ) PRP VBD PRP NNS IN PRP . NN PRP VBP VB VBG RB JJ , NNP , NNP NNP VBD , RB VBN IN PRP . PRP VBD JJ RB VBD PRP NN CC VBD PRP$ NN IN PRP , CC PRP VBD NN RB VBD PRP NN IN PRP VBD IN PRP$ NN RB . DT JJ JJ PRP VBD VBG IN PRP NN , VBG RP IN NNP CC VBG PRP IN PDT DT NNS PRP VBD . NNP VBD CC VBD PRP$ NN TO DT NN NN . PRP VBD DT NN IN JJ NN ( DT NN VBD PRP$ JJ NN IN DT NN IN PRP$ NNS VBP JJ IN DT JJ JJ NN PRP VBD , VBG NN ) CC VBD IN PRP . NN PRP VBP JJ NN TO VB IN PRP VBP VB VBG TO VB DT NN NN , NNP , CC IN PRP VBP VB RB VBG TO VB RB CC VB PRP WRB PRP VBP JJ ( JJ PRP . ) NNP NNP VBD NN VBD , CC PRP VBD , NNP PRP VBP JJ VBG RB JJ , NNP . NNP VBZ PRP$ NN IN PRP VBD NN TO VB NN IN PRP VBZ JJ NN NN IN PRP VBD VBG PRP$ NN CC NN CC DT . ( JJ PRP . ) `` PRP VBP TO VB , '' NNP VBD , CC IN DT JJ NN EX VBD DT NN TO PRP$ NN . `` DT NNP NNS VBP VBG JJR DT NN . PRP VBD RB RB JJ TO PRP JJ IN PRP MD VB JJ NN IN PRP$ NN . IN PRP MD VB JJ NN IN PRP . '' PRP VBD . `` NNP RB RB VB PRP$ NN IN PRP$ JJ NN . '' NNP VBD PRP$ NNS . NNS VBP PRP VBG PRP VBP JJ JJ . NN '' NNP PRP . '' `` PRP VBP VB JJ CC PRP VBP JJ IN PRP , '' NNP VBD , DT NN IN NN VBG TO VB RP PRP$ NNS . UH , DT , DT NN IN VBG DT JJS NN , IN PRP$ VBG DT NN IN NN NNP NNP NNP NNP NNP VBD RB , CC VBD PRP TO VB PRP$ CC NN . NNP NNP VBZ NN VBD . DT PRP VBD VBD TO VB PRP$ NNS RB JJ IN PRP MD ( PRP$ NN IN VBG DT NN IN PRP$ NNS ) CC RB PRP VBD IN VBG : NNP NNP , IN PRP VBP MD RB VBG TO VB : WP VBP NN TO VB DT CD IN PRP VBP . IN PRP VBZ VBZ DT NN PRP VBD RB JJ TO VB RB WRB EX VBD DT NN NN IN PRP$ NN CC PRP VBD JJ RB TO VB PRP$ NNS IN DT NN NNP NN NN . CC PRP VBD JJ PRP , WRB NNP NNP NN VBD RB TO VB IN PRP , CC NNP VBD TO VB PRP$ IN VBG NN . `` WP VBP PRP VB . '' PRP VBD DT NN IN PRP$ NN CC VBD RB . VB PRP IN TO VB DT RB IN DT NN IN DT NN , WRB PRP VBP JJ RB , NNP PRP VBD , VBG IN PRP$ NNS IN PRP RB VBD DT NN TO VB NN IN RB , CC PRP VBD NNP NN IN TO VB RB . NN PRP MD VB VBN NN IN DT NN WDT VBD NNP NN VB RB IN DT JJ NN NNP NNP VBD IN DT NN CC VBD PRP$ NNS . NN PRP VBP JJ NN NN PRP VBD PRP VB RB VBN . NN PRP VBP VB DT NN , RB DT NN . VB DT NN VBP PRP$ NN VBD PRP$ NN . NN PRP VBP JJ NN VBP NN NN PRP VBD DT NN IN DT NN , VBD PRP RP TO PRP$ NN . PRP VBD PRP$ NN , VBD JJR IN PRP , CC VBD DT NN RB . DT JJ NN , PRP VBD VBG PRP IN PRP$ NNS . `` JJ IN RP . '' PRP VBD , VBG PRP$ NN IN PRP$ CC NN NNS DT NN : RB TO VB RB RB . VBG\", 'IN . IN PRP MD , PRP VBZ JJ VB DT NN IN PRP$ JJ NN ( PRP VBP JJ RB VBP PRP . ) , PRP$ NN NN ( VB PRP IN VBG PRP NNS NN ) , RB DT JJR NN WP RB VBZ PRP$ NN NN JJ NN ( DT PRP VBD PRP RB , VB RB TO PRP$ NNS . ) PRP VBZ NN VBD NNS IN NNS , IN DT NN NNP VBZ JJ . CD . WP VBZ JJ VBZ DT NN IN PRP$ NN CC DT NN VBG PRP WRB PRP VBD NN IN . WP VBD JJ JJ VBZ DT JJ NN PRP VBZ IN DT NNS , RB . CD . WRB PRP VBD NN IN PRP$ NN IN NN , PRP VBZ JJ NN TO VB DT JJ NNS : DT NN IN DT NN IN PRP$ NN WDT VBZ PRP RB VBP PRP$ NN , DT NN TO VB CC VB PRP$ NN CC NN TO VB RBR JJ , RBR NN , RB . PRP$ NNS VBP RB RB CC VBZ JJ NN , RB . CD . WRB PRP VBZ TO VB UH , UH . CC PRP VBZ RB VBN TO DT RB RBR JJ , VBG NN CC PRP VBZ PRP : CC PRP NNP VBZ RB IN PRP MD VB PRP , RB . CD . NNP NN NNP NN VBP TO VB , IN . PRP RB VBD PRP$ NN CC VBZ IN PRP RB : NN , VBP PRP RB . : CC IN NN PRP VBZ NNS VBP PRP VBZ , IN NN , JJ NN NN VB RB , IN NN PRP VBZ JJ NN , DT , RB . IN NN . PRP VBD RBR RB NN PRP DT NN CC PRP VBD NNP NN VBP WRB IN RB WRB PRP VBZ IN PRP VBZ RB VBN VBG NNS DT RB . NNP . CD . PRP JJ JJ NN IN PRP VBD RP CD NN CC VBD IN PRP VBZ NNP , PRP VBZ . PRP RB VBZ RP : DT NN , DT NN , DT NN NN VBG DT JJ , JJ NN WDT VBZ IN PRP$ NN CC VBZ TO VB VB . PRP VBD JJ VBD DT NNS VBG CC VBG RB JJ TO WP PRP VBZ , PRP VBZ JJ JJ RB TO VB CC VB RB WP PRP VBZ IN PRP VBZ JJ NN . CD . RB IN VBG CC VBG PRP VBD NNS JJR , IN . DT NN VBZ : WP VBZ PRP VBG TO VB IN PRP . CD . IN IN NN , PRP RB VBD JJ IN PRP : IN VBG RB JJ , IN NN . DT NN NNP NNP VB NNS RB RB RB : PRP VBD CC VBD , IN NNS VBN IN PRP$ NN , PRP VBP WRB PRP VBP . DT VBD RB RB WRB PRP VBP WRB PRP VBP IN PRP$ JJ NN CC NN NN . DT VBP NN WRB PRP VBZ IN , NN , NNP NN NNP NN NN CC NN , NNP NN NNP NN NN TO VB JJR . CD . DT NN PRP VBD NNP IN PRP CC JJ NNS RB , DT CD IN PRP VBD VBG DT NN IN NNP CC DT , PRP RB VB NNP VB VB WRB PRP VBD IN NN DT TO NN NNP , RB , RB IN RB PRP VBZ JJ VBG IN NN NNS CC PRP VBD VBG CC NNP VBD VBG CC RB VBD PRP . PRP VBD VBN DT JJ NN . CD . IN NN PRP VBZ JJ VBN IN VBG NNP , CC VBZ IN DT NN RB . IN DT , NNP NNP VBD DT RBS JJ IN PRP DT CC PRP MD VB VBG JJ NNS CC VB JJ NNS CC DT , PRP VBD PRP RB RB VBN IN DT . CC RB NNS NNS NNP TO VB IN PRP$ NN CC , RB , NNP NNP NN VBD TO VB , PRP VBZ JJ NN IN NN TO VB NN RB VB PRP$ JJ . CD . UH , PRP VBD NNP NN VBD NNP . PRP VBZ JJ NN VBG TO . DT JJ NN PRP VBD RB , PRP VBD VBG IN NN NNS CC NN WDT VBP TO DT JJ NN IN NNS VBG IN DT JJS NNS TO VB IN ( VBG PRP$ NNS IN JJ NN VBG DT RB JJ NN PRP VBP . PRP MD RB VB IN PRP$ NN MD VB IN JJ . ) CC RB , PRP VBZ JJ NNS VBP WRB DT VBD . PRP VBD NNP NN NNP NN , RB . CD . RB PRP VBZ JJ NN IN NN IN DT JJ NN WRB PRP VBZ PRP VBZ NN CC JJ NNP NN RB JJ IN VBG IN PRP : IN PRP VBZ JJ NNS VBP WRB . PRP VBZ NN RB VB RB , VBG PRP$ NN IN VBG RB TO VB RB JJ WRB NNP VBZ NNP IN IN PRP$ NNS , CC WRB PRP VBZ TO VB CC VB NNP DT PRP$ NN ( CC RB RB JJ ) NNS , CC WRB PRP VBD NNP NNP JJ NN DT NN DT NN VBZ TO VB RB . CD . IN RB , PRP VBZ JJ RB VBZ RP DT NN PRP VBZ PRP CC VB PRP$ JJS TO VB DT JJ NN PRP VBZ IN DT NN : RB VBP RP IN PRP$ IN NN , RB IN NN . RB IN PRP VBZ PRP : PRP VBZ VBD DT JJR NN IN PRP MD RB VB TO VB . CC RB . CD . IN RB , PRP VBZ JJ RB VBP DT NN IN PRP VBZ RB TO VB IN PRP DT NN PRP RB VBZ IN NNP , NNP , CC NNP . CD . IN DT ,', 'PRP VBD : PRP VBD IN NNP NNP RB RB , IN DT NN , PRP VBD JJ TO VB IN NNP NNP NNP VBD JJ NNS . PRP VBD RBR TO VB JJ CC JJ IN TO VB VBN RB IN NN NN CC JJ NNS , PRP VBD RB VB PRP : IN PRP VBD . ( PRP VBZ JJ NN . ) CD . PRP RB VBD , RB VBD DT JJ , RB VBD TO DT JJ RB . NNP NNP NN CC NNP NNP NN NNS VBD VBN , CC PRP$ CC NNS , RB , VBD RB . CD . RB DT NN PRP VBD PRP VBD VBD VBN RB , CC NN VBD RB DT JJ RB . CD . PRP$ JJ NN NN IN PRP$ VBD RB JJ , NN VB VBN . EX VBD DT JJ NN VBG IN IN DT NN , CC NNS VBD VBG VBN IN CC VB RB CC PRP VBD DT NN WP TO VB CC PRP VBD PRP VBD TO VB PRP , CC RB PRP VBD NNS CC VBD IN DT NN CC DT NN VBD . PRP VBD RB TO VB IN DT NN WRB PRP VBD PRP VBD JJ , VBG NNS : PRP VBD RB TO VB PRP$ NN WRB DT NN NN RB CC PRP VBD NN IN PRP , IN NNP NNP NN . CD . PRP$ NNS VBD JJ CC JJ . RB , PRP VBD IN PRP$ NN CC VBD RB IN PRP , CC DT NN VBD DT JJ NN IN PRP$ NN . CD . RB PRP VBD RB VB PRP PRP VBD VBG IN NNS RB VB NNP NN . PRP DT VBD RB JJ CC RB JJ IN WP TO VB WRB DT NN VBD VBG RB RB , WP IN PRP VBD . WP IN PRP VBD . PRP VBD RB JJ , CC JJ . DT IN PRP VBD RB , RB . WP MD PRP VB . CD . PRP VBD VB NN RP IN NNP VBD RB JJ , CC PRP VBD RB JJ IN PRP CC VB RB , PRP VBD NNP NN NNP , WP VBD RB DT NN IN NN CC NN IN PRP . PRP VBD JJ NN IN PRP VBZ JJ NN NNP NNP , RB : PRP VBD RB DT NN CC DT NN TO VB . NNP , IN PRP$ NN , DT CD IN PRP VBD RB JJ NN , IN PRP$ NNS RB VBD TO VB NNP CC NNP , RB NNP CC NNP . CD . PRP VBZ JJ NNS VBP NN TO VB TO PRP$ IN DT , IN PRP VBD PRP NNS TO VB JJ . PRP VBD , RB IN PRP VBD VBN DT NN IN NNP VBD , RB NNS MD VB : CC PRP VBZ CC JJ VB CD , CC RB NN VBD . PRP VBD IN PRP CC VBD IN PRP$ NNS RB , IN NN PRP VBD . CD . WRB PRP VBD PRP$ CC NNP NNP VBD NN TO VB NN , PRP VBD TO VB RP NN NNP CC NNP VBD PRP TO . NNP NN NNP NN NN PRP TO VB DT NN , IN : CC RB , PRP VBD . CD . DT JJ NN PRP VBD , PRP$ NN VBD VBN IN DT NN IN NNS CC NN NNS WDT RB VBG VBD DT NN IN NN MD . WRB PRP VBD PRP$ NNS , PRP VBD TO DT NN IN PRP$ NN TO PRP$ NNS . PRP$ NNS VBD JJ , CC PRP VBD NNP NN NN , CC PRP$ NNS VBD JJ IN PRP VBD TO PRP . PRP VBD JJ IN PRP . CD . CD NN , IN IN RB , PRP VBD PRP . RB , VBG CC VBN , PRP VBD IN PRP$ NNS VBD PRP DT NN TO VB JJ , TO VB NN RB TO VB IN : PRP RB VBD PRP NN TO VB , CC PRP VBD JJ IN PRP . PRP VBD DT JJ CC RB JJ NN TO VB , CC PRP$ NN NNS CC RB PRP VBD RB JJ TO VB PRP$ NN . PRP VBD . CD . PRP VBD RB JJ WRB PRP VBD RB DT CD IN PRP IN DT NN , IN , JJ TO DT NN CC DT JJ , DT NNP CC NNP NNS VBD VBG TO PRP . JJS NNS VBD DT JJ : CC IN RB , WRB DT JJS NNS MD VB PRP$ NN NN , NN MD RB VB VBN DT NN : NN IN PRP RB VBD TO PRP$ . PRP VBD RB JJ IN PRP$ CC VB PRP JJ VBD PRP$ NN IN VBG NN IN PRP , RB VB PRP$ NN , CC VB DT NN RP IN DT PRP$ NNS . CC PRP VBZ JJ NN IN PRP$ JJ NN : CC CD RB VBD . CD . WRB NN IN NNP NNP VBD NN VBD TO PRP , PRP VBD PRP DT JJS . PRP VBD PRP$ NN , PRP VBD , IN VBG NNP , IN VBG JJ CC JJ CC JJ . NNP VBD RB RP PRP$ NN , CC RB VBD DT JJ NN IN PRP$ NN CC NN IN PRP$ NNS IN PRP VBD TO VB NN PRP VBZ JJ NN IN PRP$ JJR . PRP VBD IN DT NN PRP VBD IN IN PRP VBD DT JJ NN . PRP VBD CC VBD PRP TO VB PRP . CD . RB , WRB DT NN VBD VBG VBN , PRP VBP JJ NN IN NNP NNP NN NN CC PRP VBD RB VB DT NN WP MD VB DT NN RB . PRP VBZ JJ NN WRB PRP VBD CC WRB DT NN IN NNP MD VB IN RB : WRB', \"RB RB IN PRP VBZ , PRP VBZ RB VBN NNS CC PRP$ NN CC NN IN RB NNS CC PRP$ JJ NN TO VB VBN IN JJ NN CC RB . PRP VBP NNS . PRP$ NN VBZ DT NNS TO VB . PRP VBP NNS . WRB MD RB PRP RB VBP PRP . RB WP NN VBP PRP$ NNS IN . : PRP VBZ CC NNS CC NNS IN PRP$ NNS DT VBP IN PRP , DT NN IN PRP$ : TO VB PRP , VB PRP$ DT NN TO VB . PRP MD VB PRP , DT NNS PRP VBD , MD VB DT JJ NN PRP VBD , DT JJ NN PRP VBD : VB PRP . PRP VBZ NN CC NN IN CD , CC PRP VBZ RB JJ IN NN RB JJ IN DT NN IN PRP$ NNS CC DT RB NN IN NN VBG PRP$ NN . : DT JJ JJ NNS IN DT JJ NN PRP VBD , NN POS RB VBN IN DT NNS IN PRP$ NN . PRP VBZ VBN IN JJ '' TO VB '' , CC RB PRP$ NN VBZ DT PRP VBP VBN IN . VBZ PRP VBN . VBZ PRP VBG RB . PRP VBZ TO VB . VB MD VB IN PRP . PRP VBZ RB DT JJ TO VB VBN , DT JJ TO VB VBN DT NN TO VB , DT JJ WP$ NNS VBP VBN TO , CC PRP VBZ PRP , VBZ DT JJ NNS VBG PRP$ DT NN , DT JJ NNS VBG PRP IN PRP VBZ DT NNS DT NN . CC PRP VBZ PRP VBZ PRP$ NN TO VB , RB IN PRP VBP , IN PRP DT VBP TO VB . PRP VBZ NN IN DT NN IN PRP VBP VBN TO VB DT , NN IN PRP$ DT NN , RB IN PRP VBZ IN DT JJ NN IN NN CC IN IN PRP VBZ , PRP DT VBP . PRP$ NN MD VB VBN RB JJ MD VB , RB . PRP VBP VBG PRP$ RB , IN PRP MD VB PRP WRB DT NN VBZ JJ . VBG PRP$ NNS , PRP VBZ IN WRB JJ PRP MD VB VBG DT . IN NN VBZ RB JJ DT NNS PRP VBP VBN TO VB JJ . IN VBG NN CC VBD TO NNS VBZ DT JJR NN TO VB . NNP VBZ RB RB , CC WP VBZ NN VBZ RB . : PRP VBZ PRP$ NNS CC PRP VBZ , VBG IN PRP , VBG DT NN IN DT NN IN PRP$ NN . PRP VBZ VBN CD NNS RB . WRB MD PRP VB . JJ VBD . PRP NNS . PRP VBZ , VBG VBD DT NN , PRP . ( CC RB , RB , IN PRP VBZ JJ RB : PRP MD VB DT NNS IN JJ NN IN NN RB VBZ PRP IN : IN PRP VBD . ) PRP VBD DT JJ TO VB , DT JJ . RB IN NNP , CC IN NNP . NNP VBD DT JJ , NNP , JJ . DT NN RB NNS , CC PDT DT JJ RB VBP CD NN IN JJ : PRP VBZ RB IN PRP . VB VBP JJ . WP IN . : PRP VBZ CC NNS CC PRP$ NNS VB PRP$ IN PRP$ NN VBZ PRP TO PRP$ NN . DT . DT . PRP$ NN NNS CC NNS VBP TO VB IN PRP$ NN CC PRP MD RB VB , MD RB VB , RB RB TO VB NNS TO PRP IN IN PRP VBZ , IN PRP VBZ DT JJ NN , RB PRP$ NN MD VB VBN NN . IN PRP VBZ PRP VB , VBZ PRP VB , VBZ PRP VB DT NN POS NN IN PRP$ JJ : DT TO VB PRP$ NN TO VB NNS . TO VB PRP DT NN TO VB DT JJ NN RB , CC RB , CC RB : : ( PRP VBZ JJ IN PRP WDT NN , VBZ TO VB CC NNS VBP IN JJ NNS , VBZ TO VB CC VB NN IN NN , VBZ TO VB VB RP DT NN . CC PRP VBZ NNS IN DT NN WDT VBZ PRP VB IN NN MD VB VBG JJ CC PRP VBZ IN PRP : IN RB IN PRP RB VBZ TO VB NNS IN DT , IN PRP . )\", \"JJ NNP IN PRP , VB . VB PRP VBD IN DT NN IN DT NN CC RB VBN . NNP , PRP VBD DT NNP JJ NNP NN JJ NNS : CC VB PRP IN VBG WRB DT NN NNS VBD IN PRP$ JJ NN . DT JJ NN IN PRP$ , PRP VBD , IN PRP VBD NN VBD PRP$ RB RB IN DT NN IN NN . : VBG IN PRP$ NN RB , PRP VBD NNP NN VBP PRP VBZ NN RB VBD PRP DT NN NN . PRP VBD PRP$ NN NN RB RB IN DT JJ NN IN PRP$ NN VBD VBN : RB IN PRP$ NN VBD DT NN RB . PRP VBD PRP NNS CC VBN IN DT NNS . RB , PRP MD VB IN PRP VBD VBG JJ IN PRP$ NN NN JJ NN . VBG IN PRP$ JJ NN , PRP VBD WRB PRP VBD , RB , IN PRP VBZ JJ NN RB JJ IN PRP$ NN . : DT JJ NN PRP VBD PRP , PRP VBD VBG DT NNS IN PRP$ WDT RB VBD PRP TO DT NN . PRP VBD RB JJ , PRP VBD , IN DT NN IN PRP$ VBN TO VB PRP RP PRP$ NN CC VB PRP RP PRP$ NN IN PRP VBD IN NN . ( PRP VBD RBR JJ IN DT JJ NN , RB . ) IN NN , PRP VBD DT IN DT NNS . ( CC VBD PRP VB DT NN IN PRP VBD JJ . ) PRP VBD RB IN PRP RB WRB PRP VBD PRP IN PRP$ NN JJ NN : CC DT VBD DT . : NN NNP , NNP NNP : PRP VBD TO VB IN PRP IN NN NN PRP VBD IN PRP PRP$ RBS JJ NN : DT NN WDT VBD JJS IN PRP$ NN VBG RB : CC VBD , NNP NNP NNP . PRP VBP PRP$ NN VBZ VBN PRP IN PRP$ JJ JJ NN PRP VBD JJ NNS VBP JJ IN PRP$ NN , CC PRP VBD , VBG PRP$ NN JJ CC VBG PRP IN PRP$ NN . CC EX NNP NN RB DT JJ NN PRP VBP JJ NN VBD RB JJ PRP VBD PRP NN CC VBD PRP IN VBG TO VB PRP IN PRP$ NN . PRP VBD , IN DT , DT NN : RB DT NN . CD NNS RB , CC VBD PRP VBG IN PRP IN JJ NNS , CC VBG RB WRB PRP VBD DT JJ NN , WRB PRP VBD . : NN NNP : VBP DT NN , NN NNP PRP VBD VBG IN PRP$ NN , NN IN VBG PRP$ NN , IN PRP VBZ JJ NNS RB VBP IN WP VBD IN DT NN IN PRP VBD IN . PRP VBD PRP . IN NN PRP VBD . NNP VBD IN PRP VBD PRP , NNP NNP PRP , '' CC PRP VBD . PRP VBD . VB PRP JJ JJ NN , NN PRP VBD . PRP VBD IN DT NN CC RB VBN : RB RBR NNS . NNP NN PRP VBD DT JJ NN . JJ NNP NN NN , NN PRP VBD , VBG TO VB DT NN . IN DT JJ NN , PRP MD VB VBN IN IN DT NN IN JJ NN . VBG IN PRP , RB , PRP VBD PRP JJ JJ . DT , IN PRP , VBD PRP . IN PRP VBD PRP NN CC VBD , '' IN NN . PRP$ NN VBD JJ TO VB JJ NN IN NN PRP VBP JJ NN . DT NN VBD JJ , CC PRP VBD RB DT NN . PRP VBD CC VBD , `` PRP VBD . PRP VBP NNS VBD JJ JJ NNS VBG TO VB DT NN TO VB DT NN : CC IN NN , IN PRP VBP , PRP MD VB JJ VB PRP RP VB DT NN NNS VBD PRP RB , CC IN EX PRP MD VB DT JJ NNS IN PRP$ NNS . PRP VBD PRP RB VBG NNS , PRP VBD RB . VB PRP NNP VBD DT VBG PRP DT RB IN , FW PRP VBD RB . PRP RB VBD PRP NN NNS . WP DT NN NN . DT NN NNS VBD , CC IN PRP VBD IN PRP PRP VBD , NNS VBP DT JJ NN . NN PRP RB VBD DT JJ NN CC VBD IN PRP , IN VBG RB . PRP VBD DT JJ NN . NNP .\", 'NNP NNP NNP VBZ RB RBR VB NN NN IN PRP RB VBD . PRP VBP DT IN NN NN , RB IN NNP VBD NN IN PRP . CC PRP VBZ JJ RB VBN DT JJ , DT NN IN , CC PRP RB , IN PRP$ NN TO RB VB PRP VB PRP JJ . : RB JJ NNS VBP RB VBN . VBN RP WRB CC IN PRP MD . PRP VBP PRP VBP JJ JJ NNS , RB : PRP VBP PRP MD VB VB NN WP VBZ NN VBG , IN PRP VBP VB RB JJ TO VB DT JJ . PRP$ NN VBZ IN VBG IN PRP VBP VBZ RB JJ . IN DT NN , PRP VBP PRP IN PRP , RB IN IN DT NN IN NNS PRP VBP NN IN DT RB NN IN DT NN NN : RB VBN . RB VBN . EX JJ NN RB RB JJ IN PRP VBP TO VB IN DT JJ NN . IN DT NN , PRP VBP WP VBP JJ VBG . PRP VBP PRP : DT NN TO VB PRP$ NN . PRP$ NN . RB IN EX VBP JJR NN NN IN TO VB PRP . NNP VBZ NN : CC PRP VBP WP PRP VBP RB VBZ RB JJ , CC PRP NNP VBZ VBP . PRP VBD RB VB TO VB : CC PRP VBP JJ NN VBG PRP PRP$ NN . PRP VBP DT NN IN VBG RP , IN PRP$ JJ NN , RB IN EX NNP VBD RB RB JJ IN PRP VBP . ( DT VBZ PRP$ JJ NN . ) : DT IN PRP$ ( JJ ) NNS VBP RB JJ . PRP VBP DT . PRP VBP PRP VBP DT , RB IN PRP DT VBP NN VBZ DT NN TO VB IN DT NN : RB DT JJ NNS , WP MD VB IN NN IN DT JJR NNS MD VB TO PRP$ NN . PRP VBP IN DT NN ( IN JJ IN PRP VBP ) CC PRP VBP PRP$ NN : CC NN IN PRP . PRP VBP WRB PRP VBD NN , CC PRP VBP JJ NN NN . ( PRP VBP WRB , RB . ) PRP VBP VB RB JJ PRP VBD NNP NN VBD . WP VBD NN JJ . PRP VBP PRP , IN PRP$ NN NN NNP NN NN PRP VBP IN PRP$ NN . PRP VBP JJ NN VBP TO VB PRP , CC PRP$ NNS VBP DT JJ NN IN VBG NN , IN PDT DT NN PRP VBP . CC PRP VBZ IN NNS PRP VBD IN , IN : DT NN NNP VBZ DT RBR NN , RB IN NN NNP VBZ VBN : PRP VBZ JJ RB DT JJ : CC PRP RB RB WRB JJ CC JJ PRP VBZ , RB RB JJ VB PRP$ NN . PRP VBP TO VB , CC VBZ JJ NN . RB , PRP VBP PRP TO VB PRP$ NN IN VBG IN DT RB : VB PRP IN DT NN IN NN . PRP VBP WRB PRP RB VBP TO VB PRP . RB WRB PRP VBZ DT NNS PRP VBP DT RBS : PRP VBP RB RB IN JJ . ( PRP VBD PRP IN , PRP . RB . ) CC PRP VBP RP PRP$ NN , CC VB PRP$ TO VB , IN DT NNP NN WRB PRP$ NN VBZ . CC PRP VBP PRP$ NN , VBG PRP$ IN EX NNP VBD NN NNS MD VB RB TO DT NN IN PRP VBZ DT NN TO VB NNS IN PRP , CC PRP VBP PRP VBZ PDT DT NN , IN PRP VBP RB VBN PRP IN PRP . WRB PRP VBD RB VBN TO VB DT JJ NNS TO VB IN CD NN , NN PRP VBP JJ JJ NN NNP NN VBN VBN RB . VBG PRP VBP JJ JJ NN NNP NN JJ , IN . ( IN PRP . ) RB , PRP VBP WRB PRP VBP JJ NN IN PRP$ NN , WRB DT VBZ RB DT NN TO VB IN JJ NN . CC PRP VBP DT NN IN PRP$ NN ( RB ) , CC PRP VBP PRP VBZ JJ NN RB JJ VBG . RB , PRP VBZ PRP , IN RB , RB VBG PRP$ NNS IN PRP$ NN : JJ NNP NN IN PRP , NNP . NNP IN DT JJ , NNP NNP , NNP NNP , RB NNP VBP VBG RB : IN IN PRP . PRP VBP PRP VBZ PDT DT JJ NN , CC PRP VBP PDT DT WP NNP VBD NN IN NNP VBD WP DT NN IN DT NN NN VBZ RB VBN JJR JJ . ( IN CC IN . )', \"CD . PRP VBD PRP IN NNS DT JJ NN PRP VBD PRP : IN DT , PRP VBD RB CD TO VB NN RB RB IN DT NN : DT NN , IN DT NN : CC VBD PRP DT NN TO VB RB JJ TO NNS , DT NN WP PRP$ NN IN NN VBD . CD . DT NN PRP VBD IN NN IN DT NN POS NN VBD PRP$ NN : PRP$ NNS VBD RB JJ CC JJ CC PRP$ NN VBD RB JJ CC VBG CC DT , PRP VBD RB RB VB DT NN IN DT NNS VBG RP PRP$ JJ RB : DT , DT , VB PRP . CD . PRP VBD JJ RB TO VB IN PRP$ NN VBD DT NN IN NN NN TO VB DT NN POS NNS VBP , RB IN PRP VBD JJ RB TO VB PRP MD RB VB NNS , DT NN WRB JJ CC VBG PRP VBD PRP TO VB ( RB IN PRP VBD IN JJ , RB ) . CD . IN DT NN PRP VBD PRP VBZ VBG TO VB PRP$ JJS NN NN IN PRP$ : PRP MD RB VB WRB , RB IN PRP VBD DT NN VBG PRP VBD IN VBG DT NN NN IN NN : CC IN DT JJ , VB JJ NN . CD . ( IN JJ CD VBD RB VB DT NN IN NN , CC RB PRP$ NN POS NNS VBD RB VB DT NN IN NN CC , RB . ) CD . PRP RB VBD PRP IN DT NN NN , IN IN PRP VBD TO NN DT CD VBZ PRP$ JJS NN VBD RB VBN IN IN NNS IN WP PRP VBD RBS JJ NN JJ IN , RB PRP MD VB DT RB JJ NN . CD . DT JJ NN PRP VBD DT NN TO RB VB , PRP VBD RB IN DT VBD CC RB : EX VBD DT NN TO PRP$ NN IN PRP VBD NN CC PRP , VBG . CD . IN IN JJ NN , PRP VBD RB JJ IN PRP$ NN '' VBD TO DT NN WRB PRP MD VB NNS IN DT RB JJ NN , RB RB IN NN IN WRB DT JJ VBD CC RB VB JJ IN DT NN . CD . WRB PRP VBD IN WP PRP VBD TO VB IN DT NN , PRP VBD JJ IN PRP RB VBD TO VB IN WP TO VB RB : VB TO PRP , CC VB TO PRP$ : IN DT NN , RB , EX RB VBD DT NN IN WP PRP MD VB IN DT NN IN DT . ( PRP VBD TO PRP RB . ) CD . DT NN IN PRP$ JJ PRP IN PRP$ JJS NN : RB WRB PRP VBD JJ CC NN CC NN , IN DT NN , JJ NNS PRP VBD JJ CC NN CC NN IN IN CD NNS , RB RB CD . CD . CC PRP VBD IN PRP RB VBD PRP RP TO VB PRP$ NN POS NNS : DT NNS VBD RB JJ , IN DT . CD . PRP VBD RB VB WRB , RB , CC RB PRP VBD JJ TO VB DT NNS CC DT JJ NN PRP VBD PRP , PRP PRP VBD VBN IN WRB JJ PRP VBD TO VB , IN EX VBD DT NN IN NN IN PRP$ NN , RB , CC WRB PRP VBD , PRP VBD RB JJ TO VB WRB DT VBD . CD . PRP VBD VBN IN , RB DT CD IN PRP , CC PRP VBD PRP TO VB DT NN IN JJ NN DT RB CC RB : PRP$ NN VBD RB VB PRP TO VB RB RB , CC WRB PRP VBP , PRP MD RB VB PRP$ NN CC IN IN DT NN RB MD , CC RB PRP VBD WP IN PRP VBD IN PRP RB CC : CD . PRP MD VB VBG IN PRP VBD PRP VBD RB VB IN VBG PRP$ DT NN PRP VBD NNS PRP VBD JJ , VBG , CC VBG : CC PRP VBD NNP NNP CC PRP VBD NNP NNP CC DT NN WRB PRP VBD IN PRP , DT CD IN PRP MD RB VB NNP NNP IN PRP . CD . NNS , PRP RB VBD NNS WDT VBD DT JJ , CC PRP VBD RP IN PRP RB VBD TO VB IN PRP IN PRP$ NN IN IN .\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jj', 'nns', '.', '``', 'cd', 'jjr', ',', \"''\", 'nnp', 'vbd', 'vbg', 'rb', 'vbn', 'in', 'prp$', 'nn', 'prp', '(', 'dt', ')', ':', 'vbp', 'to', 'cc', 'vb', 'wp', 'rp', 'wrb', 'ex', 'vbz', 'jjs', 'md', 'pdt', 'wdt', 'jj nns', 'nns .', '. ``', \", ''\", \"'' nnp\", 'nnp vbd', 'vbd ,', ', vbg', 'vbg rb', 'rb vbn', 'vbn in', 'in prp$', 'prp$ nn', 'nn .', '`` prp', 'prp vbd', 'vbd rb', 'vbn ,', ', in', 'in rb', 'rb rb', 'rb .', '. (', 'dt nn', 'nn nn', '. prp', 'rb :', 'prp$ nns', 'nns vbp', 'vbp rb', 'rb in', 'vbn to', 'nn in', 'in nn', 'nn cc', 'cc prp$', 'nns vbd', 'vbd to', 'to vb', 'vb prp$', 'prp$ jj', 'jj .', '. cc', 'cc vbd', 'vbd in', 'in prp', 'vbd vbg', 'vbg to', 'vb nnp', 'nnp nnp', 'nnp nn', 'nn vbd', 'vbd nn', 'nn to', 'nn ,', ', cc', 'vb prp', 'prp vb', 'vb in', 'in dt', 'in nnp', 'nnp ,', ': in', 'vbd prp', 'prp cc', 'cc rb', 'vbd dt', '. nnp', 'vbd vbn', 'to dt', 'dt in', 'prp ,', ', dt', 'nn wp', 'wp prp', 'vbd .', '. in', 'vbd nnp', 'jj nn', 'nn rb', 'rb cc', 'prp rp', ', jj', 'rb to', 'prp .', ', nn', 'cc nnp', 'vbn .', 'nns ,', '. )', ': dt', 'dt jj', 'nns in', 'rb jj', 'jj in', 'in vbg', 'vbg prp$', 'prp vbp', 'vbp to', 'vb wrb', 'jj jj', 'nn nns', 'nns cc', 'vbp prp$', 'ex vbd', 'rb prp', 'prp vbz', 'vbz nn', 'vbd jj', 'nnp in', 'rb ,', 'rb dt', 'prp to', 'vb rb', 'prp md', 'vbg nn', ') prp', 'prp nns', '. nn', 'nn prp', 'vbp vb', 'vb vbg', 'jj ,', ', nnp', ', rb', 'jj rb', 'rb vbd', 'prp nn', 'vbd prp$', 'cc prp', '. dt', 'jj prp', 'vbg in', 'rp in', 'nnp cc', 'cc vbg', 'vbg prp', 'prp in', 'pdt dt', 'dt nns', 'nns prp', 'vbd cc', 'in jj', 'nn (', 'vbp jj', ') cc', 'vb dt', 'cc in', 'rb vbg', 'cc vb', 'wrb prp', 'nnp prp', 'nnp .', 'nnp vbz', 'vbz prp$', 'vb nn', 'vbz jj', 'cc nn', 'cc dt', 'dt .', 'vb ,', 'to prp$', 'jj to', 'to prp', 'prp jj', 'md vb', 'vb jj', \". ''\", 'rb vb', 'vbp prp', 'jj cc', 'nn vbg', 'rp prp$', 'dt ,', 'vbg dt', 'nn nnp', 'prp$ cc', 'nns rb', 'md rb', 'vbp nn', 'dt cd', 'cd in', 'vbp .', 'vbz dt', 'rb wrb', ', wrb', 'prp$ in', 'vb .', '. vb', 'dt rb', 'prp rb', 'vb vbn', 'nn wdt', 'nn vbp', ', prp', 'nns dt', 'nn :', ': rb', '. vbg', 'rbr', 'in .', '( prp', 'rb vbp', 'rb vbz', 'nn jj', ', vb', 'vbd nns', 'in nns', '. cd', 'cd .', '. wp', 'vbz in', '. wrb', 'vbz prp', 'vb cc', '. prp$', 'cc vbz', 'vbz to', 'vbz rb', 'prp :', ': cc', '. :', 'vbz ,', 'prp dt', 'vbp wrb', 'nns vbg', 'vbg cc', '. rb', 'nn vbz', 'in in', 'vb nns', ': prp', 'dt vbd', 'vbp in', 'dt vbp', 'nn wrb', 'in ,', 'cc jj', 'nn dt', 'vbn dt', 'nns to', 'nn md', 'wrb dt', 'jj nnp', 'cc wrb', 'vb to', 'vbp dt', 'cc nns', 'in wp', 'pos', 'nn pos', ', vbz', 'in ex']\n",
      "(7, 287)\n",
      "[[50. 20.  0. ...  0.  0.  0.]\n",
      " [55. 27.  0. ...  0.  0.  0.]\n",
      " [53. 27.  0. ...  0.  0.  0.]\n",
      " ...\n",
      " [48. 20.  0. ...  0.  0.  0.]\n",
      " [48. 13.  0. ...  0.  0.  0.]\n",
      " [35. 17.  0. ...  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "def read_files(path,label):\n",
    "    # Reads all text files located in the 'path' and assigns them to 'label' class\n",
    "    files = glob.glob(path+os.sep+label+os.sep+'*.txt')\n",
    "    texts=[]\n",
    "    for i,v in enumerate(files):\n",
    "        f=codecs.open(v,'r',encoding='utf-8')\n",
    "        texts.append((f.read(),label))\n",
    "        f.close()\n",
    "    return texts\n",
    "\n",
    "def extract_vocabulary(texts,n,ft,pos=False):\n",
    "    # Extracts all characer 'n'-grams occurring at least 'ft' times in a set of 'texts'\n",
    "    occurrences=defaultdict(int) \n",
    "    for (text,label) in texts:\n",
    "        text_occurrences = {}\n",
    "        if isinstance(n, int):\n",
    "            for x in range(1,n+1):\n",
    "                text_occurrences.update(represent_text(text,x,pos=pos))\n",
    "        else:\n",
    "            pass\n",
    "        for ngram in text_occurrences:\n",
    "            if ngram in occurrences:\n",
    "                occurrences[ngram]+=text_occurrences[ngram]\n",
    "            else:\n",
    "                occurrences[ngram]=text_occurrences[ngram]\n",
    "    vocabulary=[]\n",
    "    for i in occurrences.keys():\n",
    "        if occurrences[i]>=ft:\n",
    "            vocabulary.append(i)\n",
    "    return vocabulary\n",
    "\n",
    "\n",
    "vocab = extract_vocabulary([(x,i) for i, x in enumerate(train_sents)], 2, 5, pos=True)\n",
    "print(len(vocab))\n",
    "print(vocab)\n",
    "vectorizer = CountVectorizer(vocabulary=[x.lower() for  x in vocab])\n",
    "print([' '.join(get_pos(text)) for text in train_sents])\n",
    "\n",
    "train_data = vectorizer.fit_transform([' '.join(get_pos(text)) for text in train_sents])\n",
    "print(vectorizer.get_feature_names())\n",
    "train_data = train_data.astype(float)\n",
    "print(train_data.shape)\n",
    "print(train_data.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem00001\n",
      "(140, 64239)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 3. 3. 4.]]\n",
      "['jj', 'nns', '.', '``', 'cd', 'jjr', ',', \"''\", 'nnp', 'vbd', 'vbg', 'rb', 'vbn', 'in', 'prp$', 'nn', 'prp', '(', 'dt', ')', ':', 'vbp', 'to', 'cc', 'vb', 'wp', 'rp', 'wrb', 'ex', 'vbz', 'jjs', 'md', 'pdt', 'uh', 'wdt', 'jj nns', 'nns .', '. ``', '`` cd', 'cd jjr', 'jjr ,', \", ''\", \"'' nnp\", 'nnp vbd', 'vbd ,', ', vbg', 'vbg rb', 'rb vbn', 'vbn in', 'in prp$', 'prp$ nn', 'nn .', '`` prp', 'prp vbd', 'vbd rb', 'vbn ,', ', in', 'in rb', 'rb rb', 'rb .', '. (', '( dt', 'dt nn', 'nn nn', 'nn )', ') .', '. prp', 'rb :', ': prp$', 'prp$ nns', 'nns vbp', 'vbp rb', 'rb in', 'in vbn', 'vbn to', 'to nn', 'nn in', 'in nn', 'nn cc', 'cc prp$', 'nns vbd', 'vbd to', 'to vb', 'vb prp$', 'prp$ jj', 'jj .', '. cc', 'cc vbd', 'vbd in', 'in prp', 'vbd vbg', 'vbg to', 'vb nnp', 'nnp nnp', 'nnp nn', 'nn vbd', 'vbd nn', 'nn to', 'nn ,', ', cc', 'vb prp', 'prp vb', 'vb in', 'in dt', 'in nnp', 'nnp ,', ', wp', ': in', 'vbd prp', 'prp cc', 'cc rb', ': vbd', 'vbd dt', '. nnp', 'vbd vbn', 'to dt', 'dt in', 'prp ,', ', dt', 'nn wp', 'wp prp', 'vbd .', '. in', 'vbd nnp', 'nnp jj', 'jj nn', 'nn rb', 'rb cc', 'prp rp', 'rp dt', ', jj', 'jj prp$', 'rb to', 'prp .', \"'' prp\", ', nn', 'cc nnp', '. jj', 'jj vbn', 'vbn .', 'vbg nns', 'nns ,', '. )', ': dt', 'dt jj', 'nns in', 'rb jj', 'jj in', 'in vbg', 'vbg prp$', 'prp vbp', 'vbp to', 'vb wrb', 'wrb to', 'jj jj', 'nn nns', 'nns cc', 'cc vbp', 'vbp prp$', 'cc ex', 'ex vbd', '( rb', 'rb prp', 'prp vbz', 'vbz nn', 'vbd jj', 'vbn nnp', 'nnp in', 'in jjs', 'jjs rb', 'rb ,', 'rb dt', 'prp to', 'vb rb', 'prp md', 'vbg nn', ') prp', 'prp nns', '. nn', 'nn prp', 'vbp vb', 'vb vbg', 'jj ,', ', nnp', ', rb', 'jj rb', 'rb vbd', 'prp nn', 'vbd prp$', 'cc prp', '. dt', 'jj prp', 'vbg in', 'vbg rp', 'rp in', 'nnp cc', 'cc vbg', 'vbg prp', 'prp in', 'in pdt', 'pdt dt', 'dt nns', 'nns prp', 'vbd cc', 'in jj', 'nn (', 'vbp jj', ') cc', 'vb dt', 'cc in', 'rb vbg', 'cc vb', 'prp wrb', 'wrb prp', '( jj', 'nnp prp', 'jj vbg', 'nnp .', 'nnp vbz', 'vbz prp$', 'vb nn', 'vbz jj', 'cc nn', 'cc dt', 'dt .', 'vb ,', 'nn ex', 'to prp$', '`` dt', 'dt nnp', 'nnp nns', 'vbp vbg', 'vbg jjr', 'jjr dt', 'jj to', 'to prp', 'prp jj', 'md vb', 'vb jj', \". ''\", '`` nnp', 'nnp rb', 'rb vb', '. nns', 'vbp prp', 'prp vbg', \"nn ''\", \"'' ``\", 'jj cc', 'nn vbg', 'vb rp', 'rp prp$', '. uh', 'uh ,', 'dt ,', 'vbg dt', 'dt jjs', 'jjs nn', 'prp$ vbg', 'nn nnp', 'prp$ cc', 'dt prp', 'vbd vbd', 'nns rb', 'vbg :', ': nnp', 'vbp md', 'md rb', 'vb :', ': wp', 'wp vbp', 'vbp nn', 'dt cd', 'cd in', 'vbp .', 'vbz vbz', 'vbz dt', 'rb wrb', 'wrb ex', ', wrb', 'wrb nnp', 'prp$ in', '`` wp', 'vb .', '. vb', 'in to', 'dt rb', 'prp rb', 'vb vbn', 'vbn nn', 'nn wdt', 'wdt vbd', 'nn vb', 'nn vbp', ', vbd', 'rp to', 'vbd jjr', 'jjr in', ', prp', '`` jj', 'in rp', 'rp .', 'nns dt', 'nn :', ': rb', '. vbg', 'rbr', 'rbs', 'in .', 'md ,', 'jj vb', '( prp', 'rb vbp', ') ,', ', prp$', 'nns nn', 'dt jjr', 'jjr nn', 'wp rb', 'rb vbz', 'nn jj', ', vb', 'vbd nns', 'in nns', '. cd', 'cd .', '. wp', 'wp vbz', 'jj vbz', 'wp vbd', 'vbz in', '. wrb', 'nns :', 'wdt vbz', 'vbz prp', 'vb cc', 'vb rbr', 'rbr jj', ', rbr', 'rbr nn', '. prp$', 'cc vbz', 'vbz to', 'vb uh', ', uh', 'uh .', 'vbz rb', 'rb rbr', 'prp :', ': cc', 'prp nnp', ': nn', ', vbp', '. :', 'vbz nns', 'vbz ,', 'vbd rbr', 'rbr rb', 'rb nn', 'prp dt', 'vbp wrb', 'wrb in', 'vbn vbg', 'vbd rp', 'cd nn', 'vbz nnp', 'vbz .', 'vbz rp', 'rp :', 'vb vb', 'jj vbd', 'nns vbg', 'vbg cc', 'rb wp', '. rb', 'nns jjr', 'nn vbz', 'vbz :', 'in in', 'nnp vb', 'vb nns', ': prp', 'nns vbn', 'dt vbd', 'vbp in', 'dt vbp', 'nn wrb', 'in ,', 'vb jjr', 'jjr .', 'cc jj', 'nn dt', 'dt to', 'vbn dt', 'vbg nnp', 'dt rbs', 'rbs jj', 'dt cc', 'vbg jj', 'rb nns', 'nns nns', 'nns nnp', 'nnp to', 'cc ,', 'to .', 'wdt vbp', 'jjs nns', 'nns to', 'nn md', 'wrb dt', 'jj nnp', 'wrb .', 'jj wrb', 'cc wrb', 'nnp dt', 'dt prp$', '( cc', 'jj )', 'prp$ jjs', 'jjs to', 'vbp rp', 'vbz vbd', 'vb to', 'vbp dt', 'vbd :', 'rbr to', 'vbn rb', ') cd', 'cc nns', 'prp$ vbd', '. ex', 'vbg vbn', 'in cc', 'wp to', 'prp prp', 'in wp', 'wp in', 'wp md', 'md prp', 'vbd vb', 'nn rp', 'rb nnp', 'nns md', 'vbz cc', 'vb cd', 'cd ,', 'rp nn', 'in :', 'nns wdt', 'wdt rb', 'vbg vbd', 'md .', 'cc vbn', ', to', '. jjs', 'jj :', 'prp$ .', 'cc cd', 'cd rb', 'wrb nn', 'jjs .', 'rb rp', 'prp$ jjr', 'nnp md', ': wrb', 'pos', 'wp$', 'vbn nns', 'vbp nns', 'wrb md', 'wp nn', 'prp$ :', ': to', 'prp$ dt', ', md', ': vb', 'in cd', 'nn pos', 'pos rb', 'vbz vbn', \"jj ''\", \"vb ''\", \"'' ,\", 'rb prp$', 'vbp vbn', '. vbz', 'prp vbn', 'vb md', 'to ,', ', vbz', 'vbp ,', 'jj md', 'prp$ rb', 'in wrb', 'wrb jj', 'jj dt', 'to nns', 'nns vbz', 'cc wp', 'vbn cd', 'cd nns', 'cc pdt', 'vbp cd', 'vb vbp', 'nns vb', 'pos nn', '. to', ': :', 'prp wdt', 'wdt nn', 'fw', 'vbg wrb', 'prp$ ,', 'vbn :', 'vbd wrb', 'prp$ vbn', 'nnp :', 'prp prp$', 'vbd jjs', 'jjs in', 'vbn prp', 'ex nnp', ': vbp', \"'' cc\", 'rbr nns', \"'' in\", ', ``', 'in ex', 'ex prp', 'dt vbg', ', fw', 'fw prp', 'wp dt', ', nns', 'to rb', '. vbn', 'vbn rp', 'rp wrb', 'vbg ,', 'vbp vbz', 'ex jj', 'vbp wp', 'vbg .', 'ex vbp', 'vbp jjr', 'vbz vbp', 'rp ,', 'dt vbz', 'jjr nns', '( in', 'wrb ,', 'dt rbr', 'prp$ to', 'wrb prp$', 'vbz pdt', 'nn vbn', 'vbn vbn', 'rb )', ': jj', 'nnp vbp', 'vbp pdt', 'dt wp', 'wp nnp', 'vbd wp', 'vbn jjr', 'jjr jj', 'rb cd', 'cd to', 'wp prp$', 'pos nns', 'vbz vbg', 'jj cd', 'cd vbd', 'cd vbz', 'vbd rbs', \"'' vbd\", ', ex', 'rb md', 'cc :', ': cd', 'nnps', \"'' dt\", 'nns rp', \"'' nnps\", 'nnps vbd', 'md vbn', '`` in', ': wdt', 'wdt prp', 'nnp pos', 'rbr ,', '`` rb', \"'' nns\", '. vbd', '`` uh', 'ex vbz', '`` ex', 'prp pdt', '`` wrb', 'wrb rb', 'nnp rp', ', wdt', 'wdt md', 'vbn prp$', \"'' ''\", 'pos jj', 'prp wp', 'cc to', 'rp rb', 'wrb vbp', 'wrb vbd', 'vbp :', \": ''\", 'nnps pos', '`` cc', 'pos in', 'cd nnp', 'pos ,', 'vb wp', 'vbz wrb', 'to nnp', 'nns jj', 'vbn jj', 'jj jjr', 'md vbp', 'vb vbz', 'dt vbn', 'vbn cc', 'pos dt', 'pos vbn', 'pos nnp', 'wp ,', '`` jjr', 'nns wp', 'prp$ vbz', 'rp vbn', 'in vbd', ', vbn', 'pos .', 'dt dt', 'to cc', 'prp$ nnp', '`` vbp', '`` vb', '`` ``', 'rb jjr', 'jjr to', 'nnp jjr', 'dt md', 'jj rp', 'rp prp', 'nn jjr', 'in vbz', ': nns', 'dt rp', '`` prp$', 'pos jjs', 'jjs ,', 'vbp nnp', '. md', 'vbp vbd', 'vbg vbg', 'rp nnp', 'in wdt', 'wdt nns', 'vbz rbr', 'rbr vbg', \"'' vbg\", 'prp jjr', 'vbd cd', '`` vbz', 'vbg cd', ', cd', 'jjs vbd', 'nnp vbg', '. wdt', 'vbg wp', 'in vb', ', jjs', '. pdt', 'rp cc', \"'' cd\", 'vbn wrb', \"'' ex\", 'nnps vbp', 'to vbg', 'nnp wrb', 'wrb vbz', 'vbp cc', 'fw dt', 'nnp wp', 'vbp vbp', 'vb vbd', 'nn rbr', 'jjr cc', 'cc rbr', 'jjr rb', 'fw nnp', 'rp vbg', 'wdt nnp', 'nn prp$', '. .', 'cc jjr', 'vbd pdt', 'rb pdt', 'jjs dt', 'to jj', 'nns rbr', 'vb pdt', 'prp$ wrb', 'cd jj', 'rbr .', 'prp$ rp', 'nnp fw', 'wrb nns', 'vbg rbr', 'rbr in', 'in jjr', 'rp jj', 'rb ex', 'vb ex', 'rp rbr', '. vbp', 'nns prp$', 'jj pos', 'wp jj', 'vbz vb', 'pos vbg', \"nnp ''\", \"'' vb\", 'cc md', 'wp$ nn', 'in ``', 'cc ``', \"vbn ''\", \"prp ''\", \"'' .\", 'md jj', 'wp nns', 'nn cd', 'prp rbr', 'md in', 'nns wrb', 'nn jjs', \"'' vbz\", 'vbn wp', 'vbz wp', \"'' nn\", 'wdt jj', 'ex .', 'wp .', 'vb wdt', '`` vbd', \"'' prp$\", 'jjr prp', 'md dt', 'dt vb', 'vb pos', 'pos vb', 'fw .', 'nns pos', 'vbz rbs', \"pos ''\", 'wrb vbg', 'jj vbp', 'pos vbd', 'vbn vbd', 'nnp prp$', ', pdt', 'cd dt', 'cc .', 'nnps .', 'ex md', 'nn pdt', 'cd :', 'vbz cd', ': .', 'vbz jjr', 'cc jjs', 'jjs jj', 'to cd', \"'' rb\", 'ex ,', 'rp nns', 'prp cd', 'rbr cc', 'jj uh', 'dt :', 'jjr nnp', 'vbg ex', 'ex in', 'md prp$', 'cd vbg', 'cd cc', ': ``', 'jjr vbd', \"'' vbp\", ', jjr', 'md nn', 'rp vbd', ': vbz', \"cd ''\", '`` vbg', 'jjs prp', 'jjr vb', 'vbp rbr', 'to :', \"'' md\", 'cd vbp', 'nnp cd', \"'' wrb\", 'nnp vbn', '`` nn', 'nnp wdt', '`` md', 'pos prp$', 'dt nnps', '. nnps', 'nnp ex', 'jj fw', 'prp ex', 'fw nn', 'vb jjs', '. jjr', 'wrb vb', '`` nns', 'vbg pdt', 'pos cc', 'wdt ,', \"'' uh\", \"'' jj\", \"'' wp\", 'vbz wdt', 'md cc', 'nns (', \"vbp ''\", 'nns pdt', 'cc uh', 'vbd ``', '`` ,', 'wdt .', \"uh ''\", 'nnp nnps', 'cd prp', 'vbg md', \"rb ''\", 'prp$ cd', \"dt ''\", '( ``', '#', '. #', '# cd', \"'' #\", 'vbz ex', '`` fw', 'rbr vbd', 'vbd ex', 'in rbr', 'jj wp', '`` :', '. fw']\n",
      "\t pos vocabulary size: 861 char vocabulary size: 64239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lexical diversity: (140, 1)\n",
      "[20. 27. 27. 35. 20. 13. 17. 27. 27. 13. 30. 33. 34. 27. 29. 29. 17. 42.\n",
      " 23. 22. 38. 34. 43. 25. 16. 22. 19. 29. 36. 25. 39. 24. 30. 39. 49. 21.\n",
      " 21. 32. 23. 34. 22. 35. 15. 19.  7. 30. 18. 14. 18. 32. 20. 22. 16. 31.\n",
      " 23. 10. 16. 43. 23. 12. 23. 28. 31. 20. 34. 40. 35. 30. 15. 19. 34. 25.\n",
      " 34. 32. 18. 24. 27. 21. 17. 25. 39. 19. 27. 29. 11. 27. 12. 19. 22. 18.\n",
      " 14. 11. 21. 37. 25. 19. 30. 32. 30.  9. 22. 28. 19. 35. 19. 29. 12. 18.\n",
      " 29. 16. 25. 16. 17. 31. 28. 24. 18. 30. 24. 26. 33. 31. 38. 26. 27. 39.\n",
      " 24. 32. 28. 42. 15. 34. 22. 23. 27. 27. 36. 32. 24. 36.]\n",
      "pos data: (140, 861) char data: (140, 64239) word data: (140, 158555)\n",
      "\t language:  en\n",
      "\t 20 candidate authors\n",
      "\t 140 known texts\n",
      "lexical diversity: (105, 1)\n",
      "\t 105 unknown texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sallyisa/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/Users/sallyisa/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/sallyisa/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/sallyisa/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t answers saved to file answers-problem00001.json\n",
      "problem00002\n",
      "(35, 26761)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 3. 3. 3.]]\n",
      "['dt', 'nn', '.', \"''\", 'nnp', 'rb', 'vbz', 'in', ',', 'jj', '``', 'prp', 'md', 'vb', 'to', 'vbg', 'rp', 'prp$', 'vbd', 'wp', 'nns', 'vbp', 'cc', 'vbn', 'cd', ':', 'pos', 'wrb', 'jjs', 'uh', 'rbs', 'pdt', 'dt nn', 'nn .', \". ''\", \"'' nnp\", 'nnp rb', 'rb vbz', 'vbz dt', '. in', 'in nnp', 'nnp ,', ', dt', 'nn in', 'in dt', 'dt vbz', 'vbz jj', 'jj .', '. ``', '`` in', 'in prp', 'prp md', 'md rb', 'rb vb', 'vb to', 'to prp', 'prp .', '. rb', 'rb ,', \", ''\", 'nnp vbz', 'vbz ,', ', vbg', 'vbg rp', 'rp prp$', 'prp$ nn', 'nn ,', 'dt jj', 'jj nn', 'in nn', '. nnp', 'nnp vbd', 'vbd prp', 'prp rb', 'rb .', '`` wp', 'wp vbz', 'vbz prp', 'vbz prp$', 'prp$ nns', 'nns rb', 'vbg .', '`` prp', 'prp vbz', ', nn', '. wp', 'prp vb', 'vb in', 'in .', \"'' prp\", 'nn rp', 'rp rb', 'vbg dt', 'prp vbp', 'vbp vbg', 'vbz in', 'nn rb', 'vbg to', 'to vb', '. prp', 'vb prp', 'prp jj', 'jj nnp', 'nnp to', 'vb nn', 'nn vb', 'to dt', '. cc', 'cc nnp', 'vbz nn', 'in vbn', 'vbn .', 'md vb', 'to nnp', 'nnp in', 'vbg prp', 'prp cc', 'cc vbg', 'prp in', 'vbz cc', 'cc vbz', '. vbg', 'vbg in', ', nnp', 'vbz rp', 'rp cc', 'cc rb', 'vbz .', 'vbz rb', 'rb vbg', 'prp ,', 'vbp prp', 'md .', 'vb jj', 'nnp nns', 'nns in', ', cc', 'nn to', 'jj rb', 'rb to', 'rb prp', 'in prp$', 'nn vbz', 'vbz vbn', 'vbn in', 'in cd', 'cd nn', 'prp :', ': prp', 'vbn ,', ', rb', 'rb in', 'nn pos', 'pos in', 'nnp .', '`` nnp', 'vbp rb', 'vb dt', \"'' ``\", '`` wrb', 'wrb vbp', 'prp vbd', 'vbd dt', ', in', 'vb .', 'nnp md', 'vb ,', 'in vbg', 'vb vbn', 'in nns', 'vbd jj', 'vbd vbn', 'vbn to', \"'' rb\", \"'' ''\", \"'' vbz\", 'vbz nnp', 'nn vbg', 'nn cc', 'vbg vbn', '`` vbp', 'prp vbg', 'dt nns', 'nns cc', 'vbg prp$', 'vbg rb', 'rb rb', 'rb jj', 'jj cc', 'cc vbd', 'vbd in', 'nnp pos', 'pos jj', \"'' vbg\", 'dt rb', 'in jj', 'jj in', 'vb prp$', 'prp$ jjs', 'jjs nn', 'nn nnp', 'nn nn', 'cc nns', 'nns :', 'in ,', 'in in', '. vb', 'nnp nn', 'nnp nnp', 'pos nns', 'nns vbp', 'vbp in', ', prp$', '`` jj', 'jj vbz', ': .', 'rp in', 'rb wrb', 'wrb prp', 'nns ,', 'rb md', 'cc prp', ', prp', 'dt jjs', 'vbd rb', 'rb dt', 'dt ,', 'nns vbg', '`` uh', 'uh ,', 'vb nnp', 'pos nn', 'vbp wrb', 'wrb jj', 'jj dt', 'vbn vbg', '. dt', 'dt prp', 'vbp vbz', 'pos rb', 'vbp to', 'vbz vbg', 'vbn rb', '. nn', 'nn prp', 'rp to', 'prp to', 'vb rb', 'jj to', 'prp prp', 'jj jj', 'jj ,', 'vbp ,', 'dt vbn', 'vbn cc', 'cc vbn', '`` dt', 'vbp nns', 'nns to', 'nns prp', 'cc nn', 'vbz to', 'dt prp$', 'nn nns', 'rb vbp', 'rp .', 'nnp prp', 'vb uh', ', vbp', 'vbp dt', 'nn dt', 'cc jj', 'rb vbn', ', vbz', 'dt rbs', 'rbs jj', 'prp$ jj', 'rp dt', '`` nns', '`` rb', 'vbg pdt', 'pdt dt', 'rbr', 'jjr', 'ex', '. wrb', 'wrb nnp', 'cc in', 'nn vbd', 'vbd cc', 'vb rbr', 'rbr .', 'in vb', 'dt nnp', 'vbz vb', 'nnp vb', ', vbn', 'cd jj', 'dt in', 'nns vbd', 'nns .', 'in wrb', 'pos vbg', 'rb vbd', 'dt vbg', 'vbg nn', 'jjr .', 'pos vbn', 'vbn vbn', 'vb nns', 'jj nns', 'rb nn', \"nnp ''\", 'rb cc', 'cc vb', \"'' dt\", 'dt cd', 'rb rbr', 'rbr in', 'nnp cc', ', jj', 'cd in', 'nns vbz', 'cc dt', 'vb vbg', 'uh .', 'vbg jj', 'vbg nnp', \"'' in\", 'in rb', 'cc wrb', ', ``', 'vbp jj', 'jj prp', 'rp ,', 'md ,', 'nn rbr', 'rbr ,', 'prp dt', 'nn wp', 'wp md', '`` vb', 'prp nns', '. prp$', 'nns vb', '. ex', 'ex jj', 'vbd nns', 'in ex', 'ex md', 'vbd nn', 'nn wrb', 'dt dt', 'vbp nn', 'vb wrb', 'wrb rb', ', nns', '. jj', 'nn :', 'prp$ ,', 'jj wrb', 'rb jjr', 'jjr in', 'vbg cc', 'wdt', 'vbd ,', \"'' cc\", 'vbn dt', 'vbd rp', 'nnp vbg', 'nn jj', \"nn ''\", 'nns nnp', 'wp prp', 'vbd .', 'vb vb', ', vb', 'nnp jj', 'vbd to', 'cc md', 'vbd nnp', ', wdt', 'wp dt', 'dt .', 'in jjs', 'jjs rb', 'jj vbg', 'vbz nns', 'vbg ,', 'nn wdt', ', vbd', 'prp rp', 'nns nn', 'prp nnp', 'prp$ vbg', 'rp jj', 'vb wp', 'vb rp', 'prp wrb', 'wrb .', 'nn vbp', 'vbp .', '. vbp', ', to', 'to nn', '. uh', 'nnp dt', \"'' jj\", 'dt cc', 'vb :', ': rb', 'prp$ nnp', 'dt vb', 'wrb vbd', 'vbn prp', 'nn md', 'vbn jj', 'rb cd', 'vb cc', '(', ')', 'vbz jjr', 'jjr to', 'wp vbp', 'nn vbn', 'in to', 'cc prp$', '. vbz', 'jj vbp', 'prp vbn', 'rb :', ': nnp', ', uh', 'vbz vbp', 'nn (', 'jjs in', 'rb )', 'vbg vbg', 'dt md', 'rb nnp', 'vbd vbg', 'wdt vbp', 'vbp nnp', ': dt', 'wdt prp', 'wrb dt', 'nnp vbp', 'vb jjr', 'jjr ,', 'wdt vbz', 'nnp :', \": ''\", 'md prp', 'jj :', 'vbp cc', 'vbg wrb', 'rp nnp', 'ex vbz', 'rb nns', 'vbg vbd', 'to prp$', 'to rb', 'in wp', 'to vbg', 'rbr jj', 'wdt md', '. to', 'prp$ rb', 'vb vbd', '( dt', 'nn )', ') ,', 'to .', 'wp nnp', 'nns jj', ', wrb', 'vbd prp$', '. :', ': :', ': jj', 'vbp vb', 'jjs nns', 'vbz wp', '. vbn', ': nn', \"'' prp$\", 'jj pos', 'vbp vbn', 'wp in', 'nn prp$', 'vbn nn', 'vbn nnp', 'wp vbd', 'nns vbn', 'jjs to', '. nns', 'nns md', 'dt vbd', 'rp nn', 'to ,', 'rb wp', 'wp to', 'jjr cc', 'jjr nn', 'vbg jjr', 'vbg nns', 'in cc', '`` cd', 'cd jjr', '. (', ': prp$', ', wp', ': in', 'jj prp$', 'jj vbn', '. )', 'wrb to', 'cc vbp', 'vbp prp$', 'cc ex', 'ex vbd', '( rb', ') prp', 'prp nn', 'in pdt', ') cc', '( jj', 'nn ex', 'jjr dt', 'prp$ cc', 'vbd vbd', 'vbp md', 'prp$ in', 'wdt vbd', 'vbd jjr', 'nns dt', 'jj vb', '( prp', 'dt jjr', 'wp rb', '. cd', 'cd .', ': cc', 'vbd rbr', 'jj vbd', 'vbz :', 'dt vbp', 'cc ,', '( cc', 'vbp rp', 'vbd :', ') cd', 'vbd vb', 'vb cd', 'cd ,', 'in :', 'nns wdt', 'wdt rb', 'prp$ .', 'cc cd', 'cd rb', 'wrb nn', 'prp$ jjr', 'vbn nns', 'wrb md', 'wp nn', 'prp$ :', 'prp$ dt', ', md', ': vb', \"jj ''\", 'rb prp$', 'to nns', 'cc wp', 'cd nns', 'wdt nn', 'fw', 'vbn :', 'vbd wrb', 'prp prp$', 'ex nnp', 'ex prp', ', fw', 'fw prp', 'vbn rp', 'vbp wp', 'jjr nns', '( in', 'wrb ,', 'vbz pdt', 'vbd wp', 'jjr jj', 'jj cd', 'cd vbd', ', ex', '`` ex', \"'' ex\", \"'' nn\", 'nnps', '. .', 'rp vbg', 'wrb vbz', 'wp .', 'fw nnp', 'prp$ wrb', 'jj jjr', 'in jjr', 'cc to', 'vbp vbp', ', cd', 'in wdt', '`` cc', 'md vbp', 'rb ex', 'in vbd', 'md vbn', 'prp jjr', 'nnp wp', 'wdt nnp']\n",
      "\t pos vocabulary size: 619 char vocabulary size: 26761\n",
      "lexical diversity: (35, 1)\n",
      "[ 88. 116.  89. 108. 109.  87. 106. 129. 113.  99.  63.  96.  88.  76.\n",
      " 117. 161. 131. 114. 144. 111. 133.  95.  96. 129. 135. 104. 117. 137.\n",
      " 135. 101. 107. 105. 139. 120. 114.]\n",
      "pos data: (35, 619) char data: (35, 26761) word data: (35, 44504)\n",
      "\t language:  en\n",
      "\t 5 candidate authors\n",
      "\t 35 known texts\n",
      "lexical diversity: (21, 1)\n",
      "\t 21 unknown texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sallyisa/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/Users/sallyisa/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/sallyisa/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/sallyisa/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t answers saved to file answers-problem00002.json\n",
      "elapsed time: 37.336755990982056\n"
     ]
    }
   ],
   "source": [
    "class Feature_Extractor():\n",
    "    '''\n",
    "    Performs feature extraction on input docs.\n",
    "    '''\n",
    "    def __init__(self, n, ft):\n",
    "        self.n = n\n",
    "        self.ft = ft\n",
    "        \n",
    "    def fit_transform(self, docs):\n",
    "        ## Char-level n-grams ##\n",
    "        char_vocab = extract_vocabulary(docs,self.n,self.ft)\n",
    "        self.char_vectorizer = CountVectorizer(analyzer='char', ngram_range=(self.n,self.n),\n",
    "                                          lowercase=False, vocabulary=char_vocab\n",
    "                                              )\n",
    "        char_data, self.char_vectorizer = self._fit_transform(self.char_vectorizer, docs)\n",
    "        print(char_data.shape)\n",
    "        print(char_data.toarray())\n",
    "\n",
    "        ## POS n-grams ##\n",
    "        pos_vocab = [x.lower() for x in extract_vocabulary(docs,2,self.ft,pos=True)]\n",
    "        print(pos_vocab)\n",
    "        self.pos_vectorizer = CountVectorizer(ngram_range=(1,2), vocabulary=pos_vocab\n",
    "                                             )\n",
    "        print('\\t', 'pos vocabulary size:', len(pos_vocab), 'char vocabulary size:', len(char_vocab))\n",
    "        pos_data, self.pos_vectorizer = self._fit_transform(self.pos_vectorizer, docs, pos_replace=True)\n",
    "        \n",
    "        ## Word n-grams ##\n",
    "        self.word_vectorizer = CountVectorizer(ngram_range=(2,3))\n",
    "        word_data, self.word_vectorizer = self._fit_transform(self.word_vectorizer, docs)\n",
    "        \n",
    "        ## Lexical Diversity\n",
    "        lex_div = self.lexical_diversity(docs)\n",
    "        print(pos_data.toarray()[:,1])\n",
    "        print('pos data: %s char data: %s word data: %s'%(pos_data.shape, char_data.shape, word_data.shape))\n",
    "        feature_data = self.combine_features((lex_div, pos_data, char_data, word_data \n",
    "                                             )) \n",
    "        return feature_data\n",
    "    \n",
    "    def combine_features(self, feat_tuple):\n",
    "        feature_data = hstack(feat_tuple)\n",
    "        return feature_data\n",
    "    \n",
    "    def replace_words_POS(self, texts):\n",
    "        return [' '.join(get_pos(text)) for text in texts]\n",
    "    \n",
    "    def lexical_diversity(self, docs):\n",
    "        lex_div = np.array([len(set(text)) / len(text) for (text,label) in docs]).reshape(len(docs), 1)\n",
    "        print('lexical diversity:', lex_div.shape)\n",
    "        return lex_div\n",
    "    \n",
    "    def _fit_transform(self, vectorizer, docs, pos_replace=False):\n",
    "        texts = [text for i,(text,label) in enumerate(docs)]\n",
    "        if pos_replace is True:\n",
    "            texts = self.replace_words_POS(texts) # replace words in text with POS\n",
    "        vec_data = vectorizer.fit_transform(texts)\n",
    "        vec_data = vec_data.astype(float)\n",
    "        return vec_data, vectorizer\n",
    "    \n",
    "    def _transform(self, vectorizer, docs, pos_replace=False):\n",
    "        texts = [text for i,(text,label) in enumerate(docs)]\n",
    "        if pos_replace is True:\n",
    "            texts = self.replace_words_POS(texts) # replace words in text with POS\n",
    "        vec_data = vectorizer.transform(texts)\n",
    "        vec_data = vec_data.astype(float)\n",
    "        return vec_data\n",
    "    \n",
    "    def transform(self, docs):\n",
    "        char_data = self._transform(self.char_vectorizer, docs)\n",
    "        word_data = self._transform(self.word_vectorizer, docs)\n",
    "        pos_data = self._transform(self.pos_vectorizer, docs, pos_replace=True)\n",
    "        lex_div = self.lexical_diversity(docs)\n",
    "        feature_data = self.combine_features((lex_div, pos_data, char_data, word_data\n",
    "                                             )) \n",
    "        return feature_data\n",
    "        \n",
    "    \n",
    "\n",
    "def baseline(path, outpath, n=3, ft=5, pt=0.1, feature_selection=False, \n",
    "             open_set=False, c=1, feat_sel_percent=None, clf=None):\n",
    "    start_time = time.time()\n",
    "    # Reading information about the collection\n",
    "    infocollection = path+os.sep+'collection-info.json'\n",
    "    problems = []\n",
    "    language = []\n",
    "    with open(infocollection, 'r') as f:\n",
    "        for attrib in json.load(f):\n",
    "            problems.append(attrib['problem-name'])\n",
    "            language.append(attrib['language'])\n",
    "    for index,problem in enumerate(problems):\n",
    "        print(problem)\n",
    "        # Reading information about the problem\n",
    "        infoproblem = path+os.sep+problem+os.sep+'problem-info.json'\n",
    "        candidates = []\n",
    "        with open(infoproblem, 'r') as f:\n",
    "            fj = json.load(f)\n",
    "            unk_folder = fj['unknown-folder']\n",
    "            for attrib in fj['candidate-authors']:\n",
    "                candidates.append(attrib['author-name'])\n",
    "        # Building training set\n",
    "        train_docs=[]\n",
    "        for candidate in candidates:\n",
    "            train_docs.extend(read_files(path+os.sep+problem,candidate))\n",
    "        train_labels = [label for i,(text,label) in enumerate(train_docs)]\n",
    "        #### Feature Extraction ###\n",
    "        ###### Fit-Transform Training Set #######\n",
    "        feat_extractor = Feature_Extractor(n, ft)\n",
    "        train_data = feat_extractor.fit_transform(train_docs)\n",
    "        if feature_selection is True:\n",
    "            ####### Feature Selection - Fit #######\n",
    "            print(\"training before feature selection:\", train_data.shape)\n",
    "            #sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "            #train_data = sel.fit_transform(train_data)\n",
    "            # We use the default selection function: the 10% most significant features\n",
    "            sel = SelectPercentile(f_classif, percentile=feat_sel_percent)\n",
    "            train_data = sel.fit_transform(train_data, train_labels)\n",
    "            #sel = SelectKBest(chi2, k=100000)\n",
    "            #train_data = sel.fit_transform(train_data, train_labels)\n",
    "            print(\"training after feature selection:\", train_data.shape)\n",
    "        print('\\t', 'language: ', language[index])\n",
    "        print('\\t', len(candidates), 'candidate authors')\n",
    "        print('\\t', len(train_docs), 'known texts')\n",
    "        \n",
    "        ###### Transform Test Set #######\n",
    "        test_docs=read_files(path+os.sep+problem,unk_folder)\n",
    "        test_data = feat_extractor.transform(test_docs)\n",
    "        if feature_selection is True:\n",
    "            ####### Feature Selection #######\n",
    "            print(\"test before feature selection:\", test_data.shape)\n",
    "            test_data = sel.transform(test_data)\n",
    "            print(\"test after feature selection:\", test_data.shape)\n",
    "        print('\\t', len(test_docs), 'unknown texts')\n",
    "        \n",
    "        ###### Applying Classifiers #####\n",
    "        max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "        scaled_train_data = max_abs_scaler.fit_transform(train_data)\n",
    "        scaled_test_data = max_abs_scaler.transform(test_data)\n",
    "        clf=CalibratedClassifierCV(OneVsRestClassifier(SVC(C=c)))\n",
    "        clf.fit(scaled_train_data, train_labels)\n",
    "        predictions=clf.predict(scaled_test_data)\n",
    "        proba=clf.predict_proba(scaled_test_data)\n",
    "        if open_set is True:\n",
    "            # Reject option (used in open-set cases)\n",
    "            count=0\n",
    "            for i,p in enumerate(predictions):\n",
    "                sproba=sorted(proba[i],reverse=True)\n",
    "                if sproba[0]-sproba[1]<pt:\n",
    "                    predictions[i]=u'<UNK>'\n",
    "                    count=count+1\n",
    "            print('\\t',count,'texts left unattributed')\n",
    "        # Saving output data\n",
    "        out_data=[]\n",
    "        unk_filelist = glob.glob(path+os.sep+problem+os.sep+unk_folder+os.sep+'*.txt')\n",
    "        pathlen=len(path+os.sep+problem+os.sep+unk_folder+os.sep)\n",
    "        for i,v in enumerate(predictions):\n",
    "            out_data.append({'unknown-text': unk_filelist[i][pathlen:], 'predicted-author': v})\n",
    "        with open(outpath+os.sep+'answers-'+problem+'.json', 'w') as f:\n",
    "            json.dump(out_data, f, indent=4)\n",
    "        print('\\t', 'answers saved to file','answers-'+problem+'.json')\n",
    "    print('elapsed time:', time.time() - start_time)\n",
    "\n",
    "base_dir='pan18-cross-domain-authorship-attribution-training-dataset-2017-12-02'\n",
    "out_dir = base_dir+os.sep+'output-dir'\n",
    "eval_dir = base_dir+os.sep+'eval-dir'\n",
    "\n",
    "params = {'n': 5,'ft': 3,'pt': 0.05,'feature_selection': True, 'c':0.1, 'feat_sel_percent': 85, 'clf': 'SVC'}\n",
    "params = {'n': 5,'ft': 3,'pt': 0.05,'feature_selection': False, 'c':0.1, 'feat_sel_percent': None, 'clf': 'SVC'}\n",
    "baseline(base_dir,out_dir,**params)\n",
    "%timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem00001 Macro-F1: 0.532\n",
      "problem00002 Macro-F1: 0.783\n",
      "Overall score: 0.658\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>ft</th>\n",
       "      <th>pt</th>\n",
       "      <th>feature_selection</th>\n",
       "      <th>c</th>\n",
       "      <th>feat_sel_percent</th>\n",
       "      <th>clf</th>\n",
       "      <th>problem-name</th>\n",
       "      <th>macro-f1</th>\n",
       "      <th>macro-precision</th>\n",
       "      <th>macro-recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.05</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>85</td>\n",
       "      <td>SVC</td>\n",
       "      <td>problem00001</td>\n",
       "      <td>0.641</td>\n",
       "      <td>0.621</td>\n",
       "      <td>0.784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.05</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>85</td>\n",
       "      <td>SVC</td>\n",
       "      <td>problem00002</td>\n",
       "      <td>0.783</td>\n",
       "      <td>0.783</td>\n",
       "      <td>0.783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.05</td>\n",
       "      <td>False</td>\n",
       "      <td>0.1</td>\n",
       "      <td>SVC</td>\n",
       "      <td>problem00001</td>\n",
       "      <td>0.532</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.689</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.05</td>\n",
       "      <td>False</td>\n",
       "      <td>0.1</td>\n",
       "      <td>SVC</td>\n",
       "      <td>problem00002</td>\n",
       "      <td>0.783</td>\n",
       "      <td>0.783</td>\n",
       "      <td>0.783</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.05</td>\n",
       "      <td>False</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SVC</td>\n",
       "      <td>problem00001</td>\n",
       "      <td>0.532</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.05</td>\n",
       "      <td>False</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SVC</td>\n",
       "      <td>problem00002</td>\n",
       "      <td>0.783</td>\n",
       "      <td>0.783</td>\n",
       "      <td>0.783</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n  ft    pt  feature_selection    c feat_sel_percent           clf  \\\n",
       "0  5   3  0.05               True  0.1               85           SVC   \n",
       "1  5   3  0.05               True  0.1               85           SVC   \n",
       "2  5   3  0.05              False  0.1              SVC  problem00001   \n",
       "3  5   3  0.05              False  0.1              SVC  problem00002   \n",
       "4  5   3  0.05              False  0.1              NaN           SVC   \n",
       "5  5   3  0.05              False  0.1              NaN           SVC   \n",
       "\n",
       "   problem-name  macro-f1  macro-precision  macro-recall  \n",
       "0  problem00001     0.641            0.621         0.784  \n",
       "1  problem00002     0.783            0.783         0.783  \n",
       "2         0.532     0.496            0.689           NaN  \n",
       "3         0.783     0.783            0.783           NaN  \n",
       "4  problem00001     0.532            0.496         0.689  \n",
       "5  problem00002     0.783            0.783         0.783  "
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "# Evaluation script for the Cross-Domain Authorship Attribution task @PAN2019.\n",
    "We use the F1 metric (macro-average) as implemented in scikit-learn:\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "We include the following ad hoc rules:\n",
    "- If authors are predicted which were not seen during training,\n",
    "  these predictions will count as false predictions ('<UNK>' class)\n",
    "  and they will negatively effect performance.\n",
    "- If texts are left unattributed they will assigned to the ('<UNK>'\n",
    "  class) and they will negatively effect performance.\n",
    "- The <UNK> class is excluded from the macro-average across classes.\n",
    "- If multiple test attributions are given for a single unknown document,\n",
    "  only the first one will be taken into consideration.\n",
    "\n",
    "Dependencies:\n",
    "- Python 2.7 or 3.6 (we recommend the Anaconda Python distribution)\n",
    "- scikit-learn\n",
    "\n",
    "Usage from the command line:\n",
    ">>> python pan19-cdaa-evaluator.py -i COLLECTION -a ANSWERS -o OUTPUT\n",
    "where\n",
    "    COLLECTION is the path to the main folder of the evaluation collection\n",
    "    ANSWERS is the path to the answers folder of a submitted method\n",
    "    OUTPUT is the path to the folder where the results of the evaluation will be saved\n",
    "\n",
    "Example: \n",
    ">>>  python pan19-cdaa-evaluator.py -i \".\\pan19-cross-domain-authorship-attribution-training-dataset-2019-01-23\\\" -a \".\\answ\n",
    "ers-unigram\" -o \".\\eval-unigram\\\"\n",
    "\n",
    "# References:\n",
    "@article{scikit-learn,\n",
    " title={Scikit-learn: Machine Learning in {P}ython},\n",
    " author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.\n",
    "         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.\n",
    "         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and\n",
    "         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},\n",
    " journal={Journal of Machine Learning Research},\n",
    " volume={12},\n",
    " pages={2825--2830},\n",
    " year={2011}\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "def eval_measures(gt, pred):\n",
    "    \"\"\"Compute macro-averaged F1-scores, macro-averaged precision, \n",
    "    macro-averaged recall, and micro-averaged accuracy according the ad hoc\n",
    "    rules discussed at the top of this file.\n",
    "    Parameters\n",
    "    ----------\n",
    "    gt : dict\n",
    "        Ground truth, where keys indicate text file names\n",
    "        (e.g. `unknown00002.txt`), and values represent\n",
    "        author labels (e.g. `candidate00003`)\n",
    "    pred : dict\n",
    "        Predicted attribution, where keys indicate text file names\n",
    "        (e.g. `unknown00002.txt`), and values represent\n",
    "        author labels (e.g. `candidate00003`)\n",
    "    Returns\n",
    "    -------\n",
    "    f1 : float\n",
    "        Macro-averaged F1-score\n",
    "    precision : float\n",
    "        Macro-averaged precision\n",
    "    recall : float\n",
    "        Macro-averaged recall\n",
    "    accuracy : float\n",
    "        Micro-averaged F1-score\n",
    "    \"\"\"\n",
    "\n",
    "    actual_authors = list(gt.values())\n",
    "    encoder = LabelEncoder().fit(['<UNK>'] + actual_authors)\n",
    "\n",
    "    text_ids, gold_authors, silver_authors = [], [], []\n",
    "    for text_id in sorted(gt):\n",
    "        text_ids.append(text_id)\n",
    "        gold_authors.append(gt[text_id])\n",
    "        try:\n",
    "            silver_authors.append(pred[text_id])\n",
    "        except KeyError:\n",
    "            # missing attributions get <UNK>:\n",
    "            silver_authors.append('<UNK>')\n",
    "\n",
    "    assert len(text_ids) == len(gold_authors)\n",
    "    assert len(text_ids) == len(silver_authors)\n",
    "\n",
    "    # replace non-existent silver authors with '<UNK>':\n",
    "    silver_authors = [a if a in encoder.classes_ else '<UNK>' \n",
    "                      for a in silver_authors]\n",
    "\n",
    "    gold_author_ints = encoder.transform(gold_authors)\n",
    "    silver_author_ints = encoder.transform(silver_authors)\n",
    "\n",
    "    # get F1 for individual classes (and suppress warnings):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        labels=list(set(gold_author_ints))\n",
    "        # Exclude the <UNK> class\n",
    "        for x in labels:\n",
    "            if encoder.inverse_transform(np.array([x]))=='<UNK>':\n",
    "                labels.remove(x)\n",
    "        f1 = f1_score(gold_author_ints,\n",
    "                  silver_author_ints,\n",
    "                  labels,\n",
    "                  average='macro')\n",
    "        precision = precision_score(gold_author_ints,\n",
    "                  silver_author_ints,\n",
    "                  labels,\n",
    "                  average='macro')\n",
    "        recall = recall_score(gold_author_ints,\n",
    "                  silver_author_ints,\n",
    "                  labels,\n",
    "                  average='macro')\n",
    "        accuracy = accuracy_score(gold_author_ints,\n",
    "                  silver_author_ints)\n",
    "\n",
    "    return f1,precision,recall\n",
    "\n",
    "def evaluate(ground_truth_file,predictions_file):\n",
    "    # Calculates evaluation measures for a single attribution problem\n",
    "    gt = {}\n",
    "    with open(ground_truth_file, 'r') as f:\n",
    "        for attrib in json.load(f)['ground_truth']:\n",
    "            gt[attrib['unknown-text']] = attrib['true-author']\n",
    "\n",
    "    pred = {}\n",
    "    with open(predictions_file, 'r') as f:\n",
    "        for attrib in json.load(f):\n",
    "            if attrib['unknown-text'] not in pred:\n",
    "                pred[attrib['unknown-text']] = attrib['predicted-author']\n",
    "    f1,precision,recall =  eval_measures(gt,pred)\n",
    "    return round(f1,3), round(precision,3), round(recall,3)\n",
    "\n",
    "def evaluate_all(path_collection,path_answers,path_out,params):\n",
    "    # Calculates evaluation measures for a PAN-18 collection of attribution problems\n",
    "    infocollection = path_collection+os.sep+'collection-info.json'\n",
    "    problems = []\n",
    "    data = []\n",
    "    with open(infocollection, 'r') as f:\n",
    "        for attrib in json.load(f):\n",
    "            problems.append(attrib['problem-name'])\n",
    "    scores=[];\n",
    "    for problem in problems:\n",
    "        prob_data = deepcopy(params)\n",
    "        f1,precision,recall=evaluate(path_collection+os.sep+problem+os.sep+'ground-truth.json',path_answers+os.sep+'answers-'+problem+'.json')\n",
    "        scores.append(f1)\n",
    "        prob_data.update({'problem-name': problem, 'macro-f1': round(f1,3), 'macro-precision': round(precision,3), 'macro-recall': round(recall,3)})\n",
    "        if os.path.isfile('metrics.csv'):\n",
    "            with open('metrics.csv', 'a') as f:  # Just use 'w' mode in 3.x\n",
    "                w = csv.DictWriter(f, prob_data.keys())\n",
    "                w.writerow(prob_data)\n",
    "        else:\n",
    "            with open('metrics.csv', 'w') as f:  # Just use 'w' mode in 3.x\n",
    "                w = csv.DictWriter(f, prob_data.keys())\n",
    "                w.writeheader()\n",
    "                w.writerow(prob_data)\n",
    "        data.append(prob_data)\n",
    "        print(str(problem),'Macro-F1:',round(f1,3))\n",
    "    overall_score=sum(scores)/len(scores)\n",
    "    # Saving data to output files (out.json and evaluation.prototext)\n",
    "    with open(path_out+os.sep+'out.json', 'w') as f:\n",
    "        json.dump({'problems': data, 'overall_score': round(overall_score,3)}, f, indent=4, sort_keys=True)\n",
    "    print('Overall score:', round(overall_score,3))\n",
    "    prototext='measure {\\n key: \"mean macro-f1\"\\n value: \"'+str(round(overall_score,3))+'\"\\n}\\n'\n",
    "    with open(path_out+os.sep+'evaluation.prototext', 'w') as f:\n",
    "        f.write(prototext)\n",
    "    return pd.read_csv('metrics.csv')\n",
    "        \n",
    "params\n",
    "evaluate_all(base_dir,out_dir,eval_dir, params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "my_dict = {\"test\": 1, \"testing\": 2}\n",
    "\n",
    "with open('mycsvfile.csv', 'w') as f:  # Just use 'w' mode in 3.x\n",
    "    w = csv.DictWriter(f, my_dict.keys())\n",
    "    w.writeheader()\n",
    "    w.writerow(my_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Combine all features for each sentence.\n",
    "\n",
    "Combine all the previous features, and generate a matrix encoding all previously mentioned features: unigrams, bigrams, trigrams and pos_tags. The resulting matrix should have the following dimensions: 3x31\n",
    "\n",
    "You could use the `sklearn.pipeline.FeatureUnion` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#toks = sentence_tokenize(train_sentences)\n",
    "\n",
    "pipe2 = Pipeline (FeatureUnion([(\"uni\", CountVectorizer(ngram_range = (1,3), min_df = 1)),(\"PoS\", nltk.word_tokenize(train_sentences_1[0]))])\n",
    "\n",
    "\n",
    "#CountVectorizer(ngram_range = (1,1), min_df = 1) #token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\"\n",
    "\n",
    "#bigrams_bow = bigr.fit_transform(train_sentences)\n",
    "#union.fit_transform(train_sentences)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra to play with: Check this website and think about it. Do you think you can use this for something? (in the exam)\n",
    "\n",
    "http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHARE YOUR KNOWLEDGE!\n",
    "\n",
    "### Do you know any other way of representing the features of the training/testing set?\n",
    "\n",
    "Please share your knowledge using the forum from Absalon!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
