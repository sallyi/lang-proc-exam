{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language modeling 2\n",
    "\n",
    "In the previous exercises we saw how to calculate a language model out of raw text. In this set of exercises you will learn how to deal with zero probabilities and how to evaluate LMs. Let's get started!\n",
    "\n",
    "In order to make everything easier and to avoid repeating code (even though it can be good to internalize doing some things), here you have a part of the previous exercise to calculate language models for Shakespeare's and Carrol's book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prod (ite):\n",
    "    if len(ite)==0:\n",
    "        return 1\n",
    "    else:\n",
    "        return ite[0]*prod(ite[1:])\n",
    "    \n",
    "alice = gutenberg.sents(\"carroll-alice.txt\")\n",
    "hamlet = gutenberg.sents(\"shakespeare-hamlet.txt\")\n",
    "\n",
    "#Create a LM for Alice's Adventures in Wonderland\n",
    "alice_wbounds = [[\"<s>\"]+sentence+[\"</s>\"] for sentence in alice]\n",
    "alice_words = [word for sentence in alice_wbounds for word in sentence]\n",
    "cfreq_alice_bigrams= nltk.ConditionalFreqDist(nltk.bigrams(alice_words))\n",
    "cprob_alice_bigrams = nltk.ConditionalProbDist(cfreq_alice_bigrams, nltk.MLEProbDist)\n",
    "\n",
    "#Create a LM for Shakespeare's Hamlet\n",
    "hamlet_wbounds = [[\"<s>\"]+sentence+[\"</s>\"] for sentence in hamlet]\n",
    "hamlet_words = [word for sentence in hamlet_wbounds for word in sentence]\n",
    "cfreq_hamlet_bigrams= nltk.ConditionalFreqDist(nltk.bigrams(hamlet_words))\n",
    "cprob_hamlet_bigrams = nltk.ConditionalProbDist(cfreq_hamlet_bigrams, nltk.MLEProbDist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now learned in class how to smooth a language model in a pretty simple way. This was interesting for the cases in which some words don't appear in a corpus.\n",
    "\n",
    "For example, the conditional probability $P(w_{n}=screen | w_{n-1}=The)$ is $0.0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MLEProbDist based on 108 samples>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cprob_alice_bigrams[\"The\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cprob_alice_bigrams[\"The\"].prob('screen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Use the variable `cfreq_alice_bigrams` to calculate $P_{Laplace}(w_{n}=screen | w_{n-1}=The)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import LaplaceProbDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<LaplaceProbDist based on 108 samples>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LaplaceProbDist(cfreq_alice_bigrams[\"The\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005988023952095809"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "LaplaceProbDist(cfreq_alice_bigrams[\"The\"]).prob('screen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "As you may have imagined, we can estimate the smoothed probabilities in NLTK. It is actually pretty simple, as we only need to change the probability distribution when calculating a conditional probaility distribution.\n",
    "\n",
    "Please, check the documentation of NLTK, mainly the one about probability, and check how you can estimate the smoothed probability distributions. Once you know how to do it, create a variable with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE\n",
    "cprob_alice_bigrams_laplace = nltk.ConditionalProbDist(cfreq_alice_bigrams, LaplaceProbDist, bins=len(alice))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you create the smoothed language model, you can use it as if it was a regular language model. Then, calculate again $P_{Laplace}(w_{n}=screen | w_{n-1}=The)$ using the new language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005988023952095809"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "cprob_alice_bigrams_laplace[\"The\"].prob('screen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you get the same result? Why? Think about it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Now, following the instructions from the previous class, please calculate three smoothed language models, based on bigrams, trigrams and quadrigrams, from Carrol's book.\n",
    "\n",
    "#### Hint: This doesn't work:\n",
    "\n",
    "`cfreq_corpus1_trigrams= nltk.ConditionalFreqDist(nltk.trigrams(words))`\n",
    "\n",
    "#### Hint: Recall the word boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a LM for Alice's Adventures in Wonderland\n",
    "alice_wbounds = [[\"<s>\"]+sentence+[\"</s>\"] for sentence in alice]\n",
    "alice_words = [word for sentence in alice_wbounds for word in sentence]\n",
    "alice_trigrams = nltk.trigrams(alice_words)\n",
    "condition_pairs = (((w0, w1), w2) for w0, w1, w2 in alice_trigrams)\n",
    "# https://stackoverflow.com/questions/38068539/finding-conditional-probability-of-trigram-in-python-nltk/38098159\n",
    "cfreq_alice_trigrams = nltk.ConditionalFreqDist(condition_pairs)\n",
    "cprob_alice_trigrams = nltk.ConditionalProbDist(cfreq_alice_trigrams, LaplaceProbDist, bins=len(alice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a LM for Alice's Adventures in Wonderland\n",
    "alice_wbounds = [[\"<s>\"]+sentence+[\"</s>\"] for sentence in alice]\n",
    "alice_words = [word for sentence in alice_wbounds for word in sentence]\n",
    "alice_ngrams = nltk.ngrams(alice_words, n=4)\n",
    "condition_pairs = (((w0, w1, w2), w3) for w0, w1, w2, w3 in alice_ngrams)\n",
    "# https://stackoverflow.com/questions/38068539/finding-conditional-probability-of-trigram-in-python-nltk/38098159\n",
    "cfreq_alice_4grams = nltk.ConditionalFreqDist(condition_pairs)\n",
    "cprob_alice_4grams = nltk.ConditionalProbDist(cfreq_alice_4grams, LaplaceProbDist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "To conclude, calculate the perplexity of the three language models using this string as a test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string = 'The cat was brown. the brownie was really good'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'The',\n",
       " 'cat',\n",
       " 'was',\n",
       " 'brown',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'the',\n",
       " 'brownie',\n",
       " 'was',\n",
       " 'really',\n",
       " 'good',\n",
       " '</s>',\n",
       " '<s>']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = \"<s> \" + test_string + \" </s> <s>\"\n",
    "example_list = example.replace('.', ' </s> <s>').split()\n",
    "example_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<s>', 'The'),\n",
       " ('The', 'cat'),\n",
       " ('cat', 'was'),\n",
       " ('was', 'brown'),\n",
       " ('brown', '</s>'),\n",
       " ('</s>', '<s>'),\n",
       " ('<s>', 'the'),\n",
       " ('the', 'brownie'),\n",
       " ('brownie', 'was'),\n",
       " ('was', 'really'),\n",
       " ('really', 'good'),\n",
       " ('good', '</s>'),\n",
       " ('</s>', '<s>')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams = [bigram for bigram in nltk.bigrams(example_list)]\n",
    "bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<s>', 'The')\n",
      "0.04543053354463814\n",
      "('The', 'cat')\n",
      "0.005988023952095809\n",
      "('cat', 'was')\n",
      "0.045454545454545456\n",
      "('was', 'brown')\n",
      "0.0020242914979757085\n",
      "('brown', '</s>')\n",
      "0.25\n",
      "('</s>', '<s>')\n",
      "1.0\n",
      "('<s>', 'the')\n",
      "0.012678288431061807\n",
      "('the', 'brownie')\n",
      "0.000510986203372509\n",
      "('brownie', 'was')\n",
      "no:  ('brownie', 'was')\n",
      "('was', 'really')\n",
      "0.0020242914979757085\n",
      "('really', 'good')\n",
      "0.09090909090909091\n",
      "('good', '</s>')\n",
      "0.023809523809523808\n",
      "('</s>', '<s>')\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "for bigram in bigrams:\n",
    "    print (bigram)\n",
    "    try:\n",
    "        print(cprob_alice_bigrams_laplace[bigram[0]].prob(bigram[1]))\n",
    "    except:\n",
    "        print( 'no: ', bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.3676024796704636e-40"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "\n",
    "p = prod([cprob_alice_bigrams_laplace[bigram[0]].prob(bigram[1]) for bigram in nltk.bigrams(example_list)])\n",
    "print(np.power(2, -np.log2(p).sum() / count))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
